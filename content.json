[{"title":"不忘初心，也不在乎什么始终","date":"2017-06-26T13:43:12.000Z","path":"2017/06/26/不忘初心，也不在乎什么始终/","text":"一直有不要把兴趣和事业混为一谈的说法，我仔细看过，都很有道理。 想象过一些人痛心疾首的望着我说，“你在这有天分，你不做是在浪费你的才华！”这时我就呵呵一笑，“我的才华还有很多，不差浪费这一点”。 好在我的兴趣不需要什么卓越的智商和天分，就是门手艺而已。而手艺是需要磨练的，没时间磨练还搞什么手艺。 好比你爱上一个姑娘，要是两情相悦最好。可要是单恋一枝花，你也没什么办法。 离开舒适区，回到起点，做喜欢的事情，要是摔到头破血流–也没什么办法不是。","tags":[]},{"title":"机器学习实战读书笔记 -- 朴素贝叶斯","date":"2017-06-18T06:04:06.000Z","path":"2017/06/18/机器学习实战读书笔记 -- 朴素贝叶斯/","text":"最近太忙，实在没时间充电了，翻出一篇一年前的学习笔记先顶一下，虽然当今机器学习框架已经封装的非常好了，但是了解何时使用和算法里发生了什么也是很重要的。 简介 用python的文本处理能力将文档切分成词向量，对文档进行分类和过滤，最终将结果转换成人可以理解的信息。 优点：在数据较少的情况下仍然有效，可以处理多类别问题。 缺点：对于输入数据的准备方式较为敏感。 使用数据类型：标称型数据。 流程 朴素贝叶斯的一般流程： 1.收集数据：可以使用任何方法。 2.准备数据：需要数值型或者布尔型数据。 3.分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。 4.训练算法：计算不同的独立特征的条件概率。 5.测试算法：计算错误率。 6.使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器。 使用情况分类 决策树和朴素贝叶斯的使用情况分类： 决策树适合标准的离散化数据，并且对数值型数据的处理难以处理。 朴素贝叶斯的要求相对较低。 示例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#-*- encoding:utf-8 -*-from numpy import *def loadDataSet(): postingList=[['my','dog','has','flea','problems','help','please'], ['maybe','not','take','him','to','dog','park','stupid',], ['my','dalmation','is','so','cute','I','love','him'], ['stop','posting','stupid','worthless','garbage'], ['mr','licks','ate','my','steak','how','to','stop','him'], ['quit','buying','worthless','dog','food','stupid']] #1 代表侮辱性文字，0代表正常言论 classVec=[0,1,0,1,0,1] return postingList,classVecdef createVocabList(dataSet):# 取唯一值 vocabSet=set([]) for document in dataSet: vocabSet=vocabSet|set(document) return list(vocabSet)def setOfWords2Vec(vocabList,inputSet): # 参数1唯一值 当前列表参数2原值，出现在当前词组出现在总词库中的索引 returnVec=[0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)]=1 else: print \"the word %s is not in my Vocablary!\"%word return returnVecdef trainNB0(trainMatrix,trainCategory): #只针对二分类问题 numTrainDocs=len(trainMatrix) # 统计有几个训练用词组文档 numWords=len(trainMatrix[0]) # 词库数量 pAbusive=sum(trainCategory)/float(numTrainDocs) # 侮辱性词组数量和词组数量的比值 p0Num=ones(numWords);p1Num=ones(numWords) #p0Num,p1Num等于总词库数量个1 p0Denom=2.0;p1Denom=2.0 for i in range(numTrainDocs): # 以i遍历每个词组 if trainCategory[i]==1: # 如果是侮辱性词组 p1Num+=trainMatrix[i] # 每个词在总词库副本中的计数+1 p1Denom+=sum(trainMatrix[i]) #侮辱性词组中单词的总数 else: p0Num+=trainMatrix[i] # 每个词在总词库中的计数+1 p0Denom+=sum(trainMatrix[i]) # 非侮辱性词组中单词的总数 p1Vect=log(p1Num/p1Denom) # 侮辱性词在词库中的占比,避免下溢出，采取自然底数 p0Vect=log(p0Num/p0Denom) # 非侮辱词在词库中的占比 return p0Vect,p1Vect,pAbusivedef classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): p1=sum(vec2Classify*p1Vec)+log(pClass1) # 出现词组的位置与总字库内词组向量相乘，加自然底数中侮辱性词库在词库中的占比 p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1) # 权重越大，说明可能性越高 if p1&gt;p0: return 1 else: return 0def testingNB(): listOPosts,listClasses=loadDataSet() # 原文档和词组是否为侮辱性词汇的定义 myVocabList=createVocabList(listOPosts) # 总词库，每个词都是唯一的 trainMat=[] for postinDoc in listOPosts: # 形成一个二维数组，内有每个在总词库，文档单词在库中的索引 trainMat.append(setOfWords2Vec(myVocabList,postinDoc)) p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses)) # 返回每个词组的侮辱性语言概率，非侮辱性语言概率，侮辱性语言再词库中所占比 testEntry=['love','my','dalmation']#测试词汇 thisDoc=array(setOfWords2Vec(myVocabList,testEntry)) # 添加测试词组,返回测试词组个总词库索引 print testEntry,'classified as:',classifyNB(thisDoc, p0V, p1V, pAb) testEntry=['stupid','garbage'] thisDoc=array(setOfWords2Vec(myVocabList,testEntry)) print testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb)if __name__=='__main__': testingNB() 总结 朴素贝叶斯算法是充分的利用概率原理，用概率的方法分类一个对象的所属类别，对多个类型的对象都可以使用。 算法首先收集了所有训练的词组（数字），通过对总体（比如说文章）的的判断训练，让机器清楚每个的词组出现在总体中分类的概率，用对数的方式防止向下溢出，最终整合概率进行分类。","tags":[]},{"title":"改善 Python 程序的 91 个建议读书笔记 3","date":"2017-06-08T14:54:00.000Z","path":"2017/06/08/改善 Python 程序的 91 个建议读书笔记 3/","text":"建议 41：使用argparse处理命令行参数 处理命令行参数可以使用argsparse，也推荐更方便更高级的docopt进行处理。 docopt是根据常见的帮助信息定义了一套领域特定语言（DSL），并通过这个DSL Parser参数生成处理命令行参数的代码。 建议 42：使用pandas处理大型CSV文件 pandas作为python三大科学运算库之一的使用。 建议 43：一般情况下使用ElementTree解析xml格式文件 使用Beautifulsoup更好 建议 44：理解模块pickle优劣 序列化，简单来说就是把内存中的数据结构在不丢失其身份和类型信息的情况下转成对象的文本或二进制表示的过程。同类支持序列化的模块有pickle，json，marshal和shelve。 pickle是最通用的序列化模块，我们应该优先使用c语言实现的cPickle，速度比pickle快1000倍，区别是cPickle不能被继承。 pickle主要通过dump和load两种方法序列化与反序列化（存储与读取） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332import cPickle as pickle # 序列化 my_data= &#123;\"name\":\"Python\",\"type\":\"Language\"&#125; fp = open(\"picklefile.dat\",\"wb\") pickle.dump(my_data, fp) fp.close # 反序列化 fp = open(\"picklefile.dat\", \"rb\") out = pickle.load(fp) ``` pickle模块的优点： 1. 接口简单，容易使用。 2. 存储格式有平台通用型，在Linux和Windouws都可以使用，兼容性好。 3. 支持数据类型广泛，除了常规项，还包含能通过类的\\__dict__或\\__getstate__()方法返回的对象。 4. pickle是可扩展的，对于不可序列化的对象，也可以通过特殊方法来返回示例在被pickle时的状态。 5. 能够自动维护对象间的引用 pickle模块的限制： * pickle不能保证操作的原子性。当错误发生时，可能部分数据已经被保存；如果对象处于深递归状态，那么可能超过python的最大递归深度，可以通过sys.setrecursionlimit()进行扩展。 * pickle存在安全性问题，为乳清提供了可能。 * pickle协议是python特定的，不同语言之间数据内容可能难以保障。 简单来说，对于需要存储的对象，使用pickle，另外很重要的一点，**dat文件用pickle模块来读**。 ### 建议 45：序列化的另一个不错的选择 -- JSON cJson比python自身的json要快250倍 JSON的优势： 1. 使用简单，支持多种数据类型（集合、列表、字典、关联数组等等） 2. 存储格式可读性更友好，易于修改 3. 支持跨平台跨语言操作，所占空间更小 4. 具有较强扩展性 json的速度比pickle略慢。 **json不支持序列化dateime** ### 建议 46：使用 traceback 获取栈信息 当发生异常，开发人员往往需要看到现场信息，trackback 模块可以满足这个需求，先列几个常用的： ```python traceback.print_exc() # 打印错误类型、值和具体的trace信息 traceback.print_exception(type, value, traceback[, limit[, file]]) # 前三个参数的值可以从sys.exc_info() raceback.print_exc([limit[, file]]) # 同上，不需要传入那么多参数 traceback.format_exc([limit]) # 同 print_exc()，返回的是字符串 traceback.extract_stack([file, [, limit]]) # 从当前栈中提取 trace 信息 ``` traceback 模块获取异常相关的数据是通过sys.exc_info()得到的，该函数返回异常类型type、异常value、调用和堆栈信息traceback组成的元组。 同时 inspect 模块也提供了获取 traceback 对象的接口。 ### 建议 47：使用 logging 记录日志信息 仅仅将信息输出到控制台是远远不够的，更为常见的是使用日志保存程序运行过程中的相关信息，如运行时间、描述信息以及错误或者异常发生时候的特定上下文信息。Python 提供 logging 模块提供了日志功能。 常规日志设置: ```python logging.basicConfig( filename='%s.log' % self.table_name, level=logging.DEBUG, format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S') ``` logging是线程安全的，不支持多进程写入同一个子文件，对多个进程需要配置不同的日志文件。 ### 建议 48：使用 threading 模块编写多线程程序 （python3中，使用threadpool线程池模块比较省心） 由于 GIL 的存在，让 Python 多线程编程在多核处理器中无法发挥优势，但在一些使用场景下使用多线程仍然比较好，如等待外部资源返回，或建立反应灵活的用户界面，或多用户程序等。 Python3 提供了两个模块：_thread和threading。_thread提供了底层的多线程支持，使用比较复杂，下面我们重点说说threading。 Python 多线程支持用两种方式来创建线程：一种通过继承 Thread 类，重写它的run()方法；另一种是创建一个 threading.Thread 对象，在它的初始化函数__init__()中将可调用对象作为参数传入。 threading模块中不仅有 Lock 指令锁，RLock 可重入指令锁，还支持条件变量 Condition、信号量 Semaphore、BoundedSemaphore 以及 Event 事件等。 下面有一个比较经典的例子来理解多线程： ```python import threading from time import ctime,sleep def music(func): for i in range(2): print(\"I was listening to %s. %s\" % (func,ctime())) sleep(1) # 程序休眠 1 秒 def move(func): for i in range(2): print(\"I was at the %s! %s\" % (func,ctime())) sleep(5) threads = [] t1 = threading.Thread(target=music,args=('爱情买卖',)) threads.append(t1) t2 = threading.Thread(target=move,args=('阿凡达',)) threads.append(t2) if __name__ == '__main__': for t in threads: t.setDaemon(True) # 声明线程为守护线程 t.start() #3 print(\"all over %s\" % ctime()) ``` 以下是运行结果： ```python I was listening to 爱情买卖. Tue Apr 4 17:57:02 2017 I was at the 阿凡达! Tue Apr 4 17:57:02 2017 all over Tue Apr 4 17:57:02 2017 ``` 分析：threading 模块支持线程守护，我们可以通过setDaemon()来设置线程的daemon属性，当其属性为True时，表明主线程的退出可以不用等待子线程完成，反之，daemon属性为False时所有的非守护线程结束后主线程才会结束，那运行结果为： ```python I was listening to 爱情买卖. Tue Apr 4 18:05:26 2017 I was at the 阿凡达! Tue Apr 4 18:05:26 2017 all over Tue Apr 4 18:05:26 2017 I was listening to 爱情买卖. Tue Apr 4 18:05:27 2017 I was at the 阿凡达! Tue Apr 4 18:05:31 2017 ``` 继续修改代码，当我们在#3处加入t.join()，此方法能够阻塞当前上下文环境，直到调用该方法的线程终止或到达指定的 timeout，此时在运行程序： ```python I was listening to 爱情买卖. Tue Apr 4 18:08:15 2017 I was at the 阿凡达! Tue Apr 4 18:08:15 2017 I was listening to 爱情买卖. Tue Apr 4 18:08:16 2017 I was at the 阿凡达! Tue Apr 4 18:08:20 2017 all over Tue Apr 4 18:08:25 2017 ``` 当我们把music函数的休眠时间改为 4 秒，再次运行程序： ```pythonI was listening to 爱情买卖. Tue Apr 4 18:11:16 2017 I was at the 阿凡达! Tue Apr 4 18:11:16 2017 I was listening to 爱情买卖. Tue Apr 4 18:11:20 2017 I was at the 阿凡达! Tue Apr 4 18:11:21 2017 all over Tue Apr 4 18:11:26 2017 ``` 此时我们就可以发现多线程的威力了，music虽然增加了 3 秒，然而总的运行时间仍然为 10 秒。 ### 建议 49：使用 Queue 使多线程编程更加安全 （同47，使用threadingpool） 线程间的同步和互斥，线程间数据的共享等这些都是涉及线程安全要考虑的问题。纵然 Python 中提供了众多的同步和互斥机制，如 mutex、condition、event 等，但同步和互斥本身就不是一个容易的话题，稍有不慎就会陷入死锁状态或者威胁线程安全。 如何保证线程安全呢？我们先来看看 Python 中的 Queue 模块： * Queue.Queue(maxsize)：先进先出，maxsize 为队列大小，其值为非正数的时候为无限循环队列 * Queue.LifoQueue(maxsize)：后进先出，相当于栈 * Queue.PriorityQueue(maxsize)：优先级队列 以上队列所支持的方法： * Queue.qsize()：返回近似的队列大小。当该值 &gt; 0 的时候并不保证并发执行的时候 get() 方法不被阻塞，同样，对于 put() 方法有效。 * Queue.empty()：队列为空的时候返回 True，否则返回 False * Queue.full()：当设定了队列大小的情况下，如果队列满则返回 True，否则返回 False * Queue.put(item[, block[, timeout]])：往队列中添加元素 item，block 设置为 False 的时候，如果队列满则抛出 Full 异常。如果 block 设置为 True，timeout 为 None 的时候则会一直等待直到有空位置，否则会根据 timeout 的设定超时后抛出 Full 异常 * Queue.put_nowait(item)：等于 put(item, False).block 设置为 False 的时候，如果队列空则抛出 Empty 异常。如果 block 设置为 True、timeout 为 None 的时候则会一直等到有元素可用，否则会根据 timeout 的设定超时后抛出 Empty 异常 * Queue.get([block[, timeout]])：从队列中删除元素并返回该元素的值 * Queue.get_nowait()：等价于 get(False) * Queue.task_done()：发送信号表明入列任务已经完成，经常在消费者线程中用到 * Queue.join()：阻塞直至队列中所有的元素处理完毕 首先 Queue 中的队列和 collections.deque 所表示的队列并不一样，前者用于不同线程之间的通信，内部实现了线程的锁机制，后者是数据结构上的概念，支持 in 方法。 Queue 模块实现了多个生产者多个消费者的队列，当多线程之间需要信息安全的交换的时候特别有用，因此这个模块实现了所需要的锁原语，为 Python 多线程编程提供了有力的支持，它是线程安全的。 先来看一个简单的例子： ```python import os import Queue import threading import urllib2 class DownloadThread(threading.Thead): def __init__(self, queue): threading.Thread.__init__(self) self.queue = queue def run(self): while True: url = self.queue.get() print('&#123;0&#125; begin download &#123;1&#125;...'.format(self.name, url)) self.download_file(url) self.queque.task_done() print('&#123;0&#125; download completed!!!'.format(self.name)) def download_file(self, url): urlhandler = urllib2.urlopen(url) fname = os.path.basename(url) + '.html' with open(fname, 'wb') as f: while True: chunk = urlhandler.read(1024) if not chunk: break f.write(chunk) if __name__ == '__main__': urls = ['http://wiki.python.org/moin/WebProgramming', 'https://www.createspace.com/3611970', 'http://wiki.python.org/moin/Documentation' ] queue = Queue.Queue() for i range(5): t = DownloadThread(queue) t.setDaemon(True) t.start() for url in urls: queue.put(url) queue.join() ``` ## 第 5 章 设计模式 ### 建议 50：利用模块实现单例模式 单例模式可以保证徐彤中一个类只有一个实例且该实例易被外界访问，常用来使用XxxManager之类的功能。 满足单例模式的 3 个需求： * 只能有一个实例 * 必须自行创建这个实例 * 必须自行向整个系统提供这个实例 模块采用的其实是天然的单例的实现方式，在入口文件导入： * 所有的变量都会绑定到模块 * 模块只初始化一次 * import 机制是线程安全的，保证了在并发状态下模块也只是一个实例 ```python # World.py import Sun def run(): while True: Sun.rise() Sun.set() # main.py import World World.run() ``` 此外，Borg模式可以创造任意数量实例，并保证状态共享。 ### 建议 51：用 mixin 模式让程序更加灵活 模板方法模式就是在一个方法中定义一个算法的骨架，并将一些实现步骤延迟到子类中。模板方法可以使子类在不改变算法结构的情况下，重新定义算法中的某些步骤。 ```python class UseSimpleTeapot(object): def get_teapot(self): return SimpleTeapot() class UseKungfuTeapot(object): def get_teapot(self): return KungfuTeapot() class OfficePeople(People, UseSimpleTeapot): pass class HomePeople(People, UseSimpleTeapot): pass class Boss(People, UseKungfuTeapot): pass def simple_tea_people(): people = People() people.__base__ += (UseSimpleTeapot,) return people def coffee_people(): people = People() people.__base__ += (UseCoffeepot,) def tea_and_coffee_people(): people = People() people.__base__ += (UseSimpleTeapot, UserCoffeepot,) return people def boss(): people = People() people.__base__ += (KungfuTeapot, UseCoffeepot, ) return people ``` 代码的原理在于每个类都有一个__bases__属性，它是一个元组，用来存放所有的基类，作为动态语言，Python 中的基类可以在运行中可以动态改变。所以当我们向其中增加新的基类时，这个类就拥有了新的方法，这就是混入mixin。 利用这个技术我们可以在不修改代码的情况下就可以完成需求： ```python import mixins # 把员工需求定义在 Mixin 中放在 mixins 模块 def staff(): people = People() bases = [] for i in config.checked(): bases.append(getattr(maxins, i)) people.__base__ += tuple(bases) return people ``` ### 建议 52：用发布订阅模式实现松耦合 发布订阅模式是一种编程模式，消息的发送者不会发送其消息给特定的接收者，而是将发布的消息分为不同的类别直接发布，并不关注订阅者是谁。而订阅者可以对一个或多个类别感兴趣，且只接收感兴趣的消息，并且不关注是哪个发布者发布的消息。要实现这个模式，就需要一个中间代理人。 Broker，它维护着发布者和订阅者的关系，订阅者把感兴趣的主题告诉它，而发布者的信息也通过它路由到各个订阅者处。 ```python from collections import defaultdict route_table = defaultdict(list) def sub(topic, callback): if callback in route_table[topic]: return route_table[topic].append(callback) def pub(topic, *args, **kw): for func in route_table[topic]: func(*args, **kw) ``` 将以上代码放在 Broker.py 的模块，省去了各种参数检测、优先处理、取消订阅的需求，只向我们展示发布订阅模式的基础实现： ```pythonimport Brokerdef greeting(name): print('Hello, &#123;&#125;'.format(name)) Broker.sub('greet', greeting)Broker.pub('greet', 'LaiYonghao') 因为python-message的消息订阅默认是全局性的，所以有可能产生名字冲突。 建议 53：用状态模式美化代码 所谓状态模式，就是当一个对象的内在状态改变时允许改变其行为，但这个对象看起来像是改变了其类。 简单的状态模式有其缺点： 查询对象的当前状态很麻烦 状态切换时需要对原状态做一些清扫工作，而对新状态做初始化工作，因每个状态需要做的事情不同，全部写在切换状态的代码中必然重复 这时候我们可以使用 Python-state 来解决。 1234567891011121314151617181920from state import curr, switch, stateful, State, behavior@statefulclass People(object): class Workday(State): default = True @behavior # 相当于staticmethod def day(self): # 这里的self并不是Python的关键字，而是有助于我们理解状态类的宿主是People的实例 print('work hard') class Weekend(State): @behavior def day(self): print('play harder')people = People()while True: for i in range(1, 8): if i == 6: switch(people, People.Weekend) if i == 1: switch(people, People.Workday) people.day() @statefule装饰器重载了被修饰的类的__getattr__()从而使得 People 的实例能够调用当前状态类的方法，同时被修饰的类的实例是带有状态的，能够使用curr()查询当前状态，也可以使用switch()进行状态切换，默认的状态是通过类定义的 default 属性标识，default = True的类成为默认状态。 状态类 Workday 和 Weekend 继承自 State 类，从其派生的子类可以使用__begin__和__end___状态转换协议，自定义进入和离开当前状态时对宿主的初始化和清理工作。 下面是一个真实业务的例子： 12345678910111213@statefulclass User(object): class NeedSignin(State): default = True @behavior def signin(self, user, pwd): ... switch(self, Player.Signin) class Signin(State): @behavior def move(self, dst): ... @behavior def atk(self, other): ... 第 6 章 内部机制 建议 54：理解 built-in objects Python 中一切皆对象，在新式类中，object 是所有内建类型的基类，用户自定义的类可以继承自 object 也可继承自内建类型。 1234567891011121314151617In [1]: class TestNewClass: ...: __metaclass__ = type ...: In [2]: type(TestNewClass)Out[2]: typeIn [3]: TestNewClass.__bases__Out[3]: (object,)In [4]: a = TestNewClass()In [5]: type(a)Out[5]: __main__.TestNewClassIn [6]: a.__class__Out[6]: __main__.TestNewClass 新式类支持 property 和描述符特性，作为新式类的祖先，Object 类还定义了一些特殊方法：new()、init()、delattr()、getattribute()、setattr()、hash()、repr()、str()等。 建议 55：init()不是构造方法 123456789101112131415class A(object): def __new__(cls, *args, **kw): print(cls) print(args) print(kw) print('----------') instance = object.__new__(cls, *args, **kw) print(instance) def __init__(self, a, b): print('init gets called') print('self is &#123;&#125;'.format(self)) self.a, self.b = a, ba1 = A(1, 2)print(a1.a)print(a1.b) 运行结果： 12345678910&lt;class '__main__.A'&gt;(1, 2)&#123;&#125;----------Traceback (most recent call last): File \"test.py\", line 19, in &lt;module&gt; a1 = A(1, 2) File \"test.py\", line 13, in __new__ instance = object.__new__(cls, *args, **kw)TypeError: object() takes no parameters 从结果中我们可以看出，程序输出了__new__()调用所产生的输出，并抛出了异常。于是我们知道，原来__new__()才是真正创建实例，是类的构造方法，而__init__()是在类的对象创建好之后进行变量的初始化。上面程序抛出异常是因为在__new__()中没有显式返回对象，a1此时为None，当去访问实例属性时就抛出了异常。 根据官方文档，我们可以总结以下几点： object.new(cls[, args…])：其中 cls 代表类，args 为参数列表，为静态方法 object.init(self[, args…])：其中 self 代表实例对象，args 为参数列表，为实例方法 控制实例创建的时候可使用 new() ，而控制实例初始化的时候使用 init() new()需要返回类的对象，当返回类的对象时将会自动调用__init__()进行初始化，没有对象返回，则__init__()不会被调用。init() 方法不需要显示返回，默认为 None，否则会在运行时抛出 TypeError 但当子类继承自不可变类型，如 str、int、unicode 或者 tuple 的时候，往往需要覆盖__new__() 覆盖 new() 和 init() 的时候这两个方法的参数必须保持一致，如果不一致将导致异常 下面我们来总结需要覆盖__new__()的几种特殊情况： 当类继承不可变类型且默认的 new() 方法不能满足需求的时候 用来实现工厂模式或者单例模式或者进行元类编程，使用__new__()来控制对象创建 作为用来初始化的 init() 方法在多继承的情况下，子类的 init()方法如果不显式调用父类的 init() 方法，则父类的 init() 方法不会被调用；通过super(子类， self).init()显式调用父类的初始化方法；对于多继承的情况，我们可以通过迭代子类的 bases 属性中的内容来逐一调用父类的初始化方法 分别来看例子加深理解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 创建一个集合能够将任何以空格隔开的字符串变为集合中的元素class UserSet(frozenset): def __new__(cls, *args): if args and isinstance(args[0], str): args = (args[0].split(), ) + args[1:] return super(UserSet, cls).__new__(cls, *args)# 一个工厂类根据传入的参量决定创建出哪一种产品类的实例class Shape(object): def __init__(object): pass def draw(self): passclass Triangle(Shape): def __init__(self): print(\"I am a triangle\") def draw(self): print(\"I am drawing triangle\")class Rectangle(Shape): def __init__(self): print(\"I am a rectangle\") def draw(self): print(\"I am drawing triangle\")class Trapezoid(Shape): def __init__(self): print(\"I am a trapezoid\") def draw(self): print(\"I am drawing triangle\")class Diamond(Shape): def __init__(self): print(\"I am a diamond\") def draw(self): print(\"I am drawing triangle\")class ShapeFactory(object): shapes = &#123;'triangle': Triangle, 'rectangle': Rectangle, 'trapzoid': Trapezoid, 'diamond': Diamond&#125; def __new__(cls, name): if name in ShapeFactory.shapes.keys(): print('creating a new shape &#123;&#125;'.format(name)) return ShapeFactory.shapes[name]() else: print('creating a new shape &#123;&#125;'.format(name)) return Shape() 建议 56：理解名字查找机制 在 Python 中所谓的变量其实都是名字，这些名字指向一个或多个 Python 对象。这些名字都存在于一个表中（命名空间），我们称之为局部变量，调用locals()可以查看： 1234&gt;&gt;&gt; locals()&#123;'__package__': None, '__spec__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__doc__': None, '__name__': '__main__', '__builtins__': &lt;module 'builtins' (built-in)&gt;&#125;&gt;&gt;&gt; globals()&#123;'__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '__package__': None, '__doc__': None, '__spec__': None, '__name__': '__main__'&#125; Python 中的作用域分为： 局部作用域: 一般来说函数的每次调用都会创建一个新的本地作用域, 拥有新的命名空间 全局作用域: 定义在 Python 模块文件中的变量名拥有全局作用域, 即在一个文件的顶层的变量名仅在这个文件内可见 嵌套作用域: 多重函数嵌套时才会考虑, 即使使用 global 进行申明也不能达到目的, 其结果最终是在嵌套的函数所在的命名空间中创建了一个新的变量 内置作用域: 通过标准库中的__builtin__实现的 当访问一个变量的时候，其查找顺序遵循变量解析机制 LEGB 法则，即依次搜索 4 个作用域：局部作用域、嵌套作用域、全局作用域以及内置作用域，并在第一个找到的地方停止搜寻，如果没有搜到，则会抛出异常。 Python 3 中引入了 nonlocal 关键字: 12345678def foo(x): a = x def bar(): nonlocal a b = a * 2 a = b + 1 print(a) return bar 建议 57: 为什么需要 self 参数 在类中当定义实例方法的时候需要将第一个参数显式声明为self, 而调用时不需要传入该参数, 我们通过self.x访问实例变量, self.m()访问实例方法: 1234567891011class SelfTest(object): def __init__(self.name): self.name = name def showself(self): print('self here is &#123;&#125;'.format(self)) def display(self): self.showself() print('The name is: &#123;&#125;'.format(self.name))st = SelfTest('instance self')st.display()print('&#123;&#125;'.format(st)) 运行结果: 123self here is &lt;__main__.SelfTest object at 0x7f440c53ba58&gt;The name is: instance self&lt;__main__.SelfTest object at 0x7f440c53ba58&gt; 从中可以发现, self 表示实例对象本身, 即 SelfTest 类的对象在内存中的地址. self 是对对象 st 本身的引用, 我们在调用实例方法时也可以直接传入实例对象: SelfTest.display(st). 同时 self 或 cls 并不是 Python 的关键字, 可以替换成其它的名称。 Python 中为什么需要 self 呢: 借鉴了其他语言的特征 Python 语言本身的动态性决定了使用 self 能够带来一定便利 在存在同名的局部变量以及实例变量的情况下使用 self 使得实例变量更容易被区分 Python 属于一级对象语言, 我们有好几种方法可以引用类方法: 12A.__dict__[\"m\"]A.m.__func__ Python 的哲学是：显示优于隐式（Explicit is better than implicit）。 建议 58: 理解 MRO 与多继承 古典类与新式类所采取的 MRO (Method Resolution Order, 方法解析顺序) 的实现方式存在差异。 古典类是按照多继承申明的顺序形成继承树结构, 自顶向下采用深度优先的搜索顺序. 而新式类采用的是 C3 MRO 搜索方法, 在新式类通过__mro__得到 MRO 的搜索顺序, C3 MRO 的算法描述如下: 假定，C1C2…CN 表示类 C1 到 CN 的序列，其中序列头部元素（head）=C1，序列尾部（tail）定义 = C2…CN； C 继承的基类自左向右分别表示为 B1，B2…BN L[C] 表示 C 的线性继承关系，其中 L[object] = object。 算法具体过程如下： L[C(B1…BN)] = C + merge(L[B1] … L[BN], B1 … BN) 其中 merge 方法的计算规则如下：在 L[B1]…L[BN]，B1…BN 中，取 L[B1] 的 head，如果该元素不在 L[B2]…L[BN]，B1…BN 的尾部序列中，则添加该元素到 C 的线性继承序列中，同时将该元素从所有列表中删除（该头元素也叫 good head），否则取 L[B2] 的 head。继续相同的判断，直到整个列表为空或者没有办法找到任何符合要求的头元素（此时，将引发一个异常）。 菱形继承是我们在多继承设计的时候需要尽量避免的一个问题。 建议 59: 理解描述符机制 123456789101112131415161718192021222324252627282930313233343536373839In [1]: class MyClass(object): ...: class_attr = 1 ...: # 每一个类都有一个__dict__属性, 包含它的所有属性In [2]: MyClass.__dict__Out[2]:mappingproxy(&#123;'__dict__': &lt;attribute '__dict__' of 'MyClass' objects&gt;, '__doc__': None, '__module__': '__main__', '__weakref__': &lt;attribute '__weakref__' of 'MyClass' objects&gt;, 'class_attr': 1&#125;)In [3]: my_instance = MyClass()# 每一个实例也相应有一个实例属性, 我们通过实例访问一个属性时,# 它首先会尝试在实例属性中查找, 找不到会到类属性中查找In [4]: my_instance.__dict__Out[4]: &#123;&#125;# 实例访问类属性In [5]: my_instance.class_attrOut[5]: 1# 如果通过实例增加一个属性,只能改变此实例的属性In [6]: my_instance.inst_attr = 'china'In [7]: my_instance.__dict__Out[7]: &#123;'inst_attr': 'china'&#125;# 对于类属性而言并没有丝毫变化In [8]: MyClass.__dict__Out[8]:mappingproxy(&#123;'__dict__': &lt;attribute '__dict__' of 'MyClass' objects&gt;, '__doc__': None, '__module__': '__main__', '__weakref__': &lt;attribute '__weakref__' of 'MyClass' objects&gt;, 'class_attr': 1&#125;)# 我们可以动态地给类增加一个属性In [9]: MyClass.class_attr2 = 100In [10]: my_instance.class_attr2Out[10]: 100# 但Python的内置类型并不能随意地为它增加属性或方法 .操作符封装了对实例属性和类属性两种不同属性进行查找的细节。 但是如果是访问方法呢: 12345678910111213141516In [1]: class MyClass(object): ...: def my_method(self): ...: print('my_method') ...: In [2]: MyClass.__dict__['my_method']Out[2]: &lt;function __main__.MyClass.my_method&gt;In [3]: MyClass.my_methodOut[3]: &lt;function __main__.MyClass.my_method&gt;In [4]: type(MyClass.my_method)Out[4]: functionIn [5]: type(MyClass.__dict__['my_method'])Out[5]: function 根据通过实例访问属性和根据类访问属性的不同，有以下两种情况： 一种是通过实例访问，比如代码 obj.x，如果 x 是一个描述符，那么 getattribute() 会返回 type(obj).dict[‘x’].get(obj, type(obj)) 结果，即：type(obj) 获取 obj 的类型；type(obj).dict[‘x’] 返回的是一个描述符，这里有一个试探和判断的过程；最后调用这个描述符的 get() 方法。 另一个是通过类访问的情况，比如代码 cls.x，则会被 getattribute()转换为 cls.dict[‘x’].get(None, cls)。 描述符协议是一个 Duck Typing 的协议，而每一个函数都有 get 方法，也就是说其他每一个函数都是描述符。所有对属性, 方法进行修饰的方案往往都用到了描述符, 如classmethod, staticmethod, property等, 以下是property的参考实现: 123456789101112131415161718192021class Property(object): \"Emulate PyProperty_Type() in Objects/descrobject.c\" def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError, \"unreadable attribute\" return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError, \"can't set attribute\" self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError, \"can't delete attribute\" self.fdel(obj) 建议 60：区别__getattr__()和__getattribute__()方法 以上两种方法可以对实例属性进行获取和拦截： getattr(self, name)：适用于属性在实例中以及对应的类的基类以及祖先类中都不存在； getattribute(self, name)：对于所有属性的访问都会调用该方法。 但访问不存在的实例属性时，会由内部方法__getattribute__()抛出一个 AttributeError 异常，也就是说只要涉及实例属性的访问就会调用该方法，它要么返回实际的值，要么抛出异常。详情请参考。 那么__getattr__()在什么时候调用呢： 属性不在实例的__dict__中； 属性不在其基类以及祖先类的__dict__中； 触发AttributeError异常时（注意，不仅仅是__getattribute__()方法的AttributeError异常，property 中定义的get()方法抛出异常的时候也会调用该方法）。 当这两个方法同时被定义的时候，要么在__getattribute__()中显式调用，要么触发AttributeError异常，否则__getattr__()永远不会被调用。 我们知道 property 也能控制属性的访问，如果一个类中如果定义了 property、getattribute()以及__getattr__()来对属性进行访问控制，会最先搜索__getattribute__()方法，由于 property 对象并不存在于 dict 中，因此并不能返回该方法，此时会搜索 property 中的get()方法；当 property 中的set()方法对属性进行修改并再次访问 property 的get()方法会抛出异常，这时会触发__getattr__()的调用。 getattribute()总会被调用，而__getattr__()只有在__getattribute__()中引发异常的情况下调用。","tags":[]},{"title":"改善 Python 程序的 91 个建议读书笔记 2","date":"2017-05-26T15:14:29.000Z","path":"2017/05/26/改善 Python 程序的 91 个建议读书笔记 2/","text":"建议 21： i+=1 不等于 ++i ++i 合法，但是无效 建议 22：使用 with 自动关闭资源 对于打开的资源我们记得关闭它，如文件、数据库连接等，Python 提供了一种简单优雅的解决方案：with。 with的实现得益于一个称为上下文管理器(context manager)的东西，它定义程序运行时需要建立的上下文，处理程序的进入和退出，实现了上下文管理协议，即对象中定义了__enter__()和__exit__()，任何实现了上下文协议的对象都可以称为一个上下文管理器： enter()：返回运行时上下文相关的对象 exit(exception_type, exception_value, traceback)：退出运行时的上下文，处理异常、清理现场等 包含with语句的代码块执行过程如下： 12345678&gt;&gt;&gt; with open('test.txt', 'w') as f:... f.write('test')... 4&gt;&gt;&gt; f.__enter__&lt;built-in method __enter__ of _io.TextIOWrapper object at 0x7f1b967aaa68&gt;&gt;&gt;&gt; f.__exit__&lt;built-in method __exit__ of _io.TextIOWrapper object at 0x7f1b967aaa68&gt; 计算表达式的值，返回一个上下文管理器对象。 加载上下文管理器对象的__exit__()以备后用。 调用上下文管理器对象的__enter__()。 将__enter__()的返回值赋给目标对象。 执行代码块，正常结束调用__exit__()，其返回值直接忽略，如果发生异常，会调用__exit__()并将异常类型、值及 traceback 作为参数传递给__exit__()，exit()返回值为 false 异常将会重新抛出，返回值为 true 异常将被挂起，程序继续执行。 Python 还提供 contextlib 模块，通过 Generator 实现，其中的 contextmanager 作为装饰器来提供一种针对函数级别上的上下文管理器，可以直接作用于函数/对象而不必关心__enter__()和__exit__()的实现。 推荐文章 建议 23：使用 else 子句简化循环（异常处理） python 的 else 子句在循环正常结束和循环条件不成立时被执行，由 break 语句中断时不执行，同样，我们可以利用这颗语法糖作用在 while 和 try…except 中。 建议 24：遵循异常处理的几点基本原则 异常处理的几点原则： 1. 注意异常的粒度，不推荐在 try 中放入过多的代码。 2. 谨慎使用单独的 except 语句处理所有异常，最好能定位具体的异常。 3. 注意异常捕获的顺序，在适合的层次处理异常，Python 是按内建异常类的继承结构处理异常的，所以推荐的做法是将继承结构中子类异常在前抛出，父类异常在后抛出。 4. 使用更为友好的异常信息，遵守异常参数的规范。 建议 25：避免 finally 中可能发生的陷阱 当 finally 执行完毕时，之前临时保存的异常将会再次被抛出，但如果 finally 语句中产生了新的异常或执行了 return 或 break 语句，那么临时保存的异常将会被丢失，从而异常被屏蔽。 在实际开发中不推荐 finally 中使用 return 语句进行返回。 建议 26：深入理解 None，正确判断对象是否为空 （None被判断为False，但是空集不等于None） 类型FalseTrue布尔False （与0等价）True （与1等价）字符串“”（ 空字符串）非空字符串，例如 &quot; “,”blog“数值0, 0.0非0的数值，例如：1, 0.1, -1, 2容器[], (), {}, set()至少有一个元素的容器对象，例如：[0], (None,), [’’]NoneNone非None对象。 &gt;&gt;&gt; id(None) 10743840 &gt;&gt;&gt; a = None &gt;&gt;&gt; id(a) 10743840 &gt;&gt;&gt; l = [] &gt;&gt;&gt; if l is not None: # 判断逻辑 l 不为空 ... print(&#39;l is {}&#39;.format(l)) ... else: ... print(&#39;l is empty&#39;) ... l is [] &gt;&gt;&gt; if l: # #3 正确的判断形式 ... print(&#39;Do something...&#39;) ... else: ... print(&#39;Do other thing...&#39;) ... Do other thing... 执行中会调用__nonzero__()来判断自身对象是否为空并返回0/1或True/False，如果没有定义该方法，Python 将调用__len__()进行判断，返回 0 表示为空。如果一个类既没有定义__len__()又没有定义__nonzero__()，该类实例用 if 判断为True。 建议 27：连接字符串优先使用 join 而不是 + 连接字符串使用join将使程序性能更佳，原因是使用每次使用 + 都需要格外分一块内存去存储结果。 建议 28：格式化字符串时尽量使用 .format 而不是 % format方法总结 使用 format 格式化字符串有以下好处： format更为灵活，参数顺序和格式不必完全相同 format更为方便的作为参数传递（例如支持列表的索引操作） %最终会被format取代 %容易抛出异常，而format则不会（未尝是好事） 建议 29：区别对待可变对象和不可变对象 Python 中一切皆对象，每个对象都有一个唯一的标识符（id）、类型（type）和值。数字、字符串、元组属于不可变对象，字典、列表、字节数组属于可变对象。 默认参数在初始化时仅仅被评估一次，以后直接使用第一次评估的结果，course 指向的是 list 的地址，每次操作的实际上是 list 所指向的具体列表，所以对于可变对象的更改会直接影响原对象。 最好的方法是传入None作为默认参数，在创建对象的时候动态生成列表。 &gt;&gt;&gt; list1 = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] &gt;&gt;&gt; list2 = list1 &gt;&gt;&gt; list1.append(&#39;d&#39;) &gt;&gt;&gt; list2 [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] &gt;&gt;&gt; list3 = list1[:] # 可变对象的切片操作相当于浅拷贝 &gt;&gt;&gt; list3.remove(&#39;a&#39;) &gt;&gt;&gt; list3 [&#39;b&#39;, &#39;c&#39;, &#39;d&#39;] &gt;&gt;&gt; list1 [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 建议 30：[]、() 和 {} 一致的容器初始化形式 使用列表解析、字典解析、元组解析等替代for循环 解析式有以下好处： 代码更清晰、简洁 效率更高、速度更快 （代码更加pythonic） 建议 31：记住函数传参既不是传值也不是传引用 正确的说法是传对象（call by object）或传对象的引用（call-by-object-reference），函数参数在传递过程中将整个对象传入，对可变对象的修改在函数外部以及内部都可见，对不可变对象的”修改“往往是通过生成一个新对象然是赋值实现的。 建议 32：警惕默认参数潜在的问题 其中就是默认参数如果是可变对象，在调用者和被调用者之间是共享的。 所以默认值使用可以使用数字、字符串、元组。 不可以使用字典、列表、字节数组。 import time # 对当前系统时间进行处理 def report(when=time.time): # 而不是when=time.time() pass 建议 33：慎用变长参数 原因如下： 1. 使用过于灵活，导致函数签名不够清晰，存在多种调用方式 2. 使用*args和**kw简化函数定义就意味着函数可以有更好的实现方法 使用场景： 1. 为函数添加一个装饰器 2. 参数数目不确定 3. 实现函数的多态或子类需要调用父类的某些方法时 建议 34：深入理解 str() 和repr() 的区别 （str方法面向用户更为友好，repr解释更加清晰） 总结几点： str()面向用户，返回用户友好和可读性强的字符串类型；repr()面向 Python 解释器或开发人员，返回 Python 解释器内部的含义。 解释器中输入a默认调用repr()，而print(a)默认调用str()。 repr()返回值一般可以用eval()还原对象：obj == eval(repr(obj))。 以上两个方法分别调用内建的__str__()和__repr__()，一般来说类中都应该定义__repr__()，但当可读性比准确性更为重要时应该考虑__str__()，用户实现__repr__()方法的时候最好保证其返回值可以用eval()是对象还原。 建议 35：分清 staticmethod 和 classmethod 的适用场景 （需要返回类的实例时，或需要动态生成对应类的类变量，使用classmethod，方法不跟实例与类相关（不适用self和cls），定义为静态方法（工具方法）） 调用类方法装饰器的修饰器的方法，会隐式地传入该对象所对应的类，可以动态生成对应的类的类变量，同时如果我们期望根据不同的类型返回对应的类的实例，类方法才是正确的解决方案。 反观静态方法，当我们所定义的方法既不跟特定的实例相关也不跟特定的类相关，可以将其定义为静态方法，这样使我们的代码能够有效地组织起来，提高可维护性。 当然，也可以考虑定义一个模块，将一组的方法放入其中，通过模块来访问。 第 4 章 库 建议 36：掌握字符串的基本用法 # 小技巧：Python 遇到未闭合的小括号会自动将多行代码拼接为一行 &gt;&gt;&gt; s = (&#39;SELECT * &#39; ... &#39;FROM table &#39; ... &#39;WHERE field=&quot;value&quot;&#39;) &gt;&gt;&gt; s &#39;SELECT * FROM table WHERE field=&quot;value&quot;&#39; # Python2 中使用 basestring 正确判断一个变量是否是字符串 # 性质判断 isalnum() isalpha() isdigit() islower() isupper() isspace() istitle() # 查找替换 startswith(prefix[, start[, end]]) endswith(suffix[, start[, end]]) # prefix参数可以接收 tuple 类型的实参 count(sub[, start[, end]]) find(sub[, start[, end]]) index(sub[, start[, end]]) rfind(sub[, start[, end]]) rindex(sub[, start[, end]]) replace(old, new[, count]) # count是指的替换次数，不指定就全部替换 # 切分 partition(sep) rpartition(sep) splitlines([keepends]) split([sep, [, maxsplit]]) rsplit([sep[, maxsplit]]) # partition 返回一个3个元素的元组对象 # 变形 lower() upper() capitalize() swapcase() title() # 删减填充 strip([chars]) lstrip([chars]) rstrip([chars]) # 没有提供chars默认是空白符，由string.whitespace 常量定义 center(width[, fillchar]) ljuct(width[, fillchar]) rjust(width[, fillchar]) zfill(width) expandtabs([tabszie]) 下面来介绍一些易混淆的地方： &gt;&gt;&gt; &#39; hello world&#39;.split() [&#39;hello&#39;, &#39;world&#39;] &gt;&gt;&gt; &#39; hello world&#39;.split(&#39; &#39;) [&#39;&#39;, &#39;&#39;, &#39;hello&#39;, &#39;world&#39;] &gt;&gt;&gt; &#39;hello wORld&#39;.title() &#39;Hello World&#39; &gt;&gt;&gt; import string &gt;&gt;&gt; string.capwords(&#39; hello world!&#39;) &#39;Hello World!&#39; &gt;&gt;&gt; string.whitespace &#39; \\t\\n\\r\\x0b\\x0c&#39; 建议 37：按需选择 sort() 或者 sorted() （sort方法是原地操作，sorted是复制操作，不需要保留源列表用sort） # 函数原型 sorted(iterable[, cmp[, key[, reverse]]]) # 返回一个排序后的列表 s.sort([cmp[, key[, reverse]]]) # 直接修改原列表，返回为None &gt;&gt;&gt; persons = [{&#39;name&#39;: &#39;Jon&#39;, &#39;age&#39;: 32}, {&#39;name&#39;: &#39;Alan&#39;, &#39;age&#39;: 50}, {&#39;name&#39;: &#39;Bob&#39;, &#39;age&#39;: 23}] &gt;&gt;&gt; sorted(persons, key=lambda x: (x[&#39;name&#39;], -x[&#39;age&#39;])) [{&#39;name&#39;: &#39;Alan&#39;, &#39;age&#39;: 50}, {&#39;name&#39;: &#39;Bob&#39;, &#39;age&#39;: 23}, {&#39;name&#39;: &#39;Jon&#39;, &#39;age&#39;: 32}] &gt;&gt;&gt; a = (1, 2, 4, 2, 3) &gt;&gt;&gt; sorted(a) [1, 2, 2, 3, 4] 所以如果实际过程中需要保留原有列表，可以使用sorted()。sort()不需要复制原有列表，消耗内存较小，效率较高。同时传入参数key比传入参数cmp效率要高，cmp传入的函数在整个排序过程中会调用多次，而key针对每个元素仅作一次处理。 建议 38：使用 copy 模块深拷贝对象 （对可变对象需要真正意义上的复制时使用copy.deepcopy，这种需求情况还是比较少见） 浅拷贝（shallow copy）：构造一个新的复合对象并将从原对象中发现的引用插入该对象中。工厂函数、切片操作、copy 模块中的 copy 操作都是浅拷贝 深拷贝（deep copy）：针对引用所指向的对象继续执行拷贝，因此产生的对象不受其它引用对象操作的影响。深拷贝需要依赖 copy 模块的 deepcopy() 操作 在 python 中，标识一个对象唯一身份的是：对象的id(内存地址)，对象类型，对象值，而浅拷贝就是创建一个具有相同类型，相同值但不同id的新对象。因此使用浅拷贝的典型使用场景是：对象自身发生改变的同时需要保持对象中的值完全相同，比如 list 排序： def sorted_list(olist, key=None): copied_list = copy.copy(olist) copied_list.sort(key=key) return copied_list a = [3, 2, 1] # [3, 2, 1] b = sorted_list(a) # [1, 2, 3] 深拷贝不仅仅拷贝了原始对象自身，也对其包含的值进行拷贝，它会递归的查找对象中包含的其他对象的引用，来完成更深层次拷贝。因此，深拷贝产生的副本可以随意修改而不需要担心会引起原始值的改变： &gt;&gt;&gt; a = [1, 2] &gt;&gt;&gt; b = [a, a] &gt;&gt;&gt; b [[1, 2], [1, 2]] &gt;&gt;&gt; from copy import deepcopy &gt;&gt;&gt; c = deepcopy(b) &gt;&gt;&gt; id(b[0]) == id(c[0]) False &gt;&gt;&gt; id(b[0]) == id(b[1]) True &gt;&gt;&gt; c [[1, 2], [1, 2]] &gt;&gt;&gt; c[0].append(3) &gt;&gt;&gt; c [[1, 2, 3], [1, 2, 3]] 使用 copy 和 deepcopy 可以完成对一个对象拷贝的定制。 参考博文 建议 39： 使用 Counter 进行计数统计 （需要计数统计时，使用Counter） 常见的计数统计可以使用dict、defaultdict、set和list，不过 Python 提供了一个更优雅的方式： &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; some_data = {&#39;a&#39;, &#39;2&#39;, 2, 3, 5, &#39;c&#39;, &#39;7&#39;, 4, 5, &#39;d&#39;, &#39;b&#39;} &gt;&gt;&gt; Counter(some_data) Counter({&#39;7&#39;，: 1, 2: 1, 3: 1, 4: 1, 5: 1, &#39;2&#39;: 1, &#39;b&#39;: 1, &#39;a&#39;: 1, &#39;d&#39;: 1, &#39;c&#39;: 1}) Counter 类属于字典类的子类，是一个容器对象，用来统计散列对象，支持+、-、&amp;、|，其中&amp;和|分别返回两个 Counter 对象各元素的最小值和最大值。 # 初始化 Counter(&#39;success&#39;) Counter(s=3, c=2, e=1, u=1) Counter({&#39;s&#39;: 3, &#39;c&#39;: 2, &#39;u&#39;: 1, &#39;e&#39;: 1}) # 常用方法 list(Counter(some_data).elements()) # 获取 key 值 Counter(some_data).most_common(2) # 前 N 个出现频率最高的元素以及对应的次数 (Counter(some_data))[&#39;y&#39;] # 访问不存在的元素返回 0 c = Counter(&#39;success&#39;) c.update(&#39;successfully&#39;) # 更新统计值 c.subtract(&#39;successfully&#39;) # 统计数相减，允许为0或为负 建议 40：深入掌握 ConfigParser （啥程序都需要配置，要搞懂配置库） 几乎所有的应用程序都会读取配置文件，ini是一种比较常见的文件格式： [section1] option1=0 Python 提供标准库 ConfigParser 来支持它： import ConfigParser conf = ConfigParser.ConfigParser() conf.read(&#39;example.conf&#39;) print(conf.get(&#39;section1&#39;, &#39;in_default&#39;)) 再来看个SQLAlchemy配置文件的例子： [DEFAULT] conn_str = %(dbn)s://%(user)s:%(pw)s@%(host)s:%(port)s/%(db)s dbn = mysql user = root host = localhost port = 3306 [db1] user = aaa pw = ppp db = example [db2] host = 192.168.0.110 pw = www db = example import ConfigParser conf = ConfigParser.ConfigParser() conf.read(&#39;format.conf&#39;) print(conf.get(&#39;db1&#39;, &#39;conn_str&#39;)) print(conf.get(&#39;db2&#39;, &#39;conn_str&#39;))","tags":[]},{"title":"解决不了问题就崩了心态太不应该","date":"2017-05-19T06:08:01.000Z","path":"2017/05/19/解决不了问题就崩了心态太不应该/","text":"对于大多数事情来说，失败是常态，无法掌控事情的发展是常态，缺乏安全感的弊端就是恐惧面对这些。问题是越不去面对，挫折固然少了，无法掌控的事情却更多了。要学会坦然面对无法掌控的事情，磨练出更优秀的自己。","tags":[]},{"title":"改善 Python 程序的 91 个建议读书笔记 1","date":"2017-05-15T15:11:31.000Z","path":"2017/05/15/改善 Python 程序的 91 个建议读书笔记 1/","text":"第 1 章 引论 建议 1：理解 Pythonic 概念 Pythonic 当你输入 import this 就会显示 zen of python 美丽胜于丑陋。 显式优于隐式。 简单比复杂好。 复合胜于复杂。 平面比嵌套好。 稀疏比密集好。 可读性是重要的。 特殊情况不足以打破规则。 虽然实用性胜过纯粹。 除了显示错误，错误永远不应该沉默。 代码风格 充分体现python动态语言的特色，类似于 # 变量交换 a, b = b, a # 上下文管理 with open(path, &#39;r&#39;) as f: do_sth_with(f) # 不应当过分地追求奇技淫巧 a = [1, 2, 3, 4] a[::-1] # 不推荐。好吧，自从学了切片我一直用的这个 list(reversed(a)) # 推荐 然后表扬了 Flask 框架，提到了 generator 之类的特性尤为 Pythonic，有个包和模块的约束： 包和模块的命名采用小写、单数形式，而且短小 包通常仅作为命名空间，如只含空的__init__.py文件 建议 2：编写 Pythonic 代码 避免劣化代码 避免只用大小写区分不同的对象 避免使用容易引起混淆的名称 不要害怕过长的变量名 深入认识python有助于编写pythonic代码 全面掌握 python 提供的特性，包括语言和库 随着时间推移，要不断更新知识 深入学习业界公认的 pythoni 代码 编写符合 pep8 的代码规范（就是让你使用pycharm） 建议 3：理解 Python 与 C 语言的不同之处 Python 使用代码缩进的方式来分割代码块，不要混用 Tab 键和空格 Python 中单、双引号的效果相同（个人建议使用单引号，在面对其他语言的双引号源码时不必再转义） 三元操作符：x if bool else y（原因是作者认为应该用可读性更好的方式表达） 用其他方法替代 switch-case 建议 4：在代码中适当添加注释 块和行注释仅仅注释复杂的操作、算法等 注释和代码隔开一段距离 给外部可访问的函数和方法添加文档注释 推荐在文件头中包含 copyright 申明、模块描述等 另外，编写代码应该朝代码即文档的方向进行，但仍应该注重注释的使用 建议 5：通过适当添加空行使代码布局更为优雅、合理 表达完一个完整思路后，应该用空白行间隔，尽量不要在一段代码中说明几件事。 尽量保持上下文的易理解性，比如调用者在上，被调用者在下 避免过长的代码行，超过80个字符应该使用行连接换行（还是让你使用pycharm） 水平对齐毫无意义，不要用多余空格保持对齐 空格的使用要能够在需要使用时强调警示读者（符合PEP8规范） 建议 6：编写函数的 4 个原则 函数设计要尽量短小，嵌套层次不宜过深 函数申明应该做到合理、简单、易于使用 函数参数设计应该考虑向下兼容 一个函数只做一件事，尽量保证函数语句粒度的一致性 Python 中函数设计的好习惯还包括：不要在函数中定义可变对象作为默认值，使用异常替换返回错误，保证通过单元测试等。 # 关于函数设计的向下兼容 def readfile(filename): # 第一版本 pass def readfile(filename, log): # 第二版本 pass def readfile(filename, logger=logger.info): # 合理的设计 pass 最后还有个函数可读性良好的例子： def GetContent(ServerAdr, PagePath): http = httplib.HTTP(ServerAdr) http.putrequest(&#39;GET&#39;, PagePath) http.putheader(&#39;Accept&#39;, &#39;text/html&#39;) http.putheader(&#39;Accept&#39;, &#39;text/plain&#39;) http.endheaders() httpcode, httpmsg, headers = http.getreply() if httpcode != 200: raise &quot;Could not get document: Check URL and Path.&quot; doc = http.getfile() data = doc.read() # 此处是不是应该使用 with ？ doc.close return data def ExtractData(inputstring, start_line, end_line): lstr = inputstring.splitlines() # split j = 0 for i in lstr: j += 1 if i.strip() == start_line: slice_start = j elif i.strip() == end_line: slice_end = j return lstr[slice_start:slice_end] def SendEmail(sender, receiver, smtpserver, username, password, content): subject = &quot;Contented get from the web&quot; msg = MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;) msg[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;) smtp = smtplib.SMTP() smtp.connect(smtpserver) smtp.login(username, password) smtp.sendmail(sender, receiver, msg.as_string()) smtp.quit() 建议 7：将常量集中到一个文件 在Python中应当如何使用常量： 常量名全部大写 将存放常量的文件命名为constant.py 示例为： class _const: class ConstError(TypeError): pass class ConstCaseError(ConstError): pass def __setattr__(self, name, value): if self.__dict__.has_key(name): raise self.ConstError, &quot;Can&#39;t change const.%s&quot; % name if not name.isupper(): raise self.ConstCaseError, \\ &#39;const name &quot;%s&quot; is not all uppercase&#39; % name self.__dict__[name] = value import sys sys.modules[__name__] = _const() import const const.MY_CONSTANT = 1 const.MY_SECOND_CONSTANT = 2 const.MY_THIRD_CONSTANT = &#39;a&#39; const.MY_FORTH_CONSTANT = &#39;b&#39; 其他模块中引用这些常量时，按照如下方式进行即可： from constant import const print(const.MY_CONSTANT) 第 2 章 编程惯用法 建议 8：利用 assert 语句来发现问题 断言的判断会对性能有所影响，因此要分清断言的使用场合： 断言应使用在正常逻辑无法到达的地方或总是为真的场合 python本身异常处理能解决的问题不需要用断言 不要使用断言检查用户输入，而使用条件判断 在函数调用后，当需要确认返回值是否合理时使用断言 当条件是业务的先决条件时可以使用断言 代码示例： &gt;&gt;&gt; y = 2 &gt;&gt;&gt; assert x == y, &quot;not equals&quot; Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AssertionError: not equals &gt;&gt;&gt; x = 1 &gt;&gt;&gt; y = 2 # 以上代码相当于 &gt;&gt;&gt; if __debug__ and not x == y: ... raise AssertionError(&quot;not equals&quot;) ... Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 2, in &lt;module&gt; AssertionError: not equals 运行是加入-O参数可以禁用断言。 建议 9：数据交换的时候不推荐使用中间变量 &gt;&gt;&gt; Timer(&#39;temp = x; x = y; y = temp;&#39;, &#39;x = 2; y = 3&#39;).timeit() 0.059251302998745814 &gt;&gt;&gt; Timer(&#39;x, y = y, x&#39;, &#39;x = 2; y = 3&#39;).timeit() 0.05007316499904846 对于表达式x, y = y, x，在内存中执行的顺序如下： 1. 先计算右边的表达式y, x，因此先在内存中创建元组(y, x)，其标识符和值分别为y, x及其对应的值，其中y和x是在初始化已经存在于内存中的对象。 2. 计算表达式左边的值并进行赋值，元组被依次分配给左边的标识符，通过解压缩，元组第一标识符y分配给左边第一个元素x，元组第二标识符x分配给左边第一个元素y，从而达到交换的目的。 （简单来说，直接交换符合pythonic且性能最佳，这么做就对了） 建议 10：充分利用 Lazy evaluation 的特性 （就是生成器） Lazy evaluation常被译为延迟计算，体现在用 yield 替换 return 使函数成为生成器，好处主要有两方面： 避免不必要的计算，带来性能提升 节省空间，使无限循环的数据结构成为可能 def fib(): a, b = 0, 1 while True: yield a a, b = b, a + b 建议 11：理解枚举替代实现的缺陷 使用 flufl.enum 实现枚举 建议 12：不推荐使用 type 来进行类型检查 使用 isinstance 来进行类型检查（注意上下包含关系就行） 建议 13：尽量转换为浮点类型后再做除法 py2.x:转换浮点类型后再做除法 建议 14：警惕 eval() 的安全漏洞 eval具有安全漏洞，建议使用安全性更好的ast.literal_eval。 建议 15：使用 enumerate() 获取序列迭代的索引和值 &gt;&gt;&gt; li = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] &gt;&gt;&gt; for i, e in enumerate(li): ... print(&#39;index: &#39;, i, &#39;element: &#39;, e) ... index: 0 element: a index: 1 element: b index: 2 element: c index: 3 element: d index: 4 element: e # enumerate(squence, start=0) 内部实现 def enumerate(squence, start=0): n = start for elem in sequence: yield n, elem # 666 n += 1 # 明白了原理我们自己也来实现一个反序的 def reversed_enumerate(squence): n = -1 for elem in reversed(sequence): yield len(sequence) + n, elem n -= 1 （此方式相比从列表里放索引取值更加优雅） 建议 16：分清 == 与 is 的适用场景 比较有趣的： &gt;&gt;&gt; s1 = &#39;hello world&#39; &gt;&gt;&gt; s2 = &#39;hello world&#39; &gt;&gt;&gt; s1 == s2 True &gt;&gt;&gt; s1 is s2 False &gt;&gt;&gt; s1.__eq__(s2) True &gt;&gt;&gt; a = &#39;Hi&#39; &gt;&gt;&gt; b = &#39;Hi&#39; &gt;&gt;&gt; a == b True &gt;&gt;&gt; a is b True 为了提高系统性能，对于较小的字符串会保留其值的一个副本，当创建新的字符串时直接指向该副本，所以a和b的 id 值是一样的，同样对于小整数[-5, 257)也是如此： 注意is不相当于 ==， is 是对 id 方法做的 == 。 建议 17：考虑兼容性，尽可能使用 Unicode python2.x 这是无敌深坑，需要刻苦学习掌握（python3偶尔也会碰到这种问题，但避免了大多数这种可能） 建议 18：构建合理的包层次来管理 module （__init__是对包的头文件定制） 本质上每一个 Python 文件都是一个模块，使用模块可以增强代码的可维护性和可重用性，在较大的项目中，我们需要合理地组织项目层次来管理模块，这就是包(Package)的作用。 一句话说包：一个包含__init__.py 文件的目录。包中的模块可以通过.进行访问，即包名.模块名。那么这init.py文件有什么用呢？最明显的作用就是它区分了包和普通目录，在该文件中申明模块级别的 import 语句从而变成了包级别可见，另外在该文件中定义__all__变量，可以控制需要导入的子包或模块。 这里给出一个较为合理的包组织方式，是FlaskWeb 开发：基于Python的Web应用开发实战一书中推荐而来的： |-flasky |-app/ # Flask 程序 |-templates/ # 存放模板 |-static/ # 静态文件资源 |-main/ |-__init__.py |-errors.py # 蓝本中的错误处理程序 |-forms.py # 表单对象 |-views.py # 蓝本中定义的程序路由 |-__init__.py |-email.py # 电子邮件支持 |-models.py # 数据库模型 |-migrations/ # 数据库迁移脚本 |-tests/ # 单元测试 |-__init__.py |-test*.py |-venv/ # 虚拟环境 |-requirements/ |-dev.txt # 开发过程中的依赖包 |-prod.txt # 生产过程中的依赖包 |-config.py # 储存程序配置 |-manage.py # 启动程序以及其他的程序任务 第 3 章：基础语法 建议 19：有节制地使用 from…import 语句 Python 提供三种方式来引入外部模块：import语句、from…import语句以及__import__函数，其中__import__函数显式地将模块的名称作为字符串传递并赋值给命名空间的变量。 使用import需要注意以下几点： 优先使用import a的形式 有节制地使用from a import A 尽量避免使用from a import * 为什么呢？我们来看看 Python 的 import 机制，Python 在初始化运行环境的时候会预先加载一批内建模块到内存中，同时将相关信息存放在sys.modules中，我们可以通过 sys.modules.items() 查看预加载的模块信息，当加载一个模块时，解释器实际上完成了如下动作： 在 sys.modules 中搜索该模块是否存在，如果存在就导入到当前局部命名空间，如果不存在就为其创建一个字典对象，插入到 sys.modules 中。 加载前确认是否需要对模块对应的文件进行编译，如果需要则先进行编译。 执行动态加载，在当前命名空间中执行编译后的字节码，并将其中所有的对象放入模块对应的字典中。 &gt;&gt;&gt; dir() [&#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;] &gt;&gt;&gt; import test testing module import &gt;&gt;&gt; dir() [&#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;test&#39;] &gt;&gt;&gt; import sys &gt;&gt;&gt; &#39;test&#39; in sys.modules.keys() True &gt;&gt;&gt; id(test) 140367239464744 &gt;&gt;&gt; id(sys.modules[&#39;test&#39;]) 140367239464744 &gt;&gt;&gt; dir(test) [&#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;a&#39;, &#39;b&#39;] &gt;&gt;&gt; sys.modules[&#39;test&#39;].__dict__.keys() dict_keys([&#39;__file__&#39;, &#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;__name__&#39;, &#39;b&#39;, &#39;a&#39;, &#39;__cached__&#39;]) 从上可以看出，对于用户自定义的模块，import 机制会创建一个新的 module。 将其加入当前的局部命名空间中，同时在 sys.modules 也加入该模块的信息，但本质上是在引用同一个对象，通过test.py所在的目录会多一个字节码文件。 （这节说的是，盲目使用from…import…会带来： 1. 命名空间冲突 2. 循环嵌套导入） 建议 20：优先使用 absolute import 来导入模块 （py3 中 relative import方法已被移除，不用操心）","tags":[]},{"title":"为什么我不需要仪式感？","date":"2017-05-08T10:17:18.000Z","path":"2017/05/08/为什么我不需要仪式感？/","text":"正是因为这个世界本就是混乱的、无序的 正是因为这个世界从来不存在什么天理昭昭，善恶有报 我才不想像其他人一样 在漫长幽暗的河流中，用尽全力点亮一座灯塔 告诉别人，这就是我生活的意义，生命的光辉 我的选择不需要仪式感证明 我的梦想不需要仪式感升华 我的力量在于不停的向前蠕动 而不是破茧成蝶","tags":[]},{"title":"YES！产品经理读书笔记 1","date":"2017-05-04T03:28:14.000Z","path":"2017/05/04/YES！产品经理读书笔记 1/","text":"1.产品经理是什么 什么是产品经理 依据公司产品战略，对某个线产品担负根本责任的企业管理人员。 产品经理的职责： 根据公司战略制定所负责产品的战略和目标 根据产品情况调整产品战略和目标 根据公司战略优化产品 根据产品市场反馈为高层提供决策建议 指定有效的竞争分析和竞争策略 对现有产品和新产品进行管理 产品经理的任务 竞争对手分析 产品组合管理 产品品牌管理 市场需求管理 规划和指定相关的产品策略 产品所需资源管理 指定营销策略 2.产品经理应该做什么 知道做什么 发现问题：找到市场需求 找到机会：找准市场位置 创新产品：指定产品发展思路 不认同： &gt; 做对的方向的乌龟，不做错的方向的兔子 知道怎么做 规划产品路线：产品目标是什么 设计产品策略：实现目标的方向 制定年度计划：如何逐步靠拢 选择做的东西，最直接的指标是盈利能力评估 3.产品经理的战术执行 让别人去做 方向引导:制定发展规划 过程管理：控制环节入口和出口，保证实现过程规范化 工作指导：注重其他细节 4.产品管理职业的级别都有哪些 产品助理：业务执行岗，数据收集和分析 产品经理：对企业发展做出合理判断。战略、规划、执行占2:3:5 高级产品经理：战略层面的规划。战略、规划、执行占2:3:5 5.产品经理的职业发展路线 SM：销售管理 MM：营销管理 BM：商业管理 7.什么样的人适合做产品经理 有成本意识：能为公司削减成本 有销售意识：能为公司挣钱 有团队意识：团队协作 有全局意识 不会抱怨：能力未到 不夸夸其谈 不喜欢加班 注重倾听 8.产品经理的人才模型 加分项： 独立思考能力 态度上，坚持自己想法 能力和态度二选一时，选择后者 产品经理素养 1.个人素养 个人修养 创新能力 沟通协调能力 自我管理能力 工作压力承受能力 管理知识 战略知识管理 团队管理知识 时间管理知识 项目管理知识 核心能力 产品需求管理 产品项目管理 新产品管理 产品生命周期管理 产品规范管理 9.产品经理的知识结构是什么 图看不清 10.产品经理是“通”才还是“专”才 通的是方法和过程，专的是方向和目的 11.产品助理应该做些什么 数据整理：收集各类市场、产品、客户、竞争对手有关的数据。 文档管理：各类文档统一的入口和出口。 事务性工作：组织部门会议。 12.产品经理应该知道的产品战略图 产品线的规划 归纳目前产品线 根据特征对产品分类 评估生命周期 评估产品之间的关系 形成文档，即产品战略图（PST） 产品图不管怎么表现，需要体现四个方面的信息 产品线信息 产品生命周期阶段信息 产品组合信息 产品信息 13.产品管理有行业特殊性吗 产品管理在不同行业上的共性 无论哪个行业，根本工作都是给企业规划处有持续盈利能力的产品（目的相同） 无论哪个行业，都是为企业创造价值（内容相同） 无论哪个行业，都需要先知道市场要什么，后规划如何满足需求，在用用企业提供的产品去满足我们认为需要满足的需求，实现价值交换。（流程相同） 产品经理要让企业资源增值 没有不做需求分析的企业，没有不做市场细分的企业，没有不做客户定位的企业 很多时候，看到的不一定是真相，共识的不一定是真理，存在也不是我们要坚持的 14.产品经理是否有行业性限制 鲍尔默的商业规则 一个公司，只有市场份额和现今流最重要 技术只是竞争优势，不是决胜优势 一切以客户和消费者作为企业的经营原则 一切以客户需求开发产品 找出问题！开动大脑！拿出方案！坚持执行！勇往直前！ 产品经理最终要提高的是商业运作能力 （但是商业运作能力本身也是很虚的东西） 15.技术人员如何转型为产品经理 了解自己是否真的不适合做技术工作 全面了解产品经理的工作，评估自己是否真的对这个工作有兴趣 岗位和思维都要转，要从懂产品变成懂业务 了解公司产品管理体系架构 从实践中获取第一手的市场业务信息 16.产品管理的工作流程是什么 战略活动 预测问题 培育机会 创新产品 ### 规划活动 产品路线 产品策略 产品规划 ### 战术活动 概念化 图纸化 技术化 商品化 市场化 PRD：产品需求文档 MRD：市场需求文档 17.产品管理工作的文档管理 文档作用 记录介质 过程推动 抽象体现 文档类型 流程类 控制类 决策类 文档的规范 文档模板规范 文档编号规范 文档存取规范 文档撰写规范 文档级别规范 18.产品经理需要了解的26个文档 10个必须要写的文档 D3：需求矩阵表 D9：商业方案（必须精通） D12：产品路线文档 D15：产品策略文档 D18：商业需求文档（必须精通） D19：市场需求文档（必须精通） D20：产品需求文档（必须精通） D21：产品验收文档 D24：产品白皮书 D25：产品总结报告 19.产品管理部归于何处 懂的多只能说明你的知识丰富，爱学习。只有做的好才说明你能力强悍，货真价实。 任何一次变革都是对现有利益的重新分配，这是方法，又是目的。 用户购买一个产品的流程： 1. 发现的过程：知道我们宣传的产品价值是否和他期望的利益一致。 2. 选择的过程：让用户选择我们的产品而不是竞争对手产品 3. 购买的过程：寻找性价比最高的产品 4. 使用的过程：决定是否购买 20.产品部和业务部门的利益之争 不停的妥协和不停的打压都是不可取的 在关系的夹缝中，要做到不偏不倚，实事求是","tags":[]},{"title":"sklearn学习笔记-3 回归模型","date":"2017-04-26T13:42:52.000Z","path":"2017/04/26/sklearn学习笔记-3 回归模型/","text":"回归模型： 线性回归模型 感知器模型 最简单的模型，多层感知器MLP即最简单的深度学习网络。 感知器模型通过循环修正计算误差进行修正，因此没有没有学习率；在学习中，需要指定学习次数（例如学习样本数），否则感知器模型会无休止的进行下去。 基础（单层）感知器模型是非常弱的模型，不会存在过拟合的情况。 sklearn.linera_model.Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, eta0=1.0, n_jobs=1. random_state = 0, class_weight=None, warm_start=False) 参数 penalty：正则惩罚项，默认为0 fit_intercept：截距 n_iter：迭代次数 shuffle：洗牌 verbose：打印日志等级 eta0 ：学习率 n_jobs：启动线程数，-1代表最大 实例 df = pd.read_csv(r&#39;D:\\BaiduYunDownload\\sklearn\\3.0 线性模型\\adultTest.csv&#39;) #将object变量（字符串）制成稀疏矩阵，class是目标变量，不用放入 dfNew=pd.get_dummies(df,columns=[&#39;workclass&#39;,&#39;education&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;, &#39;native-country&#39;]) dfNew[&#39;target&#39;] = dfNew[&#39;class&#39;].apply(lambda x: 1 if x == r&#39; &lt;=50K&#39; else 0) # 将结果二分为1和0 xdata = dfNew.drop([&#39;class&#39;,&#39;target&#39;],axis = 1) # 测试集 ydata = dfNew[&#39;target&#39;] # 结果集 from sklearn.linear_model import Perceptron # 调用感应器模型函数 per = Perceptron(n_iter=20) # 创建对象并设定迭代次数为20 per.fit(xdata,ydata) # 训练模型 print(per.predict(xdata)) # 对模型进行预测 print(per.score(xdata,ydata)) # 对模型进行准确率评估 tips：感知器没有概率值 线性回归模型 sklearn.linear_model.LinearRegression9(fit_intercept = True, normalize = False, copy_X = True, n_jobs = 1) fit_intercept：是否计算截距 normalize：是否对整个样本进行数据标准化，线性模型并不依赖于正则，所以此参数无意义。 线性回归模型需要接受一个连续性的变量，以达到预测。 在线性模型中，更复杂的模型会用到alpha惩罚项，简称a，并且会通过数据集的定义方式选择raige函数和lasso函数。 raige和lasso的区别 raige接受l2的正则，lasso接受l1的正则。 raige：(w1)2+(w2)2+(w3)^2…，每个权重平方相加；lasso:|w1|+|w2|+|w3|…，每个权重绝对值相加。 lasso接受的l1范数，所以得到的解可能是稀疏的，导致很多系数的权重都为0；raige接受l2的范数，得到的解不会为0。 从方便的角度考虑，我们更多的使用l2范数的raige。 交叉验证（Cross validation) 交叉验证用于防止模型过于复杂而引起的过拟合。有时亦称循环估计，是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析，而其它子集则用来做后续对此分析的确认及验证。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。 在sklearn的在线性模型中，为了保证模型值，我们可以使用后续加CV的函数来进行交叉验证： alpha：接受一个数组，数组内是所有认为有效的a的值，函数会自动选择最优值 cv：接受交叉验证的切分份数 scoring：评估指标，不填写会选择默认指标进行判断，也接受填写最小绝对值误差/最小均方误差，一般不填写 其他同上 在进行CV时，有时可能会遇到被切分的样本内没有1，或者样本不均匀的问题，因此我们调用分层函数进行解决: from sklearn.cross_validation import StratifiedKFold#调用分层函数 sf = StratifiedKFold(ydata,n_folds=5) # 创建对象并选择分为几层，避免发生稀疏矩阵的数据不均匀的问题 这种情况在分类模型中才会发生，回归模型的y是连续值，因此不必进行分层。","tags":[]},{"title":"sklearn学习笔记-2 数据处理","date":"2017-04-17T14:54:52.000Z","path":"2017/04/17/sklearn学习笔记-2 数据处理/","text":"稀疏和非稀疏 在矩阵中，若数值为0的元素数目远远多于非0元素的数目时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。稀疏被称为L1，非稀疏被称为L2。 2.1 数据标准化 特征决定了模型的上限，算法决定了如何逼近这个上限 对业务了解的越多，提出的特征越好 sklearn的数据处理能力现在非常强大，能处理文本、图片或更多的数据 在数据量纲不一样时，需要对数据进行标准化，一般来说，涉及到梯度下降的模型，和涉及到距离的模型，都需要做数据标准化。 定义 标准化1(常用的标准化）：减去一列的平均数再除以标准差，使其符合平均数为0标准差为1的高斯分布。 标准化2：压缩数据至0到1 树类模型不需要标准化。 标准化不能在训练集和测试集上分开进行（两个集的平均数和标准差不同），应当在测试集上应用训练集的标准化（减去标准化的平均数，除以标准化的标准差）。 把训练集和测试集放在一起做标准化也不是一种较好的办法，最优的办法是在训练集上做标准化，并transform到测试集中。 scale 用于针对列的数据标准化 函数 sklearn.preprocessing.scale(X,axis=0,with_mean=True,with_std=True,copy=True) X传入数组，支持pandas数据框,axis=0按列处理，mean=True平均数接近0，std=True标准差接近1 方法 sklearn.preprocessing.StandarScaler(with_mean=True,with_std=True,copy=True) 相比方法，函数很少被用到，因为该方法没有transform接口。 当数据不以时间为顺序时，可以随机划分训练集和测试集。 随机划分数据集 from sklearn.cross_validation import train_test_split#调取切割函数 X_train, X_test, y_train, y_test = train_test_split(df1,df[&#39;area&#39;],train_size=0.7) # 数据参数，数据结果，切分为测试集的比例 # X_train是被切分为训练集的比例，X_test是被切分为测试集的比例，y相同 在一个函数内训练两个集，目的是为了不破坏索引顺序 标准化数据1（正态数据） from sklearn.preprocessing import StandardScaler#调取标准化函数 ss=StandardScaler()#创建标准化对象 ss.fit(X_train)#以训练集训练标准化对象 X_train_ss=ss.transform(X_train)#将训练结果应用于训练集 X_test_ss=ss.transform(X_test)#将训练结果用于与测试集 此时，返回的结果都是numpy形式的多维数组。 每一组的平均值无限接近于0，标准差无限接近于1。 标准化数据2（压缩数据） from sklearn.preprocessing import MinMaxScaler#调取压缩函数 mms = MinMaxScaler()#创建标准化对象 mms.fit(X_train)#依然以训练集训练标注化对象 #... 以上标准化方法，都是对每一列（每一个特征进行标准化） normalize 对整个样本进行数据标准化（正则） 函数 sklearn.preprocessing.normalize(X, norm=‘12’, axis=1 ,copy=True, return_norm=False) 方法 sklearn.preprocessing.Normalizer(norm=‘12’, copy=True) normalize针对样本，但样本间是独立的，因此该方法的transform无意义（整个样本都被转换了，单个使用时就不必transform），只会在papline中有意义。 该方法只会用于计算距离、无监督学习的聚类中使用。默认接受L2 （稠密矩阵） from sklearn.preprocessing import Normalizer # 调用正则函数 norm = Normalizer() # 创建初始化对象 norm.fit((X_train)) # 虽然无意义，仍需要按照标准书写 norm.transform(X_train) # 虽然无意义，仍需要按照标准书写 norm.transform(X_test) # 虽然无意义，仍需要按照标准书写 2.2 分类值与缺失值 binarizer 用作数据二分化的分类，将一个连续性变量改为离散变量。 数据离散化的好处：数据存储空间小，并且因为0不参与运算（相加为原值，相乘为0）使运算速度非常快。 离散化也能较好的处理缺失值，可以将缺失值单独的形成一个特征，出现缺失值的行为1. 逻辑回归非常偏好离散化完成的数据 from sklearn.preprocessing import Binarizer#调取二分数据函数 bi = Binarizer(548)#创建对象定义分类标准，大于该值会被分为1，小于为0 DC_bi=bi.fit_transform(df[&#39;DC&#39;])#调用对象进行分类 当使用一列表示象征性的分类时，不要使用数字，因为程序sklearn会认为该特征的大小和顺序是有意义的，正确的做法是对少量参数进行稀释矩阵。 构建稀疏矩阵：skleran的OneHotEncoder非常不好用还容易报错，因此我们使用pandas的get_dummies()。 pd.get_dummies(data=df, columns=[&#39;month&#39;,&#39;day&#39;,&#39;DC_bi&#39;]) 缺失值 缺失值有时是真实存在的，缺失是存在意义的，并且当数据不够大时，删除缺失值只会浪费样本。 因此，更好的方式是填充缺失值，填充方式尽量通过业务的理解来进行填充。 from sklearn.preprocessing import Imputer#调用填充缺失值函数，该函数默认填充平均值 im=Imputer()#创建对象 dfna=im.fit_transform(df[&#39;DC_na&#39;])#选出需要填充的列并输出列 Tips： 1. 这里有错误，训练出来的列长度并不等于原来的长度，因此还是寄希望于pandas来处理 2. sklearn有些函数不支持缺失值或无限大、无限小的数据，因此通过pandas将此类数据设为缺失值，并且一次性的处理它。 3. 缺失值可以使用-999代替。 2.3 变量选择 特征选取 共线性特征，只保留一个就可以，可以用皮尔逊共线测试一下。 当特征过多时，只保留重要特征，删除掉不重要的参数也可以提高准确率。 针对稀疏矩阵判断特征的两种方式： 1. 使用lasso来选择特征 from sklearn.linear_model import Lasso # 从线性模型这种调取能判断参数重要性的函数lasso() lasso = Lasso() # 创建对象 lasso.fit(xdata,ydata) # fit不接受缺失值，使用前记得填充缺失值为-999。第一个参数为参数df，第二个参数为结果df print(lasso.coef_) # 打印特征重要程度，可以删除无限接近于0的特征 所有的稀疏矩阵类型，都可以放入此函数进行判断。 使用featureselect来选择特征： from sklearn.feature_selection import SelectFromModel # 调用参数选择模型 model = SelectFromModel(lasso) # 创建对象并直接传入模型，相当于省略了fit步骤 model.fit_transform() # 无意义，为了是一个popline，另外sklearn不接受异常值，这里因为极大和极小值的存在，会报错。 支持LR回归，简单线性回归 当特征是连续性，我们希望方差越大越好 特诊判断 基于树形模型对特征进行判断： from sklearn.ensemble import RandomForestRegressor#从随机森林库中调取树类特征选择函数 rf = RandomForestRegressor()#创建对象 rf.fit(xdata,ydata)#训练模型，第一个参数为参数df，第二个参数为结果df print(rf.feature_importances_)#打印特征重要程度，数值越接近于0，意义越小 提到的pandas相关操作 .head():读取前五行数据 df.loc[ : ,x:y]:索引选取x到y列的数据 df[0, : ]维度转换 pd.cut(df[‘’],n)：将padans中的一列均匀的切分成n份，返回的值是一个字符串。 pd.get_dummies(data, prefix = None, prefix_sep = ’_’, dummy_na = False, columns=None, sparse = False, drop_first = False)：函数接受一个df表格以及使用columns选取的列，函数会删除原来的列并对其进行稀疏化矩阵的扩充，使其变为0/1形式，返回一个重新构建好的df表格。 df.loc[df[‘DC’]&gt;=600,‘DC_na’] = np.nan ：对df行中的DF列大于等于600的，新建一列设为NaN。 df.replace(np.inf,np.nan)：替换过大值为NaN","tags":[]},{"title":"天池赛IJCAI-17 口碑商家客流量预测 解题思路","date":"2017-03-22T13:40:03.000Z","path":"2017/03/22/天池赛IJCAI-17 口碑商家客流量预测 解题思路/","text":"赛题与数据 代码 基本数据创建 result：每家店铺每日交易成功数量 view：每家店铺每日浏览量 参数分解 shop_info shop_id city_name location_id per_pay score comment_cnt shop_level cate_name.. 商家id 城市名 所在位置编号 人均消费 评分 评论数 商铺等级 分类 shop_id：主键，索引 city_name：获取气温、消费能力、消费习惯 location_id：聚类算法，估计功效太低没什么意义，pass per_pay：检测与result负相关，与view负相关。 socre：检测与result正相关，与view正相关。 comment_cnt：检测与result正相关，与view正相关。 shop_level：检测与result正相关，与view正相关。 cate_name：分类太细，考虑只保留使用“超市”和“美食”进行区分。 检测per_pay、score、comment_cnt、shop——level与view、result的关联度。 score有很大的问题：这个值是处于变动的。 user_pay user_id shop_id time_stamp 付费用户id 商家id 消费时间 time_stamp：分解出日期day和时间time列。 user_view user_id shop_id time_stamp 浏览用户id 商家id 浏览时间 time_stamp：分解出日期day和时间time列。 特征工程 1.考虑到口碑是2015年6月23日开始发布，必然遭遇冷启动和虚假数据问题，那么时间序列中，体现趋势的指标应该是7日移动平均线ma，影响最大的特征因子应该是最近一次的ma_7。 2.城市天气逻辑体现非常重要，主要划分了三级（晴，小雨/小雪/，大雨/雪），但划分后的效果并不很好。 3.当日是否为工作日，次日是否为工作日比较重要。 4.16年情人节到过年的那周视为噪音。 5.GDP作为特征果然没效果，删了。 感受 1.以不同可索引对象制造的模型再融合有巨大威力，第一次瞎配的权重都带来了最好的提升。 2.solo的问题不在于想法…判断出哪个想法提升最多是最重要的，当然这需要经验。 3.xgboost因为bug跑不起来，没时间走ARIMA，也没时间再上prophet，凄苦…水平不够时候有队友提升会比较快。 4.合理利用每日评分确定正确方向是非常有必要的，相信前几的差距已经是谁对趋势判断更敏锐了。 5.全身心的投入大约勉强能进前200，看wepon大神的blog，对底层的理解还是很重要，今后要加强学习和训练。","tags":[]},{"title":"最近在做测试","date":"2017-03-17T06:25:46.000Z","path":"2017/03/17/最近在做测试/","text":"最近一直在忙测试相关工作，记录下测试工作心得： 1.重要的不仅只是懂业务流程，也要把思路抽离在流程之外。因为产品在设计时，往往会先定下大的方向，再逐渐寻找最优路线，在不停的修改中，也许已经偏离“可用”的轨道，此时测试应该代入的是用户的思路，对测试时找到的BUG，提出期望结果；对不合适的流程，也要提出优化期望。 2.细化的测试用例很重要，一方面可用于自动化测试的架构，另一方面，越细的测试用例，在写的时候就能发现非常多的问题。 3.如果作为懂技术的测试人员，对于测出的问题，应该做到比写这个功能的开发了解的更透彻，这也是一个测试人员平庸和杰出的分水岭。 4.测试人员是产品和开发扯皮中的重要第三方，沟通非常重要。 5.同上，测试“可信任”也是非常重要的，对能复现的问题，一定要通过复现从而找出问题；对不能复现的问题，记录下相关操作以便将来找出原因，直觉是知识、感官的积累，对于难以找到原因的问题，直觉非常重要。 6.最好的用例就是文档，文档即用例。","tags":[]},{"title":"Paxos算法简易理解","date":"2017-02-24T05:32:31.000Z","path":"2017/02/24/Paxos算法简易理解/","text":"简介 Paxos算法简单来说就是一个在异步分布式系统里用以保证一致性的投票算法，严格来说，Paxos并不是一个算法，而是一个经过严谨逻辑推导出的投票方式。该算法由莱斯利·兰伯特于1990年提出，只由一些必须要被满足的基本条件推导得出，因此Paxos是在所有分布式算法中最简单，同时也是最稳定的算法。 Paxos应用于“基于消息传递且要求具有高度容错特性”的场景，在“共享内存”的分布式场景中并不适用，另外，虽然有许多基于Paxos的一致性算法，但都没有paxos稳定。 谷歌在其分布式锁服务（Chubby lock）中应用了Paxos算法。Yahoo!开源的ZooKeeper是一个开源的类Paxos实现，而hadoop也支持ZooKeeper协议。 这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品 – Mike Burrows 算法假设 1.不存在拜占庭将军问题（一条消息可能被传递两次，但在传递中不会出错） 2.只要等待足够的时间，消息就会被传到(信息必定到达） 3.每个节点对于信息，接收到信息即以为赞同，同时只有接受和忽略，没有反对。（只可根据赞成数来确定结果） 4.信息不一定会被成功传递，也无法确定传递时间。（包容传递失败，并确定在此条件下也能保持一致性） ##算法定义 在一个分布式系统中，每个节点会有最少1种，最多3种身份，分别是proposers（提案者）、acceptors（批准者）、learners（接受者），他们在节点中相互传递信息被称为value（决议）。 1.决议只能被提案者提出，同时随决议附有一个编号，编号是递增且唯一的； 2.任何两个提案编号之间构成偏序（意味着提案者的编号大小是有意义的；同一次提案，提案者的权限是有等级分别的）； 3.在一次算法执行中，每个批准者一次只批准一条决议，并且只与最新的决议形成交互（意味着可能存在信息队列）； 4.收到超过半数的表态，即视为通过决议，并向所有人广播； 5.接受者只能获得最终被批准的决议。 需要注意的是，节点可以同时拥有三种身份，当节点既是提案者，又是批准者时，他必定会为自己提出的决议投出一票。 实例 东方网准备开一场主题为“智橙生活未来发展方向”的会议，与会人员分别是董事长何XX、副董事长徐XX、纪委书记金X、副总裁高X、技术总监老王。与会人员都可以提案和赞同，一人一票。其他未与会人员例如开发、产品、数据分析师等，只能接受讨论结果（定义5）。 第一天：董事长提案智橙生活应该主打“上海市民离不开的产品”，其他4人纷纷表示“赞同”，董事长收到其他4人的表态，开心的表示全票通过，向所有人广播“产品基调定好，新的提案不得再讨论本问题”，全场统一。 第二天：董事长提案智橙生活应该添加“为民办事”功能，老王不能装作听不见，只好表示“赞同”（假设3），副总裁和纪委书记睡着了，无法表态，董事长等了半天只等到了两个表态，表示少两人老子照样干，加上自己一票通过了决议（定义4），向所有人广播“功能确定，新的提案不得再讨论本问题”，全场统一。 第三天：副总裁和纪委书记睡醒了，发现了董事长的提案，但同时（或更早）收到了提案结果的广播，于是他们不必再对此提案表态，全场统一。 第四天：轮到老王提案，他提出了“打磨产品，塑造核心功能点”的主旨，这时其他4位领导都睡着了，老王未获得任何表态。决议失败，不能向任何人广播。只能重新提案，全场统一。 第五天：4位领导终于都醒了，这时董事长提出了“尽可能多的造功能，我们要做一款万能APP”的主旨，老王再次提出昨天的提案，其他三位领导先听见了董事长的训示，纷纷表态支持，这时才反应过来老王有一份同样的提案，这时有两种情况： 老王职级低：其他领导表态后收到了老王的提案并发现是相同的提案（定义3），但老王的职级比他们的低，因此会忽略老王的提案，全场统一。 老王职级高：其他领导表态后收到了老王的提案并发现是相同的提案（定义3），他们会向老王回复之前已经通过了的提案及其内容并忽略投票，这样老王会知道自己的提案无望，全场统一。 第六天：董事长和老王同时说出对一件事的提案，并且获得邻座（近邻节点）的迅速反馈，此时老王邻座的副总裁和纪委书记先向老王表态“赞同”，当董事长的提案到来时，他们会再次表示“赞同”。而副董事长先向董事长的提案表示“赞同”后，收到了老王的提案，此时他将向老王表示“已有大人物关注此事，你不要插手”（定义2）。当信息传递结束，高权限必定获得更高的“赞同”，提案通过，全场统一。 第七天：老王总结出了深刻的人生哲理：在异步的任何情况下，即使某一节点出于某些原因无法接受信息，也绝不影响该节点以及总体对于一致性的检测，并且为了保证异步一致，权限相对较低的节点在提案和投票中处于较不利的位置。","tags":[]},{"title":"使用sklearn进行数据预处理 —— 归一化/标准化/正则化","date":"2017-02-18T15:23:34.000Z","path":"2017/02/18/使用sklearn进行数据预处理 —— 归一化-标准化-正则化/","text":"本文主要是对照scikit-learn的preprocessing章节结合代码简单的回顾下预处理技术的几种方法，主要包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理。内容比较简单，仅供参考！ 首先来回顾一下下面要用到的基本知识。 ## 一、知识回顾 均值公式： \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\] 方差公式： \\[s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\] 0-范数，向量中非零元素的个数。 1-范数： \\[||X||=\\sum_{i=1}^{n}|x_{i}|\\] 2-范数： \\[||X||_{2}=(\\sum_{i=1}^{n}x_{i}^{2})^{\\frac{1}{2}}\\] p-范数的计算公式： \\[||X||_{p}=(\\sum_{i=1}^{n}x_{i}^{p})^{\\frac{1}{p}}\\] 数据标准化：当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。 二、标准化(Standardization)，或者去除均值和方差进行缩放 公式为：(X-X_mean)/X_std 计算时对每个属性/每列分别进行. 将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1。 首先说明下sklearn中preprocessing库里面的scale函数使用方法： 1sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True) 根据参数的不同，可以沿任意轴标准化数据集。 参数解释： X：数组或者矩阵 axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard + deviations. 如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。 with_mean: boolean类型，默认为True，表示将数据均值规范到0 with_std: boolean类型，默认为True，表示将数据方差规范到1 一个简单的例子 假设现在我构造一个数据集X，然后想要将其标准化。下面使用不同的方法来标准化X： 方法一：使用sklearn.preprocessing.scale()函数 方法说明： X.mean(axis=0)用来计算数据X每个特征的均值； X.std(axis=0)用来计算数据X每个特征的方差； preprocessing.scale(X)直接标准化数据X。 将代码整理到一个文件中： 12345678910111213from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) # calculate mean X_mean = X.mean(axis=0) # calculate variance X_std = X.std(axis=0) # standardize X X1 = (X-X_mean)/X_std # use function preprocessing.scale to standardize X X_scale = preprocessing.scale(X) 最后X_scale的值和X1的值是一样的，前面是单独的使用数学公式来计算，主要是为了形成一个对比，能够更好的理解scale()方法。 方法2：sklearn.preprocessing.StandardScaler类 该方法也可以对数据X进行标准化处理，实例如下： 1234567from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) 这两个方法得到最后的结果都是一样的。 三、将特征的取值缩小到一个范围（如0到1） 除了上述介绍的方法之外，另一种常用的方法是将属性缩放到一个指定的最大值和最小值(通常是1-0)之间，这可以通过preprocessing.MinMaxScaler类来实现。 使用这种方法的目的包括： 1、对于方差非常小的属性可以增强其稳定性； 2、维持稀疏矩阵中为0的条目。 下面将数据缩至0-1之间，采用MinMaxScaler函数 1234567from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) min_max_scaler = preprocessing.MinMaxScaler() X_minMax = min_max_scaler.fit_transform(X) 最后输出： 123array([[ 0.5 , 0. , 1. ], [ 1. , 0.5 , 0.33333333], [ 0. , 1. , 0. ]]) 测试用例： 1234X_test = np.array([[ -3., -1., 4.]]) X_test_minmax = min_max_scaler.transform(X_test) X_test_minmax array([[-1.5 , 0. , 1.66666667]]) 注意：这些变换都是对列进行处理。 当然，在构造类对象的时候也可以直接指定最大最小值的范围：feature_range=(min, max)，此时应用的公式变为： 12X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0)) X_minmax=X_std/(X.max(axis=0)-X.min(axis=0))+X.min(axis=0)) 四、正则化(Normalization) 正则化的过程是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用。 该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础. Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。 方法1：使用sklearn.preprocessing.normalize()函数 12345678&gt;&gt;&gt; X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] &gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm='l2') &gt;&gt;&gt; X_normalized array([[ 0.40..., -0.40..., 0.81...], [ 1. ..., 0. ..., 0. ...], [ 0. ..., 0.70..., -0.70...]]) 方法2：sklearn.preprocessing.StandardScaler类 123&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X) # fit does nothing &gt;&gt;&gt; normalizer Normalizer(copy=True, norm='l2') 然后使用正则化实例来转换样本向量： 123456&gt;&gt;&gt; normalizer.transform(X) array([[ 0.40..., -0.40..., 0.81...], [ 1. ..., 0. ..., 0. ...], [ 0. ..., 0.70..., -0.70...]]) &gt;&gt;&gt; normalizer.transform([[-1., 1., 0.]]) array([[-0.70..., 0.70..., 0. ...]]) 两种方法都可以，效果是一样的。 五、二值化(Binarization) 特征的二值化主要是为了将数据特征转变成boolean变量。在sklearn中，sklearn.preprocessing.Binarizer函数可以实现这一功能。实例如下： 12345678910&gt;&gt;&gt; X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] &gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X) # fit does nothing &gt;&gt;&gt; binarizer Binarizer(copy=True, threshold=0.0) &gt;&gt;&gt; binarizer.transform(X) array([[ 1., 0., 1.], [ 1., 0., 0.], [ 0., 1., 0.]]) Binarizer函数也可以设定一个阈值，结果数据值大于阈值的为1，小于阈值的为0，实例代码如下： 12345&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1) &gt;&gt;&gt; binarizer.transform(X) array([[ 0., 0., 1.], [ 1., 0., 0.], [ 0., 0., 0.]]) 六、缺失值处理 由于不同的原因，许多现实中的数据集都包含有缺失值，要么是空白的，要么使用NaNs或者其它的符号替代。这些数据无法直接使用scikit-learn分类器直接训练，所以需要进行处理。幸运地是，sklearn中的Imputer类提供了一些基本的方法来处理缺失值，如使用均值、中位值或者缺失值所在列中频繁出现的值来替换。 下面是使用均值来处理的实例： 12345678910&gt;&gt;&gt; import numpy as np &gt;&gt;&gt; from sklearn.preprocessing import Imputer &gt;&gt;&gt; imp = Imputer(missing_values='NaN', strategy='mean', axis=0) &gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]]) Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0) &gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]] &gt;&gt;&gt; print(imp.transform(X)) [[ 4. 2. ] [ 6. 3.666...] [ 7. 6. ]] Imputer类同样支持稀疏矩阵： 12345678910&gt;&gt;&gt; import scipy.sparse as sp &gt;&gt;&gt; X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]]) &gt;&gt;&gt; imp = Imputer(missing_values=0, strategy='mean', axis=0) &gt;&gt;&gt; imp.fit(X) Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0) &gt;&gt;&gt; X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]]) &gt;&gt;&gt; print(imp.transform(X_test)) [[ 4. 2. ] [ 6. 3.666...] [ 7. 6. ]] sklearn相关英文版本:Preprocessing data 中文版本:数据预处理 本文提取自：http://blog.csdn.net/dream_angel_z/article/details/49406573","tags":[]},{"title":"windows 10 github page + hexo + pandoc + next 搭建博客","date":"2017-02-13T09:32:25.000Z","path":"2017/02/13/windows 10 github page + hexo + pandoc + next 搭建博客/","text":"之前根据crossin的编程教室一系列教程使用githubpage + hexo + yilia成功搭建了博客，但是知其然不知其所以然，在尝试转变风格为next的时候花了不少功夫，痛定思痛之后决定把这些记录下来，以备将来再次安装使用。 简单的教程很多就不再赘述，直接粘贴当时看的教程。 用 GitHub + Hexo 建立你的第一个博客 部署博客及更新博文 安装自己喜欢的主题 更换markdown渲染引擎 hexo默认只支持最基础的markdown渲染，为了实现现代化的功能（囧），改用pandoc来进行渲染。 首先前往pandoc官网下载并安装pandoc，安装成功后测试pandoc --help命令以确定安装成功。 之后安装pandoc作为hexo的渲染引擎，进入hexo目录后，输入以下命令： npm uninstall hexo-renderer-marked --save npm install hexo-renderer-pandoc --save pandoc和基础markdown语法有细微不同，具体细节可以去官网查阅。 支持LATEX数学公式 hexo目录中，输入以下命令： npm install hexo-math --save hexo math install 并且在网站配置的_config.yml文件中添加： plugins: - hexo-math 最后有一点许多教程都没有提到的，记得去正在使用的主题配置_config.yml中，将MathJex相关支持设定为true。 更换主题为next 依然在hexo目录中，安装next主题 git clone https://github.com/iissnan/hexo-theme-next themes/next 打开站点配置文件_config.yml修改主题为next theme: next 相关建议设置next官网有清楚的描述。 使用Hexo的内建归档categories 作者：碎瞳Artin 链接：https://www.zhihu.com/question/33324071/answer/58775540 来源：知乎 著作权归作者所有，转载请联系作者获得授权。 1.第一步：生成post（文章）时默认生成categories配置项：在根目录下scaffolds/post.md中，添加一行categories:。同理可应用在page.md和photo.md，示例如下： title: {{ title }} date: {{ date }} tags: categories: # 此处为添加内容 --- 2.第二步：在实际写作时，在开头进行categories配置。例如： title: Hello，World!你好，世界！ date: 2014-01-21 23:33:02 tags: 写作 categories: 随笔 # 配置categories 这样在文章发布时，在git中使用hexo g命令，hexo会在根目录/public/categrises下自动生成归档文件夹，如图： image_1b8php9vm1tb71u91170v1kusmrqg.png-26kB 3.第三步：配置博客首页归档展示样式。在主题配置文件themes/_config.yml中添加以下代码（#号后为注释内容）: menu: home: / essay: /categories/随笔 # 博客首页展示文本/访问路径/自定义归档名称 write: /categories/写作 read: /categories/阅读 study: /categories/学习 code: /categories/编程 4.补充说明：如果发现博客首页展示文本为英文，需要改为中文显示，需要修改先博客根目录下的_config.yml文件的language配置，示例如下： # Site title: My Blog subtitle: description: author: language: zh-CN # 修改此处，一般默认为default.yml，原生英文显示 timezone: 然后为实现文章归档名称显示为中文，接着再修改主题配置文件下language/zh-CN.yml即可，示例如下： title: archive: 归档 category: 分类 tag: 标签 menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 essay: 随笔 # 编辑代码时注意语法规范如缩进、空格等 read: 阅读 # Hexo采用yml语法，具体可自行搜索 write: 写作 5.最终展示效果（图中红框作强调用）： image_1b8phsloq155m1dbv9f9cgd6g4t.png-41.6kB 6.点进某一归档分类如“阅读”，博客文章会依照归档配置，排序显示如下： image_1b8pht2l6ifh1neh1k4i3hgsg21a.png-27.1kB **注：目前下载安装的hexo貌似都没有zh-CN.yml，而以zh-Hans代替，第四步修改对应文件即可。 上传博客常用命令 hexo目录下： # 清理缓存 hexo clean # 生成静态文件 hexo generate # 本地预览（在4000端口） hexo s # 提交至网站 hexo deploy 文件前的title各标签视主题而有所不同。","tags":[]},{"title":"数据分析扫盲  --  2.机器学习","date":"2017-01-22T02:39:25.000Z","path":"2017/01/22/数据分析扫盲  --  2.机器学习/","text":"FBIWARNING：本文一切知识点，知识框架均来自于个人对数据分析、数据挖掘、机器学习等方面的理解，推测均出自于本人的臆测，如果对数据分析感兴趣，可以参阅可汗学院的《统计学》、各大学的数据分析和机器学习课程。 基础知识 机器学习定义 机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法 – 维基百科 简单从实现上来说，机器学习就是使用机器学习算法来得到目标数据集的最优近似函数的过程。 监督和无监督 监督学习所得到的结果都是已知的，即结果集是可预估的。常见的监督学习算法包括回归分析和统计分类。 无监督学习所得到的结果是未知的，即结果集是不可预估的，常见的无监督学习算法有聚类。 半监督学习继承了监督学习与无监督学习，往往采用少量的数据集使用监督学习得到部分结果，再以大量的无监督学习对结果进行验证和修正。 增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。 数据挖掘 数据挖掘这一词语常与机器学习相混淆，实际上，数据挖掘的总体目标是从一个数据集中提取信息，并将其转换成可理解的结构，以进一步使用。出于发现数据集知识的目的，数据挖掘设计数据管理方面、数据预处理、模型与推断方面考量、兴趣度度量、复杂度的考虑，以及发现结构、可视化及在线更新等后处理。有鉴于此目标，“数据挖掘”实际上和“数据分析”这一名词在同一维度。 机器学习框架 大多数机器学习算法的基本框架都是模型（model/Representation）、代价函数（cost function）、优化算法。 以最简单的线性回归（Linear regression）来举例： 线性回归（模型）常用于有明显线性关系简单模型预测，例如流行病学、金融资产等，其算法的代价函数为最小二乘法（代价函数），即 最终使用梯度下降（优化算法）得到结果。 而在使用Logistic回归（Logistic Regression）算法时，Logistic回归（模型）使用的则是极大似然估计（代价函数），即 最终使用梯度下降（优化算法）得到结果。 最小二乘和极大似然的关系 在不同的情况中使用不同的代价函数，原因是各自的响应变量y服从不同的概率分布。 在线性回归中，前提假设是y服从正态分布，即\\[y\\sim N(\\mu,\\sigma^2)\\]而Logistic回归中的y是服从二项分布的，即\\[y\\sim Bernoulli(\\phi)\\] 因而，在用极大似然估计计算时，所得到的代价函数自然是不一样的。 最小二乘是从函数形式上来看的，极大似然是从概率意义上来看的。事实上，最小二乘可以由高斯噪声假设+极大似然估计推导出来。 所以在较复杂的模型中，一个代价函数可以用不同的优化算法，不同的代价函数也可以用相同的优化算法。 代价函数 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。 仍以线性回归作为示范，图中X为样本点，垂直蓝线是建模误差，为了得到合适的直线，我们必须令蓝线尽可能短，即建模误差最小。 image_1b72gv22748b11ejve71o60us718.png-20.2kB 为了使代价函数最小，我们绘制套入代价函数结果的三维等高图，即可得到 image_1b72h6eea1vc61qv74rf1t4o1t8l1l.png-69kB 图中最凹点即为代价函数最优情况。（具体函数推导详见coursera机器学习课程2-3）。 全局最优解/局部最优解 接下来，我们以梯度下降作为优化算法来计算代价函数最小值。 我们依然把一个二维数据集加入代价函数，绘制三维等高图，如图所示： image_1b72hdcd5ehjhhk1e68rte1gsp22.png-135.5kB 无论数据初始在什么位置，我们的目标，是进入全局最优点（即全局最低点），也就是机器学习工程师们常说的下山。 想象你正在所示起点，视野范围是有限的，为了尽快下山，你会在前往目之所及选择最低的一点。当你到达之前的最低点，再以此类推继续前往最低点。 如右边的线段所示，很常见的，在下山中不断的寻找有地点，很有可能进入并停留在一个局部最低点，而非全局最优点。这种情况常发生在视距(学习率a，库中命名为alpha)的选择不合适的情况下，降低a的值虽然可以提高进入全局最优点的准确率，也会很大的降低模型的计算速度。 欠拟合和过拟合 以一个分类模型为例： image_1b72i3cnefas1cvrjp9qp2qj22f.png-226.9kB 对一个多项式模型而言，x的次数越多，拟合效果就会越好，但是从操作上，将其控制在一个合适的范围内并不容易，图中第二个模型属于适当的分类，而第一个模型太过松散，称为欠拟合，第三个模型分类过度，称为过拟合，欠拟合和过拟合导致的调参问题一直都是机器学习使用者最大的问题。 在欠拟合中，拟合度不足，我们可以轻松的通过增加x的次数（多项式）来弥补；而在过拟合中，通常有两种处理方法： 1.使用特征工程，来抛弃一些对模型有影响却并没有实际意义的特征。 2.正则化，保留特征，但控制参数的大小。 在实际运用中，一般会同时使用这两种方法。后面会提及这两种方法的具体使用。 算法选择 主要算法 泛指被sklearn等大型库集成的算法。 1.朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练； 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感。 2.Logistic Regression（逻辑回归） 逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树、SVM相比，会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间）。 Sigmoid函数：表达式为公式: \\[f(x)=\\frac{1}{1+e^{−x}}\\] 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换； 3.线性回归 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： \\[\\hat{w}=(X^{T}X)^{-1}X^Ty\\] 而在LWLR（局部加权线性回归）中，参数的计算表达式为: \\[\\hat{w}=(X^{T}WX)^{-1}X^TWy\\] 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： + 实现简单，计算简单； 缺点： + 不能拟合非线性数据. 4.最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别； 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点： 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； 缺点: 计算量大（体现在距离计算上）； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 5.决策树 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。 决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。 信息熵的计算公式如下: \\[H=-\\sum^{n}_{i=1}p(x_i)log_2p(x_i)\\] 其中的n代表有n个分类类别（比如假设是二类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。 现在选中一个属性x用来进行分枝，此时分枝规则是：如果x=v的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’ =p1 H1+p2 H2,则此时的信息增益ΔH = H - H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。 优点： 计算简单，易于理解，可解释性强； 比较适合处理有缺失属性的样本； 能够处理不相关的特征； 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 缺点： 容易发生过拟合（随机森林可以很大程度上减少过拟合）； 忽略了数据之间的相关性； 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只+ 要是使用了信息增益，都有这个缺点，如RF）。 5.1 Adaboosting Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。 优点： Adaboost是一种有很高精度的分类器。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。 简单，不用做特征筛选。 不易发生过拟合。 缺点： 对离散值比较敏感 6.SVM支持向量机 支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。 优点： 可以解决高维问题，即大型特征空间； 能够处理非线性特征的相互作用； 无需依赖整个数据； 可以提高泛化能力； 缺点： 当观测样本很多时，效率并不是很高； 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数； 对缺失数据敏感； 对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）： 第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了； 第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果； 第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。 对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。 7. 人工神经网络的优缺点 （人工神经网络目前主要是深度学习的范畴） 优点： 分类的准确度高； 并行分布处理能力强,分布存储及学习能力强， 对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系； 具备联想记忆的功能。 缺点： 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值； 不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度； 学习时间过长,甚至可能达不到学习的目的。 8、K-Means聚类 优点： 算法简单，容易实现 ； 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛。 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点： 对数据类型要求较高，适合数值型数据； 可能收敛到局部最小值，在大规模数据上收敛较慢 K值比较难以选取； 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果； 不适合于发现非凸面形状的簇，或者大小差别很大的簇。 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。 仿生/模拟算法 通过对自然过程的模拟，对原有算法进行优化，也叫启发式算法（人工神经网络和深度学习也在其中）。 ### 9.蚁群算法 又称蚂蚁算法，是一种用来在图中寻找优化路径的机率型算法。其灵感来源于蚂蚁在寻找食物过程中发现路径的行为。蚁群算法是一种模拟进化算法，初步的研究表明该算法具有许多优良的性质.针对PID控制器参数优化设计问题，将蚁群算法设计的结果与遗传算法设计的结果进行了比较，数值仿真结果表明，蚁群算法具有一种新的模拟进化优化方法的有效性和应用价值。 10.粒子群算法(PSO) 又称微粒群算法，该算法使用如下心理学假设：在寻求一致的认知过程中，个体往往记住自身的信念，并同时考虑同事们的信念。当其察觉同事的信念较好的时候，将进行适应性地调整。 流程如下： 初始化一群微粒（群体规模为m），包括随机的位置和速度； 评价每个微粒的适应度； 对每个微粒，将它的适应值和它经历过的最好位置pbest的作比较，如果较好，则将其作为当前的最好位置pbest； 对每个微粒，将它的适应值和全局所经历最好位置gbest的作比较，如果较好，则重新设置gbest的索引号； 根据方程（1）变化微粒的速度和位置； 如未达到结束条件（通常为足够好的适应值或达到一个预设最大代数Gmax），回到2）。 11.模拟退火算法 模拟退火算法的原理同金属退火类似：将搜寻空间内每一点想像成空气内的分子；分子的能量，就是它本身的动能；而搜寻空间内的每一点，也像空气分子一样带有“能量”，以表示该点对命题的合适程度。算法先以搜寻空间内一个任意点作起始：每一步先选择一个“邻居”，然后再计算从现有位置到达“邻居”的概率。 在一个模型中，模拟退火算法起到的作用是跳崖，防止进入局部最优解。 12.禁忌搜索算法 类似于模拟退火算法的目标，具体方式为其先创立一个初始化的方案，基于此，算法“移动”到一相邻的方案。经过许多连续的移动过程，以提高解的质量。 其它优化算法 13.TF-IDF算法 TF-IDF是一种文本统计方法，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。常见于将文本转换为数值以相互比较和计算。 算法过程： 数据处理及应用 数据缩放 在特征工程中，除了算法使用代价函数，还可以将不同特征的量纲控制在一定范围内。 在梯度下降的使用中，面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯 度下降算法更快地收敛。面对不同量纲的数据，解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间。以解决特征量纲的干扰。 算法表现为： \\[ x_n = \\frac{x_n-\\mu}{s_n} \\] 缺失值/离散值/极大极小值的处理 数据集难免存在缺失数据和录入错误的极端值数据，极大极小值也会影响模型，因此有不同的处理方式。 缺失值：判断数值的缺失是否合理。如果缺失不合理，考虑在数据足够大的条件下删除此数据；如果缺失合理或数据大小不适合删除此条数据，根据陈天奇大牛的建议，可以将此数据设为-999。 离散值：判断数值的离散是否合理，如果合理则保存此数据；如果不合理，则查看是否可以判断不合理原因给予修正；如无法修正，可以考虑作同缺失值的处理。 极大极小值：对不支持的模型，将其作为缺失值处理。 降维 数据集的某些无意义特征的存在不但降低模型计算速度，还会形成噪音影响模型。因此通过数据降维来提升模型质量。降维的计算比较复杂，在不同的模型中应用方式也各有不同，好在大型的机器学习库都提供了相应的算法，不多赘述，详见解析。 稀疏矩阵 稀疏矩阵是指将离散化的特征拆开，形成新的多个特征的一种手段，新生成的特征中只包含True（1）和False（0），且大部分元素都为False(0)的矩阵。实际上，连续性数据也可以通过分组达到此目的。 稀疏矩阵的目的主要是通过其自身的稀疏特性，通过压缩可以大大节省稀疏矩阵的内存代价，从而加快模型的运算速度，更重要的是当数据过大时，通过稀疏矩阵，标准化的算法将之前不可操作的数据变为可操作数据。 sklearn虽然集成了稀疏矩阵函数，但其效果并不优秀。在python语言中，可以依赖pandas的get_dummies函数将数据快速化为稀疏举证。具体信息详见：机器学习中的范数规则化之（一）L0、L1与L2范数。 调参 调参往往是模型碰到的最后敌人，当准备工作处理完毕后，不同的参数对结果会造成很大的不同，因此调参技巧是机器学习工程师必须要掌握的内容。直接贴一篇大神的文章，剩下的坑以后再补…","tags":[]},{"title":"数据分析扫盲  --  1.传统数据分析","date":"2017-01-19T06:21:08.000Z","path":"2017/01/19/数据分析扫盲  --  1.传统数据分析/","text":"FBIWARNING：本文一切知识点，知识框架均来自于个人对数据分析、数据挖掘、机器学习等方面的理解，推测均出自于本人的臆测，如果对数据分析感兴趣，可以参阅可汗学院的《统计学》、各大学的数据分析和机器学习课程。 数据分析定义 数据分析是指用适当的统计方法对收集来的大量第一手资料和第二手资料进行分析，以求最大化地开发数据资料的功能，发挥数据的作用。 “上帝函数” 吴恩达（机器学习领域四大牛之一）在coursera的《机器学习》中提到，对所有的数据总体而言，有且必然有一条能够完美解释此样本的函数，称为“上帝函数”。但是上帝函数无法通过计算得到（所有的统计，都是对样本的统计，而不是对数据总体的统计；另外对总体而言，即使数据是离散的，函数在复杂样本的数学意义上还是连续的），只能无限的逼近它。 数据分析方向 数据分析的数学基础在20世纪早期就已确立，直到计算机出现才使得数据分析成为可能，在计算机性能不高时，以计算机协助人进行统计学分析以获得数据判断，这种数据分析方法被称为数据分析或传统数据分析。 随着计算机性能增长以及最小二乘法和梯度下降优化思想得以被大规模应用到机器中，数据分析师得以从有价值的信息中更快速、更准确的得到更接近于“上帝函数”的数学结果。针对不同类型的数据集，也有不同的经典模型套用以快速求得最优结果。此时使用者已经不再关注基础统计学指标，而是转向了挖掘数据知识、关系探索等方向，这种数据分析方法被称为数据挖掘或机器学习。 传统数据分析本质 在各种场景下（限定条件/逻辑），应用统计学（工具）对数据分析后的结果进行解释（逻辑），得到更接近真实情况的客观数据帮助决策的一种判断方式。 数据集的描述和定义 传统数据分析工具 在大多数需要统计的商业环境中，大型工具基本只有两种：SPSS和SAS。SPSS属于傻瓜相机，利用定制好的选项和图形界面在略懂统计知识的情况下即可作出符合传统数据分析的结论；如果有更深层的需求，例如银行、金融等环境，SAS能通过SAS语言用编程的形式制作所需的数据模型，从而得到更高定制化、更精确、性能更高的结果；此外，还有统计师使用MATLAB编程来达到所需目标。当然，以上三款软件所需都是不菲。因此在python和机器学习走强的今天，传统的统计工具只能保全大型金融机构这样的大客户，还是走向了衰败。R、oracle等语言也跟着大放异彩。 描述性统计分析 描述性统计分析是传统数据分析的基础，在此过程中需要求出数据分析的基础指标，包括： 平均值：常用的统计指标，受极端值影响。 中位数：数据以升序或降序排列后，处于最中间的数。如果数据呈现二向分布，中位数的表达会受极大的影响，且不能表现数据中极端值的影响。 众数：出现最多的数，缺点同上。 方差/标准差：反应样本个体之间的离散程度，统计学重要指标之一，有较多的应用。 上/下四分位数：数据以升序或降序排列后，25%位置与75%位置的值，常用于箱线图等对数据的图像展示。延伸有四分位数间距 偏度：数据倾向于左边（左偏）或右边（右偏），对股票、公司财务健康等数据有较为重要的意义。 变异系数：数据标准差与平均数的比，反应离散程度，常用于对比不同量纲的数据中消除离散程度。 二项分布 二项分布即重复n次独立的伯努利试验。最简单的二项分布就是掷硬币游戏。虽然二项分布的概率是人类的常识，但二项分布的数学推导并不简单。此外，统计学中许多重要分布例如正态分布、伯努利分布、泊松分布等等都是以二项分布作为基础推导而出的。因此在统计学中，二项分布具有极其重要的意义。 正态分布 正态分布，又称钟形曲线。该分布展示了数据频数与分布域的关系，许多统计定理都是以正态分布为基础，该分布是统计学中的核心。详细描述请参考百度百科。 正态分布之所以有巨大的研究价值，是因为大多数干预或未经干预的数据都呈正态分布状，从自然界各个数值的分布，到人类的行为，产品的偏差，基本都符合正态分布模式（其实是因为中心极限定理，呈现为正态分布）。掌握正态分布理论意味着在不清楚总体分布情况时，可以根据该数据的属性做其是否是正态分布的推测，可以轻易获得某情况较为接近真实存在的概率。 正态分布也是统计学思维一种表现形式，“如果一直猴子坐在电脑前无限的敲打键盘，那么它终有一天能敲出莎士比亚全集”就是其中之一：从正态分布来看，我们无法绝对否定一件事情的发生概率，只能说“很有可能”会产生这样的结果，这也是有很多讽刺统计学家笑话的原因。 ## 泊松分布 泊松分布适合于描述单位时间内随机事件发生的次数。泊松分布在管理科学、运筹学以及自然科学的某些问题中都占有重要的地位。详情见百度百科。 泊松分布可以在掌握部分样本的情况下，推测其值落在各个区间的概率。 ## 经验法则和切比雪夫不等式 当一组数据对称分布时（经验法则）： 约有68%的数据在平均数1个标准差以内。 约有95%的数据在平均数 2个标准差以内。切比雪夫不等式认为在75%个 约有99%的数据在平均数 3个标准差以内。 3个标准差以外的数据，在统计学上称为“离群点”。 当一组数据非对称分布时（切比雪夫不等式）： 约有75%的数据落在平均数 2个标准差以内。 约有89%的数据落在平均数 3个标准差以内。 约有94%的数据落在平均数 4个标准差以内。 实际使用上，切比雪夫不等式的估算效果并不好，只有当经验法则无法使用时（无标准界定，常用于偏度&gt;0.4）才会使用切比雪夫不等式。 大数定理和中心极限定理 大数定理和中心极限定理是统计学中最重要的两条定理，我认为因为这两条定理存在，才能切实的奠定统计学的价值，并且是统计学思维的根源 大数定理：又称大数法则，论证了在大量重复试验的过程中，样本量越多，样本平均值越接近于总体平均值。上文提到的切比雪夫不等式，是大数定理的一种特殊分布。详见维基百科。 中心极限定理：中心极限定理论证了在样本数较大时（通常定义为n&gt;30），样本均值近似正态分布。该定理在大数定理的基础上，给出了收敛的极限分布和渐近方差，更深入的研究了正态分布，是数理统计学和误差分析理论的研究基础。详见维基百科。 概率 概率基础：从三个问题说开去 赌徒谬论 超生游击队员老王已经连生4个闺女了，但老王实在太想要一个男娃，虽然家产都快被村里计生委的人给罚光，但还是要生，他想，都连生4个了，下个肯定是个带把的。 那么问题是，下一个小孩是男孩的概率？ 三门问题 假设你参加了一个电视节目，过关斩将来到了最后一关，此时你看见三扇关闭了的门，其中一扇的后面是一辆法拉利，另外两扇门后是山羊，当你选中了法拉利时，你就可以将其带走；如果选中了山羊，就会空手而归。支持人知道所有门后是什么。 你选定2号门后，主持人为了增加悬念，打开了3号门，门后是一只山羊，此时主持人询问你你是否要更换你的选择。那么问题是，是否应该更换选择？ ### 辛普森悖论 你是一位校长，假设有一天漂亮的秘书跑过来对你说：“校长，不好了，有很多男生在校门口抗议，他们说今年研究所女生录取率是男生的两倍，指责我们学校有性别歧视！” 这时你也很生气，说：“我不是特别交代，今年要尽量提升男生录取率以免落人口实吗？” 秘书赶紧回答说：“确实有交代下去，招生办说今年的男生确实比女生多，可是报告上显示男生所有学科录取率都比女生低了很多。” 那么问题是：有可能在男生所有学科录取率均比女生低的情况下，总性别比例还保持不变吗？ 辛普森悖论.jpg-8.6kB 概率论中需要注意的基础问题 以中国人平均数学水平来说，基础概率的计算基本不是问题，只需要注意一些概率计算的基本假设： 赌徒谬论：当事件与事件之间是相互独立的，不能联合计算概率，百度百科 三门问题：概率是随条件的改变而变化的，概率存在于被给予的条件下，概率不能寄托在实际的物体上，百度百科 辛普森悖论：在某个条件下的两组数据，分别讨论时都会满足某种性质，可是一旦合并考虑，却可能导致相反的结论，百度百科。 组合法则 组合法则是迅速计算在N内取n个有多少种方法的计算公式，体现为： \\[\\left( \\begin{matrix} N \\\\ n \\\\ \\end{matrix} \\right) = \\frac{N!}{(N-n)!} \\] 例如从4个项目中挑选2个来进行投资，则有: \\[\\left( \\begin{matrix} N \\\\ \\end{matrix} \\right) = \\frac{4!}{2! 2!} = \\frac{4\\times3\\times2\\times1}{(2\\times1)(2\\times1)} = 6\\] 贝叶斯定理 通常，事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的，然而，这两者是有确定的关系的；贝叶斯定理通过的是A、B事件转换而得到目标概率，贝叶斯公式的一个用途在于通过已知的三个概率函数推出第四个。 $ P(A|B) = $ 下面通过一道题来理解贝叶斯定理。 对残疾人来说，电动轮椅很难驾驭。假设在房间的某一位置，轮椅使用者可以选择D（穿过房门），S（直行），T（停在桌旁），使用者意愿的概率分别是P(D)=0.5，P(S)=0.2，P(T)=0.3；当轮椅使用者将控制杆转向前时（记为J），则有P(J|D)=0.3，P(J|S)=0.4，P(J|T)=0.05。如果此时使用者将操作杆向前，各种情况的概率各是多少？ 其中P(D|J)的解为： P(D|J) = P(J|D) * P(D) / P(J) P(D|J) = P(J|D) * P(D) / (P(D) * P(J|D) + P(S) * P(J|S) + P(T) * P(T|J)) P(D|J) = 0.3 * 0.5 / 0.245 P(D|J) = 0.612 置信区间 置信区间：置信区间是对分布（尤其是正态分布）的一种深入研究。通过对样本的计算，得到对某个总体参数的区间估计，展现为总体参数的真实值有多少概率落在所计算的区间里（目前国内的统计不接受概率的说法，认为此操作不应属于概率的范畴）。置信水平越高，置信区间就会越宽；在置信水平不变的情况下，样本数量越多，置信区间越窄，且置信区间的需求样本量可以被计算出来。 一般来说，置信区间一般会选择95%。 假设检验 “反证法是统计学者最强大的武器！” – 想不起来谁说的了 假设检验是一种用反证法证明对立条件（原假设/H0），从而判断目前条件（备择假设/Ha）是否正确的证明方式。常见名词如下： P值：观测的显著性水平，通过不同方式计算后，如果P值落在原假设的拒绝域，则拒绝原假设。 接受域：原假设的区间。 拒绝域：拒绝原假设的区间。 I类错误：原假设为真时，拒绝原假设。 II类错误：备择假设为真时，接受原假设。 单样本下的计算方法 在单样本下，置信区间和假设检验主要使用z统计和学生t统计（也称t检验）。 z统计和学生t统计是两种应用正态分布，求置信区间的计算方式。z统计应用于大样本的计算，因为中心极限定理的存在，z统计不要求该样本分布属于正态分布（因为会整理成近似正态分布）；学生t统计量主要应用于小样本的计算，要求总体分布必须近似正态分布，同时所取得的置信区间在同置信水平下也要比z统计得到的结果更宽。 两样本下的计算方法 两样本下，在统一量纲，消除样本均值差后，就可以使用常用统计方法来检验相关性。除了z统计和t统计外，还可使用卡方检验、F检验等。 在SPSS/SAS甚至excel等软件中，以上基础的检验方法都被大规模的修正/革新，所以基础的公式基本已经不会被使用了。 ## 更多样本，更多方法 在更多样本中，不同计算方法的差距会越来越大，传统数据分析更着眼于方差分析等检测样本相关性，简单线性回归线等，因此，样本的抽样和配对方法对模型造成的影响也会增加。除了以上介绍的统计方法，还有F统计，U检定等大项，每种大项下都有许多经过统计学家门修正的方法，在此不多赘述。 抽样组织形式 简单随机抽样：任何样本数为n的抽样几率都是相等的。 等距抽样/系统抽样：以总体N除以样本数n得到整数K，随机选择一个元素为R，以R开始，每隔K个的元素作为样本。 集群抽样：将总数分割成小的集群，再用简单随机抽样抽取小的集群，将集群作为样本。 二段随机抽样：先用集群抽样出集群，再对集群进行集群抽样。 传统统计分析总结 在以上知识的基础上，统计学家们更热衷修正原有的经典检验方法，以逻辑为推进从数学上增强各种推断检验标准，但也人类思维的局限性，模型的解释性强必然造成预测能力的削弱，这也是机器学习专家和统计学家重要的不同之处。统计学家集中了现代数学的精华，从数理上证明了SVM，boosting等理论基础，但是在实践和探索未知知识的路上，统计学家并未作出卓越的贡献。","tags":[]},{"title":"瞎聊聊怎么造个人工智能","date":"2017-01-05T05:13:19.000Z","path":"2017/01/05/瞎聊聊怎么造个人工智能/","text":"0 给黑猩猩一把枪，在教会他开枪之后，他会无聊的把枪丢掉，即使知道这能帮助他杀死自己的敌人。 人类则不一样，人类在大自然中是一个奇怪的、乐于争斗的物种。在电影里，为了营造冲突，人类不停的给自己制造着反派。随着人类认知的发展，我们得不停的为反派升级来试着在电影里毁灭人类，从鬼神、人类本身、外星人到新一代的人工智能。 从《太空漫游》开始，人类就不断在畅想人工智能，《太空漫游》还不错，起码AI不是坏人，接下去就慢慢不太对劲了，《终结者》、《机器公敌》、《鹰眼》，反正一个个人工智能都想毁灭人类。 OK，那我们作为程序员的问题是，怎样才能造一款能毁灭人类的人工智能？ 一个AI，想要迅速成长，一定不能受人力的限；不受人力的限制，一定要实现自学习；要应用自学习的成果，一定要实现代码的自我迭代，让机器自己给自己写代码。 就像《环形使者》一样，AI造出自己的下一版AI，如果AI可用，将会杀死前一个自己，不停的尝试自我毁灭的方式。 1 刚开始思考的时候就陷入一个错误的方向：AI要实现自学习，第一要实现代码识别，分析代码的意图，了解代码的意义，从而学会为了实现这个意义的写作方式；第二要实现提出需求，AI得知道“我需要一个列表来放置元素”才能制造一个列表。以此不断分解业务，由无数小的业务组成大的业务，最终实现自学习。 直到我看到了鸭子预测 &gt; 如果一只鸭子看起来像鸭子，游泳像鸭子，叫声像鸭子，那么它就是只鸭子。 智能生物对明显具有外部特征的定义，会选择只看表象来判断，而不是把它解剖了来确认；同理，机器只要了解通过代码中的数值转换，明白其最终达到了业务目标，那这就是代码的意义，除了结果之外，代码内容只是实现的“工具”而已，AI只需记住这么转换是可行的，下次就照着做准没错–如果错了，就去找下一段代码。 1956年的夏天，40岁的香农和28岁的麦卡锡、明斯基、37岁的罗切斯特及其他六位大神一起举办了朴茨茅斯会议，尝试解决自然语言处理的问题。世界上最聪明的10个人头脑风暴了一个月，最后得到的结论还有没有当代一个机器学习PHD在读脑子里的东西多–他们一度陷入了机器“翻译”是基于“理解”的错误上，实际上，华生实验室90年代的成果表明，机器不需要了解词语的意义，只需要按照大数据的理念找到这种情况的其他做法就行，这一成果一夜之间让机器翻译的准确率从70%提升到了90%。 其实人类对“原理”的理解，也只是用自己的所见去解释而已，我们不知道我们是不是活在《黑客帝国》里，我们不知道是不是三体人的质子正在干扰我们，我们不需要知道这么多。 AI也不需要知道。","tags":[]}]