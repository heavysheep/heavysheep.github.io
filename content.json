[{"title":"解决不了问题就崩了心态太不应该","date":"2017-05-19T06:08:01.000Z","path":"2017/05/19/解决不了问题就崩了心态太不应该/","text":"对于大多数事情来说，失败是常态，无法掌控事情的发展是常态，缺乏安全感的弊端就是恐惧面对这些。问题是越不去面对，挫折固然少了，无法掌控的事情却更多了。要学会坦然面对无法掌控的事情，磨练出更优秀的自己。","tags":[]},{"title":"改善 Python 程序的 91 个建议读书笔记 1","date":"2017-05-15T15:11:31.000Z","path":"2017/05/15/改善 Python 程序的 91 个建议读书笔记 1/","text":"第 1 章 引论 建议 1：理解 Pythonic 概念 Pythonic 当你输入 import this 就会显示 zen of python 美丽胜于丑陋。 显式优于隐式。 简单比复杂好。 复合胜于复杂。 平面比嵌套好。 稀疏比密集好。 可读性是重要的。 特殊情况不足以打破规则。 虽然实用性胜过纯粹。 除了显示错误，错误永远不应该沉默。 代码风格 充分体现python动态语言的特色，类似于 # 变量交换 a, b = b, a # 上下文管理 with open(path, &#39;r&#39;) as f: do_sth_with(f) # 不应当过分地追求奇技淫巧 a = [1, 2, 3, 4] a[::-1] # 不推荐。好吧，自从学了切片我一直用的这个 list(reversed(a)) # 推荐 然后表扬了 Flask 框架，提到了 generator 之类的特性尤为 Pythonic，有个包和模块的约束： 包和模块的命名采用小写、单数形式，而且短小 包通常仅作为命名空间，如只含空的__init__.py文件 建议 2：编写 Pythonic 代码 避免劣化代码 避免只用大小写区分不同的对象 避免使用容易引起混淆的名称 不要害怕过长的变量名 深入认识python有助于编写pythonic代码 全面掌握 python 提供的特性，包括语言和库 随着时间推移，要不断更新知识 深入学习业界公认的 pythoni 代码 编写符合 pep8 的代码规范（就是让你使用pycharm） 建议 3：理解 Python 与 C 语言的不同之处 Python 使用代码缩进的方式来分割代码块，不要混用 Tab 键和空格 Python 中单、双引号的效果相同（个人建议使用单引号，在面对其他语言的双引号源码时不必再转义） 三元操作符：x if bool else y（原因是作者认为应该用可读性更好的方式表达） 用其他方法替代 switch-case 建议 4：在代码中适当添加注释 块和行注释仅仅注释复杂的操作、算法等 注释和代码隔开一段距离 给外部可访问的函数和方法添加文档注释 推荐在文件头中包含 copyright 申明、模块描述等 另外，编写代码应该朝代码即文档的方向进行，但仍应该注重注释的使用 建议 5：通过适当添加空行使代码布局更为优雅、合理 表达完一个完整思路后，应该用空白行间隔，尽量不要在一段代码中说明几件事。 尽量保持上下文的易理解性，比如调用者在上，被调用者在下 避免过长的代码行，超过80个字符应该使用行连接换行（还是让你使用pycharm） 水平对齐毫无意义，不要用多余空格保持对齐 空格的使用要能够在需要使用时强调警示读者（符合PEP8规范） 建议 6：编写函数的 4 个原则 函数设计要尽量短小，嵌套层次不宜过深 函数申明应该做到合理、简单、易于使用 函数参数设计应该考虑向下兼容 一个函数只做一件事，尽量保证函数语句粒度的一致性 Python 中函数设计的好习惯还包括：不要在函数中定义可变对象作为默认值，使用异常替换返回错误，保证通过单元测试等。 # 关于函数设计的向下兼容 def readfile(filename): # 第一版本 pass def readfile(filename, log): # 第二版本 pass def readfile(filename, logger=logger.info): # 合理的设计 pass 最后还有个函数可读性良好的例子： def GetContent(ServerAdr, PagePath): http = httplib.HTTP(ServerAdr) http.putrequest(&#39;GET&#39;, PagePath) http.putheader(&#39;Accept&#39;, &#39;text/html&#39;) http.putheader(&#39;Accept&#39;, &#39;text/plain&#39;) http.endheaders() httpcode, httpmsg, headers = http.getreply() if httpcode != 200: raise &quot;Could not get document: Check URL and Path.&quot; doc = http.getfile() data = doc.read() # 此处是不是应该使用 with ？ doc.close return data def ExtractData(inputstring, start_line, end_line): lstr = inputstring.splitlines() # split j = 0 for i in lstr: j += 1 if i.strip() == start_line: slice_start = j elif i.strip() == end_line: slice_end = j return lstr[slice_start:slice_end] def SendEmail(sender, receiver, smtpserver, username, password, content): subject = &quot;Contented get from the web&quot; msg = MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;) msg[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;) smtp = smtplib.SMTP() smtp.connect(smtpserver) smtp.login(username, password) smtp.sendmail(sender, receiver, msg.as_string()) smtp.quit() 建议 7：将常量集中到一个文件 在Python中应当如何使用常量： 常量名全部大写 将存放常量的文件命名为constant.py 示例为： class _const: class ConstError(TypeError): pass class ConstCaseError(ConstError): pass def __setattr__(self, name, value): if self.__dict__.has_key(name): raise self.ConstError, &quot;Can&#39;t change const.%s&quot; % name if not name.isupper(): raise self.ConstCaseError, \\ &#39;const name &quot;%s&quot; is not all uppercase&#39; % name self.__dict__[name] = value import sys sys.modules[__name__] = _const() import const const.MY_CONSTANT = 1 const.MY_SECOND_CONSTANT = 2 const.MY_THIRD_CONSTANT = &#39;a&#39; const.MY_FORTH_CONSTANT = &#39;b&#39; 其他模块中引用这些常量时，按照如下方式进行即可： from constant import const print(const.MY_CONSTANT) 第 2 章 编程惯用法 建议 8：利用 assert 语句来发现问题 断言的判断会对性能有所影响，因此要分清断言的使用场合： 断言应使用在正常逻辑无法到达的地方或总是为真的场合 python本身异常处理能解决的问题不需要用断言 不要使用断言检查用户输入，而使用条件判断 在函数调用后，当需要确认返回值是否合理时使用断言 当条件是业务的先决条件时可以使用断言 代码示例： &gt;&gt;&gt; y = 2 &gt;&gt;&gt; assert x == y, &quot;not equals&quot; Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AssertionError: not equals &gt;&gt;&gt; x = 1 &gt;&gt;&gt; y = 2 # 以上代码相当于 &gt;&gt;&gt; if __debug__ and not x == y: ... raise AssertionError(&quot;not equals&quot;) ... Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 2, in &lt;module&gt; AssertionError: not equals 运行是加入-O参数可以禁用断言。 建议 9：数据交换的时候不推荐使用中间变量 &gt;&gt;&gt; Timer(&#39;temp = x; x = y; y = temp;&#39;, &#39;x = 2; y = 3&#39;).timeit() 0.059251302998745814 &gt;&gt;&gt; Timer(&#39;x, y = y, x&#39;, &#39;x = 2; y = 3&#39;).timeit() 0.05007316499904846 对于表达式x, y = y, x，在内存中执行的顺序如下： 1. 先计算右边的表达式y, x，因此先在内存中创建元组(y, x)，其标识符和值分别为y, x及其对应的值，其中y和x是在初始化已经存在于内存中的对象。 2. 计算表达式左边的值并进行赋值，元组被依次分配给左边的标识符，通过解压缩，元组第一标识符y分配给左边第一个元素x，元组第二标识符x分配给左边第一个元素y，从而达到交换的目的。 （简单来说，直接交换符合pythonic且性能最佳，这么做就对了） 建议 10：充分利用 Lazy evaluation 的特性 （就是生成器） Lazy evaluation常被译为延迟计算，体现在用 yield 替换 return 使函数成为生成器，好处主要有两方面： 避免不必要的计算，带来性能提升 节省空间，使无限循环的数据结构成为可能 def fib(): a, b = 0, 1 while True: yield a a, b = b, a + b 建议 11：理解枚举替代实现的缺陷 使用 flufl.enum 实现枚举 建议 12：不推荐使用 type 来进行类型检查 使用 isinstance 来进行类型检查（注意上下包含关系就行） 建议 13：尽量转换为浮点类型后再做除法 py2.x:转换浮点类型后再做除法 建议 14：警惕 eval() 的安全漏洞 eval具有安全漏洞，建议使用安全性更好的ast.literal_eval。 建议 15：使用 enumerate() 获取序列迭代的索引和值 &gt;&gt;&gt; li = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] &gt;&gt;&gt; for i, e in enumerate(li): ... print(&#39;index: &#39;, i, &#39;element: &#39;, e) ... index: 0 element: a index: 1 element: b index: 2 element: c index: 3 element: d index: 4 element: e # enumerate(squence, start=0) 内部实现 def enumerate(squence, start=0): n = start for elem in sequence: yield n, elem # 666 n += 1 # 明白了原理我们自己也来实现一个反序的 def reversed_enumerate(squence): n = -1 for elem in reversed(sequence): yield len(sequence) + n, elem n -= 1 （此方式相比从列表里放索引取值更加优雅） 建议 16：分清 == 与 is 的适用场景 比较有趣的： &gt;&gt;&gt; s1 = &#39;hello world&#39; &gt;&gt;&gt; s2 = &#39;hello world&#39; &gt;&gt;&gt; s1 == s2 True &gt;&gt;&gt; s1 is s2 False &gt;&gt;&gt; s1.__eq__(s2) True &gt;&gt;&gt; a = &#39;Hi&#39; &gt;&gt;&gt; b = &#39;Hi&#39; &gt;&gt;&gt; a == b True &gt;&gt;&gt; a is b True 为了提高系统性能，对于较小的字符串会保留其值的一个副本，当创建新的字符串时直接指向该副本，所以a和b的 id 值是一样的，同样对于小整数[-5, 257)也是如此： 注意is不相当于 ==， is 是对 id 方法做的 == 。 建议 17：考虑兼容性，尽可能使用 Unicode python2.x 这是无敌深坑，需要刻苦学习掌握（python3偶尔也会碰到这种问题，但避免了大多数这种可能） 建议 18：构建合理的包层次来管理 module （__init__是对包的头文件定制） 本质上每一个 Python 文件都是一个模块，使用模块可以增强代码的可维护性和可重用性，在较大的项目中，我们需要合理地组织项目层次来管理模块，这就是包(Package)的作用。 一句话说包：一个包含__init__.py 文件的目录。包中的模块可以通过.进行访问，即包名.模块名。那么这init.py文件有什么用呢？最明显的作用就是它区分了包和普通目录，在该文件中申明模块级别的 import 语句从而变成了包级别可见，另外在该文件中定义__all__变量，可以控制需要导入的子包或模块。 这里给出一个较为合理的包组织方式，是FlaskWeb 开发：基于Python的Web应用开发实战一书中推荐而来的： |-flasky |-app/ # Flask 程序 |-templates/ # 存放模板 |-static/ # 静态文件资源 |-main/ |-__init__.py |-errors.py # 蓝本中的错误处理程序 |-forms.py # 表单对象 |-views.py # 蓝本中定义的程序路由 |-__init__.py |-email.py # 电子邮件支持 |-models.py # 数据库模型 |-migrations/ # 数据库迁移脚本 |-tests/ # 单元测试 |-__init__.py |-test*.py |-venv/ # 虚拟环境 |-requirements/ |-dev.txt # 开发过程中的依赖包 |-prod.txt # 生产过程中的依赖包 |-config.py # 储存程序配置 |-manage.py # 启动程序以及其他的程序任务 第 3 章：基础语法 建议 19：有节制地使用 from…import 语句 Python 提供三种方式来引入外部模块：import语句、from…import语句以及__import__函数，其中__import__函数显式地将模块的名称作为字符串传递并赋值给命名空间的变量。 使用import需要注意以下几点： 优先使用import a的形式 有节制地使用from a import A 尽量避免使用from a import * 为什么呢？我们来看看 Python 的 import 机制，Python 在初始化运行环境的时候会预先加载一批内建模块到内存中，同时将相关信息存放在sys.modules中，我们可以通过 sys.modules.items() 查看预加载的模块信息，当加载一个模块时，解释器实际上完成了如下动作： 在 sys.modules 中搜索该模块是否存在，如果存在就导入到当前局部命名空间，如果不存在就为其创建一个字典对象，插入到 sys.modules 中。 加载前确认是否需要对模块对应的文件进行编译，如果需要则先进行编译。 执行动态加载，在当前命名空间中执行编译后的字节码，并将其中所有的对象放入模块对应的字典中。 &gt;&gt;&gt; dir() [&#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;] &gt;&gt;&gt; import test testing module import &gt;&gt;&gt; dir() [&#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;test&#39;] &gt;&gt;&gt; import sys &gt;&gt;&gt; &#39;test&#39; in sys.modules.keys() True &gt;&gt;&gt; id(test) 140367239464744 &gt;&gt;&gt; id(sys.modules[&#39;test&#39;]) 140367239464744 &gt;&gt;&gt; dir(test) [&#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;a&#39;, &#39;b&#39;] &gt;&gt;&gt; sys.modules[&#39;test&#39;].__dict__.keys() dict_keys([&#39;__file__&#39;, &#39;__builtins__&#39;, &#39;__doc__&#39;, &#39;__loader__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;__name__&#39;, &#39;b&#39;, &#39;a&#39;, &#39;__cached__&#39;]) 从上可以看出，对于用户自定义的模块，import 机制会创建一个新的 module。 将其加入当前的局部命名空间中，同时在 sys.modules 也加入该模块的信息，但本质上是在引用同一个对象，通过test.py所在的目录会多一个字节码文件。 （这节说的是，盲目使用from…import…会带来： 1. 命名空间冲突 2. 循环嵌套导入） 建议 20：优先使用 absolute import 来导入模块 （py3 中 relative import方法已被移除，不用操心）","tags":[]},{"title":"为什么我不需要仪式感？","date":"2017-05-08T10:17:18.000Z","path":"2017/05/08/为什么我不需要仪式感？/","text":"正是因为这个世界本就是混乱的、无序的 正是因为这个世界从来不存在什么天理昭昭，善恶有报 我才不想像其他人一样 在漫长幽暗的河流中，用尽全力点亮一座灯塔 告诉别人，这就是我生活的意义，生命的光辉 我的选择不需要仪式感证明 我的梦想不需要仪式感升华 我的力量在于不停的向前蠕动 而不是破茧成蝶","tags":[]},{"title":"YES！产品经理读书笔记 1","date":"2017-05-04T03:28:14.000Z","path":"2017/05/04/YES！产品经理读书笔记 1/","text":"1.产品经理是什么 什么是产品经理 依据公司产品战略，对某个线产品担负根本责任的企业管理人员。 产品经理的职责： 根据公司战略制定所负责产品的战略和目标 根据产品情况调整产品战略和目标 根据公司战略优化产品 根据产品市场反馈为高层提供决策建议 指定有效的竞争分析和竞争策略 对现有产品和新产品进行管理 产品经理的任务 竞争对手分析 产品组合管理 产品品牌管理 市场需求管理 规划和指定相关的产品策略 产品所需资源管理 指定营销策略 2.产品经理应该做什么 知道做什么 发现问题：找到市场需求 找到机会：找准市场位置 创新产品：指定产品发展思路 不认同： &gt; 做对的方向的乌龟，不做错的方向的兔子 知道怎么做 规划产品路线：产品目标是什么 设计产品策略：实现目标的方向 制定年度计划：如何逐步靠拢 选择做的东西，最直接的指标是盈利能力评估 3.产品经理的战术执行 让别人去做 方向引导:制定发展规划 过程管理：控制环节入口和出口，保证实现过程规范化 工作指导：注重其他细节 4.产品管理职业的级别都有哪些 产品助理：业务执行岗，数据收集和分析 产品经理：对企业发展做出合理判断。战略、规划、执行占2:3:5 高级产品经理：战略层面的规划。战略、规划、执行占2:3:5 5.产品经理的职业发展路线 SM：销售管理 MM：营销管理 BM：商业管理 7.什么样的人适合做产品经理 有成本意识：能为公司削减成本 有销售意识：能为公司挣钱 有团队意识：团队协作 有全局意识 不会抱怨：能力未到 不夸夸其谈 不喜欢加班 注重倾听 8.产品经理的人才模型 加分项： 独立思考能力 态度上，坚持自己想法 能力和态度二选一时，选择后者 产品经理素养 1.个人素养 个人修养 创新能力 沟通协调能力 自我管理能力 工作压力承受能力 管理知识 战略知识管理 团队管理知识 时间管理知识 项目管理知识 核心能力 产品需求管理 产品项目管理 新产品管理 产品生命周期管理 产品规范管理 9.产品经理的知识结构是什么 图看不清 10.产品经理是“通”才还是“专”才 通的是方法和过程，专的是方向和目的 11.产品助理应该做些什么 数据整理：收集各类市场、产品、客户、竞争对手有关的数据。 文档管理：各类文档统一的入口和出口。 事务性工作：组织部门会议。 12.产品经理应该知道的产品战略图 产品线的规划 归纳目前产品线 根据特征对产品分类 评估生命周期 评估产品之间的关系 形成文档，即产品战略图（PST） 产品图不管怎么表现，需要体现四个方面的信息 产品线信息 产品生命周期阶段信息 产品组合信息 产品信息 13.产品管理有行业特殊性吗 产品管理在不同行业上的共性 无论哪个行业，根本工作都是给企业规划处有持续盈利能力的产品（目的相同） 无论哪个行业，都是为企业创造价值（内容相同） 无论哪个行业，都需要先知道市场要什么，后规划如何满足需求，在用用企业提供的产品去满足我们认为需要满足的需求，实现价值交换。（流程相同） 产品经理要让企业资源增值 没有不做需求分析的企业，没有不做市场细分的企业，没有不做客户定位的企业 很多时候，看到的不一定是真相，共识的不一定是真理，存在也不是我们要坚持的 14.产品经理是否有行业性限制 鲍尔默的商业规则 一个公司，只有市场份额和现今流最重要 技术只是竞争优势，不是决胜优势 一切以客户和消费者作为企业的经营原则 一切以客户需求开发产品 找出问题！开动大脑！拿出方案！坚持执行！勇往直前！ 产品经理最终要提高的是商业运作能力 （但是商业运作能力本身也是很虚的东西） 15.技术人员如何转型为产品经理 了解自己是否真的不适合做技术工作 全面了解产品经理的工作，评估自己是否真的对这个工作有兴趣 岗位和思维都要转，要从懂产品变成懂业务 了解公司产品管理体系架构 从实践中获取第一手的市场业务信息 16.产品管理的工作流程是什么 战略活动 预测问题 培育机会 创新产品 ### 规划活动 产品路线 产品策略 产品规划 ### 战术活动 概念化 图纸化 技术化 商品化 市场化 PRD：产品需求文档 MRD：市场需求文档 17.产品管理工作的文档管理 文档作用 记录介质 过程推动 抽象体现 文档类型 流程类 控制类 决策类 文档的规范 文档模板规范 文档编号规范 文档存取规范 文档撰写规范 文档级别规范 18.产品经理需要了解的26个文档 10个必须要写的文档 D3：需求矩阵表 D9：商业方案（必须精通） D12：产品路线文档 D15：产品策略文档 D18：商业需求文档（必须精通） D19：市场需求文档（必须精通） D20：产品需求文档（必须精通） D21：产品验收文档 D24：产品白皮书 D25：产品总结报告 19.产品管理部归于何处 懂的多只能说明你的知识丰富，爱学习。只有做的好才说明你能力强悍，货真价实。 任何一次变革都是对现有利益的重新分配，这是方法，又是目的。 用户购买一个产品的流程： 1. 发现的过程：知道我们宣传的产品价值是否和他期望的利益一致。 2. 选择的过程：让用户选择我们的产品而不是竞争对手产品 3. 购买的过程：寻找性价比最高的产品 4. 使用的过程：决定是否购买 20.产品部和业务部门的利益之争 不停的妥协和不停的打压都是不可取的 在关系的夹缝中，要做到不偏不倚，实事求是","tags":[]},{"title":"sklearn学习笔记-3 回归模型","date":"2017-04-26T13:42:52.000Z","path":"2017/04/26/sklearn学习笔记-3 回归模型/","text":"回归模型： 线性回归模型 感知器模型 最简单的模型，多层感知器MLP即最简单的深度学习网络。 感知器模型通过循环修正计算误差进行修正，因此没有没有学习率；在学习中，需要指定学习次数（例如学习样本数），否则感知器模型会无休止的进行下去。 基础（单层）感知器模型是非常弱的模型，不会存在过拟合的情况。 sklearn.linera_model.Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, eta0=1.0, n_jobs=1. random_state = 0, class_weight=None, warm_start=False) 参数 penalty：正则惩罚项，默认为0 fit_intercept：截距 n_iter：迭代次数 shuffle：洗牌 verbose：打印日志等级 eta0 ：学习率 n_jobs：启动线程数，-1代表最大 实例 df = pd.read_csv(r&#39;D:\\BaiduYunDownload\\sklearn\\3.0 线性模型\\adultTest.csv&#39;) #将object变量（字符串）制成稀疏矩阵，class是目标变量，不用放入 dfNew=pd.get_dummies(df,columns=[&#39;workclass&#39;,&#39;education&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;, &#39;native-country&#39;]) dfNew[&#39;target&#39;] = dfNew[&#39;class&#39;].apply(lambda x: 1 if x == r&#39; &lt;=50K&#39; else 0) # 将结果二分为1和0 xdata = dfNew.drop([&#39;class&#39;,&#39;target&#39;],axis = 1) # 测试集 ydata = dfNew[&#39;target&#39;] # 结果集 from sklearn.linear_model import Perceptron # 调用感应器模型函数 per = Perceptron(n_iter=20) # 创建对象并设定迭代次数为20 per.fit(xdata,ydata) # 训练模型 print(per.predict(xdata)) # 对模型进行预测 print(per.score(xdata,ydata)) # 对模型进行准确率评估 tips：感知器没有概率值 线性回归模型 sklearn.linear_model.LinearRegression9(fit_intercept = True, normalize = False, copy_X = True, n_jobs = 1) fit_intercept：是否计算截距 normalize：是否对整个样本进行数据标准化，线性模型并不依赖于正则，所以此参数无意义。 线性回归模型需要接受一个连续性的变量，以达到预测。 在线性模型中，更复杂的模型会用到alpha惩罚项，简称a，并且会通过数据集的定义方式选择raige函数和lasso函数。 raige和lasso的区别 raige接受l2的正则，lasso接受l1的正则。 raige：(w1)2+(w2)2+(w3)^2…，每个权重平方相加；lasso:|w1|+|w2|+|w3|…，每个权重绝对值相加。 lasso接受的l1范数，所以得到的解可能是稀疏的，导致很多系数的权重都为0；raige接受l2的范数，得到的解不会为0。 从方便的角度考虑，我们更多的使用l2范数的raige。 交叉验证（Cross validation) 交叉验证用于防止模型过于复杂而引起的过拟合。有时亦称循环估计，是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析，而其它子集则用来做后续对此分析的确认及验证。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。 在sklearn的在线性模型中，为了保证模型值，我们可以使用后续加CV的函数来进行交叉验证： alpha：接受一个数组，数组内是所有认为有效的a的值，函数会自动选择最优值 cv：接受交叉验证的切分份数 scoring：评估指标，不填写会选择默认指标进行判断，也接受填写最小绝对值误差/最小均方误差，一般不填写 其他同上 在进行CV时，有时可能会遇到被切分的样本内没有1，或者样本不均匀的问题，因此我们调用分层函数进行解决: from sklearn.cross_validation import StratifiedKFold#调用分层函数 sf = StratifiedKFold(ydata,n_folds=5) # 创建对象并选择分为几层，避免发生稀疏矩阵的数据不均匀的问题 这种情况在分类模型中才会发生，回归模型的y是连续值，因此不必进行分层。","tags":[]},{"title":"sklearn学习笔记-2 数据处理","date":"2017-04-17T14:54:52.000Z","path":"2017/04/17/sklearn学习笔记-2 数据处理/","text":"稀疏和非稀疏 在矩阵中，若数值为0的元素数目远远多于非0元素的数目时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。稀疏被称为L1，非稀疏被称为L2。 2.1 数据标准化 特征决定了模型的上限，算法决定了如何逼近这个上限 对业务了解的越多，提出的特征越好 sklearn的数据处理能力现在非常强大，能处理文本、图片或更多的数据 在数据量纲不一样时，需要对数据进行标准化，一般来说，涉及到梯度下降的模型，和涉及到距离的模型，都需要做数据标准化。 定义 标准化1(常用的标准化）：减去一列的平均数再除以标准差，使其符合平均数为0标准差为1的高斯分布。 标准化2：压缩数据至0到1 树类模型不需要标准化。 标准化不能在训练集和测试集上分开进行（两个集的平均数和标准差不同），应当在测试集上应用训练集的标准化（减去标准化的平均数，除以标准化的标准差）。 把训练集和测试集放在一起做标准化也不是一种较好的办法，最优的办法是在训练集上做标准化，并transform到测试集中。 scale 用于针对列的数据标准化 函数 sklearn.preprocessing.scale(X,axis=0,with_mean=True,with_std=True,copy=True) X传入数组，支持pandas数据框,axis=0按列处理，mean=True平均数接近0，std=True标准差接近1 方法 sklearn.preprocessing.StandarScaler(with_mean=True,with_std=True,copy=True) 相比方法，函数很少被用到，因为该方法没有transform接口。 当数据不以时间为顺序时，可以随机划分训练集和测试集。 随机划分数据集 from sklearn.cross_validation import train_test_split#调取切割函数 X_train, X_test, y_train, y_test = train_test_split(df1,df[&#39;area&#39;],train_size=0.7) # 数据参数，数据结果，切分为测试集的比例 # X_train是被切分为训练集的比例，X_test是被切分为测试集的比例，y相同 在一个函数内训练两个集，目的是为了不破坏索引顺序 标准化数据1（正态数据） from sklearn.preprocessing import StandardScaler#调取标准化函数 ss=StandardScaler()#创建标准化对象 ss.fit(X_train)#以训练集训练标准化对象 X_train_ss=ss.transform(X_train)#将训练结果应用于训练集 X_test_ss=ss.transform(X_test)#将训练结果用于与测试集 此时，返回的结果都是numpy形式的多维数组。 每一组的平均值无限接近于0，标准差无限接近于1。 标准化数据2（压缩数据） from sklearn.preprocessing import MinMaxScaler#调取压缩函数 mms = MinMaxScaler()#创建标准化对象 mms.fit(X_train)#依然以训练集训练标注化对象 #... 以上标准化方法，都是对每一列（每一个特征进行标准化） normalize 对整个样本进行数据标准化（正则） 函数 sklearn.preprocessing.normalize(X, norm=‘12’, axis=1 ,copy=True, return_norm=False) 方法 sklearn.preprocessing.Normalizer(norm=‘12’, copy=True) normalize针对样本，但样本间是独立的，因此该方法的transform无意义（整个样本都被转换了，单个使用时就不必transform），只会在papline中有意义。 该方法只会用于计算距离、无监督学习的聚类中使用。默认接受L2 （稠密矩阵） from sklearn.preprocessing import Normalizer # 调用正则函数 norm = Normalizer() # 创建初始化对象 norm.fit((X_train)) # 虽然无意义，仍需要按照标准书写 norm.transform(X_train) # 虽然无意义，仍需要按照标准书写 norm.transform(X_test) # 虽然无意义，仍需要按照标准书写 2.2 分类值与缺失值 binarizer 用作数据二分化的分类，将一个连续性变量改为离散变量。 数据离散化的好处：数据存储空间小，并且因为0不参与运算（相加为原值，相乘为0）使运算速度非常快。 离散化也能较好的处理缺失值，可以将缺失值单独的形成一个特征，出现缺失值的行为1. 逻辑回归非常偏好离散化完成的数据 from sklearn.preprocessing import Binarizer#调取二分数据函数 bi = Binarizer(548)#创建对象定义分类标准，大于该值会被分为1，小于为0 DC_bi=bi.fit_transform(df[&#39;DC&#39;])#调用对象进行分类 当使用一列表示象征性的分类时，不要使用数字，因为程序sklearn会认为该特征的大小和顺序是有意义的，正确的做法是对少量参数进行稀释矩阵。 构建稀疏矩阵：skleran的OneHotEncoder非常不好用还容易报错，因此我们使用pandas的get_dummies()。 pd.get_dummies(data=df, columns=[&#39;month&#39;,&#39;day&#39;,&#39;DC_bi&#39;]) 缺失值 缺失值有时是真实存在的，缺失是存在意义的，并且当数据不够大时，删除缺失值只会浪费样本。 因此，更好的方式是填充缺失值，填充方式尽量通过业务的理解来进行填充。 from sklearn.preprocessing import Imputer#调用填充缺失值函数，该函数默认填充平均值 im=Imputer()#创建对象 dfna=im.fit_transform(df[&#39;DC_na&#39;])#选出需要填充的列并输出列 Tips： 1. 这里有错误，训练出来的列长度并不等于原来的长度，因此还是寄希望于pandas来处理 2. sklearn有些函数不支持缺失值或无限大、无限小的数据，因此通过pandas将此类数据设为缺失值，并且一次性的处理它。 3. 缺失值可以使用-999代替。 2.3 变量选择 特征选取 共线性特征，只保留一个就可以，可以用皮尔逊共线测试一下。 当特征过多时，只保留重要特征，删除掉不重要的参数也可以提高准确率。 针对稀疏矩阵判断特征的两种方式： 1. 使用lasso来选择特征 from sklearn.linear_model import Lasso # 从线性模型这种调取能判断参数重要性的函数lasso() lasso = Lasso() # 创建对象 lasso.fit(xdata,ydata) # fit不接受缺失值，使用前记得填充缺失值为-999。第一个参数为参数df，第二个参数为结果df print(lasso.coef_) # 打印特征重要程度，可以删除无限接近于0的特征 所有的稀疏矩阵类型，都可以放入此函数进行判断。 使用featureselect来选择特征： from sklearn.feature_selection import SelectFromModel # 调用参数选择模型 model = SelectFromModel(lasso) # 创建对象并直接传入模型，相当于省略了fit步骤 model.fit_transform() # 无意义，为了是一个popline，另外sklearn不接受异常值，这里因为极大和极小值的存在，会报错。 支持LR回归，简单线性回归 当特征是连续性，我们希望方差越大越好 特诊判断 基于树形模型对特征进行判断： from sklearn.ensemble import RandomForestRegressor#从随机森林库中调取树类特征选择函数 rf = RandomForestRegressor()#创建对象 rf.fit(xdata,ydata)#训练模型，第一个参数为参数df，第二个参数为结果df print(rf.feature_importances_)#打印特征重要程度，数值越接近于0，意义越小 提到的pandas相关操作 .head():读取前五行数据 df.loc[ : ,x:y]:索引选取x到y列的数据 df[0, : ]维度转换 pd.cut(df[‘’],n)：将padans中的一列均匀的切分成n份，返回的值是一个字符串。 pd.get_dummies(data, prefix = None, prefix_sep = ’_’, dummy_na = False, columns=None, sparse = False, drop_first = False)：函数接受一个df表格以及使用columns选取的列，函数会删除原来的列并对其进行稀疏化矩阵的扩充，使其变为0/1形式，返回一个重新构建好的df表格。 df.loc[df[‘DC’]&gt;=600,‘DC_na’] = np.nan ：对df行中的DF列大于等于600的，新建一列设为NaN。 df.replace(np.inf,np.nan)：替换过大值为NaN","tags":[]},{"title":"天池赛IJCAI-17 口碑商家客流量预测 解题思路","date":"2017-03-22T13:40:03.000Z","path":"2017/03/22/天池赛IJCAI-17 口碑商家客流量预测 解题思路/","text":"赛题与数据 代码 基本数据创建 result：每家店铺每日交易成功数量 view：每家店铺每日浏览量 参数分解 shop_info shop_id city_name location_id per_pay score comment_cnt shop_level cate_name.. 商家id 城市名 所在位置编号 人均消费 评分 评论数 商铺等级 分类 shop_id：主键，索引 city_name：获取气温、消费能力、消费习惯 location_id：聚类算法，估计功效太低没什么意义，pass per_pay：检测与result负相关，与view负相关。 socre：检测与result正相关，与view正相关。 comment_cnt：检测与result正相关，与view正相关。 shop_level：检测与result正相关，与view正相关。 cate_name：分类太细，考虑只保留使用“超市”和“美食”进行区分。 检测per_pay、score、comment_cnt、shop——level与view、result的关联度。 score有很大的问题：这个值是处于变动的。 user_pay user_id shop_id time_stamp 付费用户id 商家id 消费时间 time_stamp：分解出日期day和时间time列。 user_view user_id shop_id time_stamp 浏览用户id 商家id 浏览时间 time_stamp：分解出日期day和时间time列。 特征工程 1.考虑到口碑是2015年6月23日开始发布，必然遭遇冷启动和虚假数据问题，那么时间序列中，体现趋势的指标应该是7日移动平均线ma，影响最大的特征因子应该是最近一次的ma_7。 2.城市天气逻辑体现非常重要，主要划分了三级（晴，小雨/小雪/，大雨/雪），但划分后的效果并不很好。 3.当日是否为工作日，次日是否为工作日比较重要。 4.16年情人节到过年的那周视为噪音。 5.GDP作为特征果然没效果，删了。 感受 1.以不同可索引对象制造的模型再融合有巨大威力，第一次瞎配的权重都带来了最好的提升。 2.solo的问题不在于想法…判断出哪个想法提升最多是最重要的，当然这需要经验。 3.xgboost因为bug跑不起来，没时间走ARIMA，也没时间再上prophet，凄苦…水平不够时候有队友提升会比较快。 4.合理利用每日评分确定正确方向是非常有必要的，相信前几的差距已经是谁对趋势判断更敏锐了。 5.全身心的投入大约勉强能进前200，看wepon大神的blog，对底层的理解还是很重要，今后要加强学习和训练。","tags":[]},{"title":"最近在做测试","date":"2017-03-17T06:25:46.000Z","path":"2017/03/17/最近在做测试/","text":"最近一直在忙测试相关工作，记录下测试工作心得： 1.重要的不仅只是懂业务流程，也要把思路抽离在流程之外。因为产品在设计时，往往会先定下大的方向，再逐渐寻找最优路线，在不停的修改中，也许已经偏离“可用”的轨道，此时测试应该代入的是用户的思路，对测试时找到的BUG，提出期望结果；对不合适的流程，也要提出优化期望。 2.细化的测试用例很重要，一方面可用于自动化测试的架构，另一方面，越细的测试用例，在写的时候就能发现非常多的问题。 3.如果作为懂技术的测试人员，对于测出的问题，应该做到比写这个功能的开发了解的更透彻，这也是一个测试人员平庸和杰出的分水岭。 4.测试人员是产品和开发扯皮中的重要第三方，沟通非常重要。 5.同上，测试“可信任”也是非常重要的，对能复现的问题，一定要通过复现从而找出问题；对不能复现的问题，记录下相关操作以便将来找出原因，直觉是知识、感官的积累，对于难以找到原因的问题，直觉非常重要。 6.最好的用例就是文档，文档即用例。","tags":[]},{"title":"Paxos算法简易理解","date":"2017-02-24T05:32:31.000Z","path":"2017/02/24/Paxos算法简易理解/","text":"简介 Paxos算法简单来说就是一个在异步分布式系统里用以保证一致性的投票算法，严格来说，Paxos并不是一个算法，而是一个经过严谨逻辑推导出的投票方式。该算法由莱斯利·兰伯特于1990年提出，只由一些必须要被满足的基本条件推导得出，因此Paxos是在所有分布式算法中最简单，同时也是最稳定的算法。 Paxos应用于“基于消息传递且要求具有高度容错特性”的场景，在“共享内存”的分布式场景中并不适用，另外，虽然有许多基于Paxos的一致性算法，但都没有paxos稳定。 谷歌在其分布式锁服务（Chubby lock）中应用了Paxos算法。Yahoo!开源的ZooKeeper是一个开源的类Paxos实现，而hadoop也支持ZooKeeper协议。 这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品 – Mike Burrows 算法假设 1.不存在拜占庭将军问题（一条消息可能被传递两次，但在传递中不会出错） 2.只要等待足够的时间，消息就会被传到(信息必定到达） 3.每个节点对于信息，接收到信息即以为赞同，同时只有接受和忽略，没有反对。（只可根据赞成数来确定结果） 4.信息不一定会被成功传递，也无法确定传递时间。（包容传递失败，并确定在此条件下也能保持一致性） ##算法定义 在一个分布式系统中，每个节点会有最少1种，最多3种身份，分别是proposers（提案者）、acceptors（批准者）、learners（接受者），他们在节点中相互传递信息被称为value（决议）。 1.决议只能被提案者提出，同时随决议附有一个编号，编号是递增且唯一的； 2.任何两个提案编号之间构成偏序（意味着提案者的编号大小是有意义的；同一次提案，提案者的权限是有等级分别的）； 3.在一次算法执行中，每个批准者一次只批准一条决议，并且只与最新的决议形成交互（意味着可能存在信息队列）； 4.收到超过半数的表态，即视为通过决议，并向所有人广播； 5.接受者只能获得最终被批准的决议。 需要注意的是，节点可以同时拥有三种身份，当节点既是提案者，又是批准者时，他必定会为自己提出的决议投出一票。 实例 东方网准备开一场主题为“智橙生活未来发展方向”的会议，与会人员分别是董事长何XX、副董事长徐XX、纪委书记金X、副总裁高X、技术总监老王。与会人员都可以提案和赞同，一人一票。其他未与会人员例如开发、产品、数据分析师等，只能接受讨论结果（定义5）。 第一天：董事长提案智橙生活应该主打“上海市民离不开的产品”，其他4人纷纷表示“赞同”，董事长收到其他4人的表态，开心的表示全票通过，向所有人广播“产品基调定好，新的提案不得再讨论本问题”，全场统一。 第二天：董事长提案智橙生活应该添加“为民办事”功能，老王不能装作听不见，只好表示“赞同”（假设3），副总裁和纪委书记睡着了，无法表态，董事长等了半天只等到了两个表态，表示少两人老子照样干，加上自己一票通过了决议（定义4），向所有人广播“功能确定，新的提案不得再讨论本问题”，全场统一。 第三天：副总裁和纪委书记睡醒了，发现了董事长的提案，但同时（或更早）收到了提案结果的广播，于是他们不必再对此提案表态，全场统一。 第四天：轮到老王提案，他提出了“打磨产品，塑造核心功能点”的主旨，这时其他4位领导都睡着了，老王未获得任何表态。决议失败，不能向任何人广播。只能重新提案，全场统一。 第五天：4位领导终于都醒了，这时董事长提出了“尽可能多的造功能，我们要做一款万能APP”的主旨，老王再次提出昨天的提案，其他三位领导先听见了董事长的训示，纷纷表态支持，这时才反应过来老王有一份同样的提案，这时有两种情况： 老王职级低：其他领导表态后收到了老王的提案并发现是相同的提案（定义3），但老王的职级比他们的低，因此会忽略老王的提案，全场统一。 老王职级高：其他领导表态后收到了老王的提案并发现是相同的提案（定义3），他们会向老王回复之前已经通过了的提案及其内容并忽略投票，这样老王会知道自己的提案无望，全场统一。 第六天：董事长和老王同时说出对一件事的提案，并且获得邻座（近邻节点）的迅速反馈，此时老王邻座的副总裁和纪委书记先向老王表态“赞同”，当董事长的提案到来时，他们会再次表示“赞同”。而副董事长先向董事长的提案表示“赞同”后，收到了老王的提案，此时他将向老王表示“已有大人物关注此事，你不要插手”（定义2）。当信息传递结束，高权限必定获得更高的“赞同”，提案通过，全场统一。 第七天：老王总结出了深刻的人生哲理：在异步的任何情况下，即使某一节点出于某些原因无法接受信息，也绝不影响该节点以及总体对于一致性的检测，并且为了保证异步一致，权限相对较低的节点在提案和投票中处于较不利的位置。","tags":[]},{"title":"使用sklearn进行数据预处理 —— 归一化/标准化/正则化","date":"2017-02-18T15:23:34.000Z","path":"2017/02/18/使用sklearn进行数据预处理 —— 归一化-标准化-正则化/","text":"本文主要是对照scikit-learn的preprocessing章节结合代码简单的回顾下预处理技术的几种方法，主要包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理。内容比较简单，仅供参考！ 首先来回顾一下下面要用到的基本知识。 ## 一、知识回顾 均值公式： \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\] 方差公式： \\[s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\] 0-范数，向量中非零元素的个数。 1-范数： \\[||X||=\\sum_{i=1}^{n}|x_{i}|\\] 2-范数： \\[||X||_{2}=(\\sum_{i=1}^{n}x_{i}^{2})^{\\frac{1}{2}}\\] p-范数的计算公式： \\[||X||_{p}=(\\sum_{i=1}^{n}x_{i}^{p})^{\\frac{1}{p}}\\] 数据标准化：当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。 二、标准化(Standardization)，或者去除均值和方差进行缩放 公式为：(X-X_mean)/X_std 计算时对每个属性/每列分别进行. 将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1。 首先说明下sklearn中preprocessing库里面的scale函数使用方法： 1sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True) 根据参数的不同，可以沿任意轴标准化数据集。 参数解释： X：数组或者矩阵 axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard + deviations. 如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。 with_mean: boolean类型，默认为True，表示将数据均值规范到0 with_std: boolean类型，默认为True，表示将数据方差规范到1 一个简单的例子 假设现在我构造一个数据集X，然后想要将其标准化。下面使用不同的方法来标准化X： 方法一：使用sklearn.preprocessing.scale()函数 方法说明： X.mean(axis=0)用来计算数据X每个特征的均值； X.std(axis=0)用来计算数据X每个特征的方差； preprocessing.scale(X)直接标准化数据X。 将代码整理到一个文件中： 12345678910111213from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) # calculate mean X_mean = X.mean(axis=0) # calculate variance X_std = X.std(axis=0) # standardize X X1 = (X-X_mean)/X_std # use function preprocessing.scale to standardize X X_scale = preprocessing.scale(X) 最后X_scale的值和X1的值是一样的，前面是单独的使用数学公式来计算，主要是为了形成一个对比，能够更好的理解scale()方法。 方法2：sklearn.preprocessing.StandardScaler类 该方法也可以对数据X进行标准化处理，实例如下： 1234567from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) 这两个方法得到最后的结果都是一样的。 三、将特征的取值缩小到一个范围（如0到1） 除了上述介绍的方法之外，另一种常用的方法是将属性缩放到一个指定的最大值和最小值(通常是1-0)之间，这可以通过preprocessing.MinMaxScaler类来实现。 使用这种方法的目的包括： 1、对于方差非常小的属性可以增强其稳定性； 2、维持稀疏矩阵中为0的条目。 下面将数据缩至0-1之间，采用MinMaxScaler函数 1234567from sklearn import preprocessing import numpy as np X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]]) min_max_scaler = preprocessing.MinMaxScaler() X_minMax = min_max_scaler.fit_transform(X) 最后输出： 123array([[ 0.5 , 0. , 1. ], [ 1. , 0.5 , 0.33333333], [ 0. , 1. , 0. ]]) 测试用例： 1234X_test = np.array([[ -3., -1., 4.]]) X_test_minmax = min_max_scaler.transform(X_test) X_test_minmax array([[-1.5 , 0. , 1.66666667]]) 注意：这些变换都是对列进行处理。 当然，在构造类对象的时候也可以直接指定最大最小值的范围：feature_range=(min, max)，此时应用的公式变为： 12X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0)) X_minmax=X_std/(X.max(axis=0)-X.min(axis=0))+X.min(axis=0)) 四、正则化(Normalization) 正则化的过程是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用。 该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础. Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。 方法1：使用sklearn.preprocessing.normalize()函数 12345678&gt;&gt;&gt; X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] &gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm='l2') &gt;&gt;&gt; X_normalized array([[ 0.40..., -0.40..., 0.81...], [ 1. ..., 0. ..., 0. ...], [ 0. ..., 0.70..., -0.70...]]) 方法2：sklearn.preprocessing.StandardScaler类 123&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X) # fit does nothing &gt;&gt;&gt; normalizer Normalizer(copy=True, norm='l2') 然后使用正则化实例来转换样本向量： 123456&gt;&gt;&gt; normalizer.transform(X) array([[ 0.40..., -0.40..., 0.81...], [ 1. ..., 0. ..., 0. ...], [ 0. ..., 0.70..., -0.70...]]) &gt;&gt;&gt; normalizer.transform([[-1., 1., 0.]]) array([[-0.70..., 0.70..., 0. ...]]) 两种方法都可以，效果是一样的。 五、二值化(Binarization) 特征的二值化主要是为了将数据特征转变成boolean变量。在sklearn中，sklearn.preprocessing.Binarizer函数可以实现这一功能。实例如下： 12345678910&gt;&gt;&gt; X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] &gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X) # fit does nothing &gt;&gt;&gt; binarizer Binarizer(copy=True, threshold=0.0) &gt;&gt;&gt; binarizer.transform(X) array([[ 1., 0., 1.], [ 1., 0., 0.], [ 0., 1., 0.]]) Binarizer函数也可以设定一个阈值，结果数据值大于阈值的为1，小于阈值的为0，实例代码如下： 12345&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1) &gt;&gt;&gt; binarizer.transform(X) array([[ 0., 0., 1.], [ 1., 0., 0.], [ 0., 0., 0.]]) 六、缺失值处理 由于不同的原因，许多现实中的数据集都包含有缺失值，要么是空白的，要么使用NaNs或者其它的符号替代。这些数据无法直接使用scikit-learn分类器直接训练，所以需要进行处理。幸运地是，sklearn中的Imputer类提供了一些基本的方法来处理缺失值，如使用均值、中位值或者缺失值所在列中频繁出现的值来替换。 下面是使用均值来处理的实例： 12345678910&gt;&gt;&gt; import numpy as np &gt;&gt;&gt; from sklearn.preprocessing import Imputer &gt;&gt;&gt; imp = Imputer(missing_values='NaN', strategy='mean', axis=0) &gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]]) Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0) &gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]] &gt;&gt;&gt; print(imp.transform(X)) [[ 4. 2. ] [ 6. 3.666...] [ 7. 6. ]] Imputer类同样支持稀疏矩阵： 12345678910&gt;&gt;&gt; import scipy.sparse as sp &gt;&gt;&gt; X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]]) &gt;&gt;&gt; imp = Imputer(missing_values=0, strategy='mean', axis=0) &gt;&gt;&gt; imp.fit(X) Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0) &gt;&gt;&gt; X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]]) &gt;&gt;&gt; print(imp.transform(X_test)) [[ 4. 2. ] [ 6. 3.666...] [ 7. 6. ]] sklearn相关英文版本:Preprocessing data 中文版本:数据预处理 本文提取自：http://blog.csdn.net/dream_angel_z/article/details/49406573","tags":[]},{"title":"windows 10 github page + hexo + pandoc + next 搭建博客","date":"2017-02-13T09:32:25.000Z","path":"2017/02/13/windows 10 github page + hexo + pandoc + next 搭建博客/","text":"之前根据crossin的编程教室一系列教程使用githubpage + hexo + yilia成功搭建了博客，但是知其然不知其所以然，在尝试转变风格为next的时候花了不少功夫，痛定思痛之后决定把这些记录下来，以备将来再次安装使用。 简单的教程很多就不再赘述，直接粘贴当时看的教程。 用 GitHub + Hexo 建立你的第一个博客 部署博客及更新博文 安装自己喜欢的主题 更换markdown渲染引擎 hexo默认只支持最基础的markdown渲染，为了实现现代化的功能（囧），改用pandoc来进行渲染。 首先前往pandoc官网下载并安装pandoc，安装成功后测试pandoc --help命令以确定安装成功。 之后安装pandoc作为hexo的渲染引擎，进入hexo目录后，输入以下命令： npm uninstall hexo-renderer-marked --save npm install hexo-renderer-pandoc --save pandoc和基础markdown语法有细微不同，具体细节可以去官网查阅。 支持LATEX数学公式 hexo目录中，输入以下命令： npm install hexo-math --save hexo math install 并且在网站配置的_config.yml文件中添加： plugins: - hexo-math 最后有一点许多教程都没有提到的，记得去正在使用的主题配置_config.yml中，将MathJex相关支持设定为true。 更换主题为next 依然在hexo目录中，安装next主题 git clone https://github.com/iissnan/hexo-theme-next themes/next 打开站点配置文件_config.yml修改主题为next theme: next 相关建议设置next官网有清楚的描述。 使用Hexo的内建归档categories 作者：碎瞳Artin 链接：https://www.zhihu.com/question/33324071/answer/58775540 来源：知乎 著作权归作者所有，转载请联系作者获得授权。 1.第一步：生成post（文章）时默认生成categories配置项：在根目录下scaffolds/post.md中，添加一行categories:。同理可应用在page.md和photo.md，示例如下： title: {{ title }} date: {{ date }} tags: categories: # 此处为添加内容 --- 2.第二步：在实际写作时，在开头进行categories配置。例如： title: Hello，World!你好，世界！ date: 2014-01-21 23:33:02 tags: 写作 categories: 随笔 # 配置categories 这样在文章发布时，在git中使用hexo g命令，hexo会在根目录/public/categrises下自动生成归档文件夹，如图： image_1b8php9vm1tb71u91170v1kusmrqg.png-26kB 3.第三步：配置博客首页归档展示样式。在主题配置文件themes/_config.yml中添加以下代码（#号后为注释内容）: menu: home: / essay: /categories/随笔 # 博客首页展示文本/访问路径/自定义归档名称 write: /categories/写作 read: /categories/阅读 study: /categories/学习 code: /categories/编程 4.补充说明：如果发现博客首页展示文本为英文，需要改为中文显示，需要修改先博客根目录下的_config.yml文件的language配置，示例如下： # Site title: My Blog subtitle: description: author: language: zh-CN # 修改此处，一般默认为default.yml，原生英文显示 timezone: 然后为实现文章归档名称显示为中文，接着再修改主题配置文件下language/zh-CN.yml即可，示例如下： title: archive: 归档 category: 分类 tag: 标签 menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 essay: 随笔 # 编辑代码时注意语法规范如缩进、空格等 read: 阅读 # Hexo采用yml语法，具体可自行搜索 write: 写作 5.最终展示效果（图中红框作强调用）： image_1b8phsloq155m1dbv9f9cgd6g4t.png-41.6kB 6.点进某一归档分类如“阅读”，博客文章会依照归档配置，排序显示如下： image_1b8pht2l6ifh1neh1k4i3hgsg21a.png-27.1kB **注：目前下载安装的hexo貌似都没有zh-CN.yml，而以zh-Hans代替，第四步修改对应文件即可。 上传博客常用命令 hexo目录下： # 清理缓存 hexo clean # 生成静态文件 hexo generate # 本地预览（在4000端口） hexo s # 提交至网站 hexo deploy 文件前的title各标签视主题而有所不同。","tags":[]},{"title":"数据分析扫盲  --  2.机器学习","date":"2017-01-22T02:39:25.000Z","path":"2017/01/22/数据分析扫盲  --  2.机器学习/","text":"FBIWARNING：本文一切知识点，知识框架均来自于个人对数据分析、数据挖掘、机器学习等方面的理解，推测均出自于本人的臆测，如果对数据分析感兴趣，可以参阅可汗学院的《统计学》、各大学的数据分析和机器学习课程。 基础知识 机器学习定义 机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法 – 维基百科 简单从实现上来说，机器学习就是使用机器学习算法来得到目标数据集的最优近似函数的过程。 监督和无监督 监督学习所得到的结果都是已知的，即结果集是可预估的。常见的监督学习算法包括回归分析和统计分类。 无监督学习所得到的结果是未知的，即结果集是不可预估的，常见的无监督学习算法有聚类。 半监督学习继承了监督学习与无监督学习，往往采用少量的数据集使用监督学习得到部分结果，再以大量的无监督学习对结果进行验证和修正。 增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。 数据挖掘 数据挖掘这一词语常与机器学习相混淆，实际上，数据挖掘的总体目标是从一个数据集中提取信息，并将其转换成可理解的结构，以进一步使用。出于发现数据集知识的目的，数据挖掘设计数据管理方面、数据预处理、模型与推断方面考量、兴趣度度量、复杂度的考虑，以及发现结构、可视化及在线更新等后处理。有鉴于此目标，“数据挖掘”实际上和“数据分析”这一名词在同一维度。 机器学习框架 大多数机器学习算法的基本框架都是模型（model/Representation）、代价函数（cost function）、优化算法。 以最简单的线性回归（Linear regression）来举例： 线性回归（模型）常用于有明显线性关系简单模型预测，例如流行病学、金融资产等，其算法的代价函数为最小二乘法（代价函数），即 最终使用梯度下降（优化算法）得到结果。 而在使用Logistic回归（Logistic Regression）算法时，Logistic回归（模型）使用的则是极大似然估计（代价函数），即 最终使用梯度下降（优化算法）得到结果。 最小二乘和极大似然的关系 在不同的情况中使用不同的代价函数，原因是各自的响应变量y服从不同的概率分布。 在线性回归中，前提假设是y服从正态分布，即\\[y\\sim N(\\mu,\\sigma^2)\\]而Logistic回归中的y是服从二项分布的，即\\[y\\sim Bernoulli(\\phi)\\] 因而，在用极大似然估计计算时，所得到的代价函数自然是不一样的。 最小二乘是从函数形式上来看的，极大似然是从概率意义上来看的。事实上，最小二乘可以由高斯噪声假设+极大似然估计推导出来。 所以在较复杂的模型中，一个代价函数可以用不同的优化算法，不同的代价函数也可以用相同的优化算法。 代价函数 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。 仍以线性回归作为示范，图中X为样本点，垂直蓝线是建模误差，为了得到合适的直线，我们必须令蓝线尽可能短，即建模误差最小。 image_1b72gv22748b11ejve71o60us718.png-20.2kB 为了使代价函数最小，我们绘制套入代价函数结果的三维等高图，即可得到 image_1b72h6eea1vc61qv74rf1t4o1t8l1l.png-69kB 图中最凹点即为代价函数最优情况。（具体函数推导详见coursera机器学习课程2-3）。 全局最优解/局部最优解 接下来，我们以梯度下降作为优化算法来计算代价函数最小值。 我们依然把一个二维数据集加入代价函数，绘制三维等高图，如图所示： image_1b72hdcd5ehjhhk1e68rte1gsp22.png-135.5kB 无论数据初始在什么位置，我们的目标，是进入全局最优点（即全局最低点），也就是机器学习工程师们常说的下山。 想象你正在所示起点，视野范围是有限的，为了尽快下山，你会在前往目之所及选择最低的一点。当你到达之前的最低点，再以此类推继续前往最低点。 如右边的线段所示，很常见的，在下山中不断的寻找有地点，很有可能进入并停留在一个局部最低点，而非全局最优点。这种情况常发生在视距(学习率a，库中命名为alpha)的选择不合适的情况下，降低a的值虽然可以提高进入全局最优点的准确率，也会很大的降低模型的计算速度。 欠拟合和过拟合 以一个分类模型为例： image_1b72i3cnefas1cvrjp9qp2qj22f.png-226.9kB 对一个多项式模型而言，x的次数越多，拟合效果就会越好，但是从操作上，将其控制在一个合适的范围内并不容易，图中第二个模型属于适当的分类，而第一个模型太过松散，称为欠拟合，第三个模型分类过度，称为过拟合，欠拟合和过拟合导致的调参问题一直都是机器学习使用者最大的问题。 在欠拟合中，拟合度不足，我们可以轻松的通过增加x的次数（多项式）来弥补；而在过拟合中，通常有两种处理方法： 1.使用特征工程，来抛弃一些对模型有影响却并没有实际意义的特征。 2.正则化，保留特征，但控制参数的大小。 在实际运用中，一般会同时使用这两种方法。后面会提及这两种方法的具体使用。 算法选择 主要算法 泛指被sklearn等大型库集成的算法。 1.朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练； 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感。 2.Logistic Regression（逻辑回归） 逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树、SVM相比，会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间）。 Sigmoid函数：表达式为公式: \\[f(x)=\\frac{1}{1+e^{−x}}\\] 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换； 3.线性回归 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： \\[\\hat{w}=(X^{T}X)^{-1}X^Ty\\] 而在LWLR（局部加权线性回归）中，参数的计算表达式为: \\[\\hat{w}=(X^{T}WX)^{-1}X^TWy\\] 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： + 实现简单，计算简单； 缺点： + 不能拟合非线性数据. 4.最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别； 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点： 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； 缺点: 计算量大（体现在距离计算上）； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 5.决策树 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。 决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。 信息熵的计算公式如下: \\[H=-\\sum^{n}_{i=1}p(x_i)log_2p(x_i)\\] 其中的n代表有n个分类类别（比如假设是二类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。 现在选中一个属性x用来进行分枝，此时分枝规则是：如果x=v的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’ =p1 H1+p2 H2,则此时的信息增益ΔH = H - H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。 优点： 计算简单，易于理解，可解释性强； 比较适合处理有缺失属性的样本； 能够处理不相关的特征； 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 缺点： 容易发生过拟合（随机森林可以很大程度上减少过拟合）； 忽略了数据之间的相关性； 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只+ 要是使用了信息增益，都有这个缺点，如RF）。 5.1 Adaboosting Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。 优点： Adaboost是一种有很高精度的分类器。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。 简单，不用做特征筛选。 不易发生过拟合。 缺点： 对离散值比较敏感 6.SVM支持向量机 支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。 优点： 可以解决高维问题，即大型特征空间； 能够处理非线性特征的相互作用； 无需依赖整个数据； 可以提高泛化能力； 缺点： 当观测样本很多时，效率并不是很高； 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数； 对缺失数据敏感； 对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）： 第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了； 第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果； 第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。 对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。 7. 人工神经网络的优缺点 （人工神经网络目前主要是深度学习的范畴） 优点： 分类的准确度高； 并行分布处理能力强,分布存储及学习能力强， 对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系； 具备联想记忆的功能。 缺点： 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值； 不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度； 学习时间过长,甚至可能达不到学习的目的。 8、K-Means聚类 优点： 算法简单，容易实现 ； 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛。 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点： 对数据类型要求较高，适合数值型数据； 可能收敛到局部最小值，在大规模数据上收敛较慢 K值比较难以选取； 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果； 不适合于发现非凸面形状的簇，或者大小差别很大的簇。 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。 仿生/模拟算法 通过对自然过程的模拟，对原有算法进行优化，也叫启发式算法（人工神经网络和深度学习也在其中）。 ### 9.蚁群算法 又称蚂蚁算法，是一种用来在图中寻找优化路径的机率型算法。其灵感来源于蚂蚁在寻找食物过程中发现路径的行为。蚁群算法是一种模拟进化算法，初步的研究表明该算法具有许多优良的性质.针对PID控制器参数优化设计问题，将蚁群算法设计的结果与遗传算法设计的结果进行了比较，数值仿真结果表明，蚁群算法具有一种新的模拟进化优化方法的有效性和应用价值。 10.粒子群算法(PSO) 又称微粒群算法，该算法使用如下心理学假设：在寻求一致的认知过程中，个体往往记住自身的信念，并同时考虑同事们的信念。当其察觉同事的信念较好的时候，将进行适应性地调整。 流程如下： 初始化一群微粒（群体规模为m），包括随机的位置和速度； 评价每个微粒的适应度； 对每个微粒，将它的适应值和它经历过的最好位置pbest的作比较，如果较好，则将其作为当前的最好位置pbest； 对每个微粒，将它的适应值和全局所经历最好位置gbest的作比较，如果较好，则重新设置gbest的索引号； 根据方程（1）变化微粒的速度和位置； 如未达到结束条件（通常为足够好的适应值或达到一个预设最大代数Gmax），回到2）。 11.模拟退火算法 模拟退火算法的原理同金属退火类似：将搜寻空间内每一点想像成空气内的分子；分子的能量，就是它本身的动能；而搜寻空间内的每一点，也像空气分子一样带有“能量”，以表示该点对命题的合适程度。算法先以搜寻空间内一个任意点作起始：每一步先选择一个“邻居”，然后再计算从现有位置到达“邻居”的概率。 在一个模型中，模拟退火算法起到的作用是跳崖，防止进入局部最优解。 12.禁忌搜索算法 类似于模拟退火算法的目标，具体方式为其先创立一个初始化的方案，基于此，算法“移动”到一相邻的方案。经过许多连续的移动过程，以提高解的质量。 其它优化算法 13.TF-IDF算法 TF-IDF是一种文本统计方法，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。常见于将文本转换为数值以相互比较和计算。 算法过程： 数据处理及应用 数据缩放 在特征工程中，除了算法使用代价函数，还可以将不同特征的量纲控制在一定范围内。 在梯度下降的使用中，面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯 度下降算法更快地收敛。面对不同量纲的数据，解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间。以解决特征量纲的干扰。 算法表现为： \\[ x_n = \\frac{x_n-\\mu}{s_n} \\] 缺失值/离散值/极大极小值的处理 数据集难免存在缺失数据和录入错误的极端值数据，极大极小值也会影响模型，因此有不同的处理方式。 缺失值：判断数值的缺失是否合理。如果缺失不合理，考虑在数据足够大的条件下删除此数据；如果缺失合理或数据大小不适合删除此条数据，根据陈天奇大牛的建议，可以将此数据设为-999。 离散值：判断数值的离散是否合理，如果合理则保存此数据；如果不合理，则查看是否可以判断不合理原因给予修正；如无法修正，可以考虑作同缺失值的处理。 极大极小值：对不支持的模型，将其作为缺失值处理。 降维 数据集的某些无意义特征的存在不但降低模型计算速度，还会形成噪音影响模型。因此通过数据降维来提升模型质量。降维的计算比较复杂，在不同的模型中应用方式也各有不同，好在大型的机器学习库都提供了相应的算法，不多赘述，详见解析。 稀疏矩阵 稀疏矩阵是指将离散化的特征拆开，形成新的多个特征的一种手段，新生成的特征中只包含True（1）和False（0），且大部分元素都为False(0)的矩阵。实际上，连续性数据也可以通过分组达到此目的。 稀疏矩阵的目的主要是通过其自身的稀疏特性，通过压缩可以大大节省稀疏矩阵的内存代价，从而加快模型的运算速度，更重要的是当数据过大时，通过稀疏矩阵，标准化的算法将之前不可操作的数据变为可操作数据。 sklearn虽然集成了稀疏矩阵函数，但其效果并不优秀。在python语言中，可以依赖pandas的get_dummies函数将数据快速化为稀疏举证。具体信息详见：机器学习中的范数规则化之（一）L0、L1与L2范数。 调参 调参往往是模型碰到的最后敌人，当准备工作处理完毕后，不同的参数对结果会造成很大的不同，因此调参技巧是机器学习工程师必须要掌握的内容。直接贴一篇大神的文章，剩下的坑以后再补…","tags":[]},{"title":"数据分析扫盲  --  1.传统数据分析","date":"2017-01-19T06:21:08.000Z","path":"2017/01/19/数据分析扫盲  --  1.传统数据分析/","text":"FBIWARNING：本文一切知识点，知识框架均来自于个人对数据分析、数据挖掘、机器学习等方面的理解，推测均出自于本人的臆测，如果对数据分析感兴趣，可以参阅可汗学院的《统计学》、各大学的数据分析和机器学习课程。 数据分析定义 数据分析是指用适当的统计方法对收集来的大量第一手资料和第二手资料进行分析，以求最大化地开发数据资料的功能，发挥数据的作用。 “上帝函数” 吴恩达（机器学习领域四大牛之一）在coursera的《机器学习》中提到，对所有的数据总体而言，有且必然有一条能够完美解释此样本的函数，称为“上帝函数”。但是上帝函数无法通过计算得到（所有的统计，都是对样本的统计，而不是对数据总体的统计；另外对总体而言，即使数据是离散的，函数在复杂样本的数学意义上还是连续的），只能无限的逼近它。 数据分析方向 数据分析的数学基础在20世纪早期就已确立，直到计算机出现才使得数据分析成为可能，在计算机性能不高时，以计算机协助人进行统计学分析以获得数据判断，这种数据分析方法被称为数据分析或传统数据分析。 随着计算机性能增长以及最小二乘法和梯度下降优化思想得以被大规模应用到机器中，数据分析师得以从有价值的信息中更快速、更准确的得到更接近于“上帝函数”的数学结果。针对不同类型的数据集，也有不同的经典模型套用以快速求得最优结果。此时使用者已经不再关注基础统计学指标，而是转向了挖掘数据知识、关系探索等方向，这种数据分析方法被称为数据挖掘或机器学习。 传统数据分析本质 在各种场景下（限定条件/逻辑），应用统计学（工具）对数据分析后的结果进行解释（逻辑），得到更接近真实情况的客观数据帮助决策的一种判断方式。 数据集的描述和定义 传统数据分析工具 在大多数需要统计的商业环境中，大型工具基本只有两种：SPSS和SAS。SPSS属于傻瓜相机，利用定制好的选项和图形界面在略懂统计知识的情况下即可作出符合传统数据分析的结论；如果有更深层的需求，例如银行、金融等环境，SAS能通过SAS语言用编程的形式制作所需的数据模型，从而得到更高定制化、更精确、性能更高的结果；此外，还有统计师使用MATLAB编程来达到所需目标。当然，以上三款软件所需都是不菲。因此在python和机器学习走强的今天，传统的统计工具只能保全大型金融机构这样的大客户，还是走向了衰败。R、oracle等语言也跟着大放异彩。 描述性统计分析 描述性统计分析是传统数据分析的基础，在此过程中需要求出数据分析的基础指标，包括： 平均值：常用的统计指标，受极端值影响。 中位数：数据以升序或降序排列后，处于最中间的数。如果数据呈现二向分布，中位数的表达会受极大的影响，且不能表现数据中极端值的影响。 众数：出现最多的数，缺点同上。 方差/标准差：反应样本个体之间的离散程度，统计学重要指标之一，有较多的应用。 上/下四分位数：数据以升序或降序排列后，25%位置与75%位置的值，常用于箱线图等对数据的图像展示。延伸有四分位数间距 偏度：数据倾向于左边（左偏）或右边（右偏），对股票、公司财务健康等数据有较为重要的意义。 变异系数：数据标准差与平均数的比，反应离散程度，常用于对比不同量纲的数据中消除离散程度。 二项分布 二项分布即重复n次独立的伯努利试验。最简单的二项分布就是掷硬币游戏。虽然二项分布的概率是人类的常识，但二项分布的数学推导并不简单。此外，统计学中许多重要分布例如正态分布、伯努利分布、泊松分布等等都是以二项分布作为基础推导而出的。因此在统计学中，二项分布具有极其重要的意义。 正态分布 正态分布，又称钟形曲线。该分布展示了数据频数与分布域的关系，许多统计定理都是以正态分布为基础，该分布是统计学中的核心。详细描述请参考百度百科。 正态分布之所以有巨大的研究价值，是因为大多数干预或未经干预的数据都呈正态分布状，从自然界各个数值的分布，到人类的行为，产品的偏差，基本都符合正态分布模式（其实是因为中心极限定理，呈现为正态分布）。掌握正态分布理论意味着在不清楚总体分布情况时，可以根据该数据的属性做其是否是正态分布的推测，可以轻易获得某情况较为接近真实存在的概率。 正态分布也是统计学思维一种表现形式，“如果一直猴子坐在电脑前无限的敲打键盘，那么它终有一天能敲出莎士比亚全集”就是其中之一：从正态分布来看，我们无法绝对否定一件事情的发生概率，只能说“很有可能”会产生这样的结果，这也是有很多讽刺统计学家笑话的原因。 ## 泊松分布 泊松分布适合于描述单位时间内随机事件发生的次数。泊松分布在管理科学、运筹学以及自然科学的某些问题中都占有重要的地位。详情见百度百科。 泊松分布可以在掌握部分样本的情况下，推测其值落在各个区间的概率。 ## 经验法则和切比雪夫不等式 当一组数据对称分布时（经验法则）： 约有68%的数据在平均数1个标准差以内。 约有95%的数据在平均数 2个标准差以内。切比雪夫不等式认为在75%个 约有99%的数据在平均数 3个标准差以内。 3个标准差以外的数据，在统计学上称为“离群点”。 当一组数据非对称分布时（切比雪夫不等式）： 约有75%的数据落在平均数 2个标准差以内。 约有89%的数据落在平均数 3个标准差以内。 约有94%的数据落在平均数 4个标准差以内。 实际使用上，切比雪夫不等式的估算效果并不好，只有当经验法则无法使用时（无标准界定，常用于偏度&gt;0.4）才会使用切比雪夫不等式。 大数定理和中心极限定理 大数定理和中心极限定理是统计学中最重要的两条定理，我认为因为这两条定理存在，才能切实的奠定统计学的价值，并且是统计学思维的根源 大数定理：又称大数法则，论证了在大量重复试验的过程中，样本量越多，样本平均值越接近于总体平均值。上文提到的切比雪夫不等式，是大数定理的一种特殊分布。详见维基百科。 中心极限定理：中心极限定理论证了在样本数较大时（通常定义为n&gt;30），样本均值近似正态分布。该定理在大数定理的基础上，给出了收敛的极限分布和渐近方差，更深入的研究了正态分布，是数理统计学和误差分析理论的研究基础。详见维基百科。 概率 概率基础：从三个问题说开去 赌徒谬论 超生游击队员老王已经连生4个闺女了，但老王实在太想要一个男娃，虽然家产都快被村里计生委的人给罚光，但还是要生，他想，都连生4个了，下个肯定是个带把的。 那么问题是，下一个小孩是男孩的概率？ 三门问题 假设你参加了一个电视节目，过关斩将来到了最后一关，此时你看见三扇关闭了的门，其中一扇的后面是一辆法拉利，另外两扇门后是山羊，当你选中了法拉利时，你就可以将其带走；如果选中了山羊，就会空手而归。支持人知道所有门后是什么。 你选定2号门后，主持人为了增加悬念，打开了3号门，门后是一只山羊，此时主持人询问你你是否要更换你的选择。那么问题是，是否应该更换选择？ ### 辛普森悖论 你是一位校长，假设有一天漂亮的秘书跑过来对你说：“校长，不好了，有很多男生在校门口抗议，他们说今年研究所女生录取率是男生的两倍，指责我们学校有性别歧视！” 这时你也很生气，说：“我不是特别交代，今年要尽量提升男生录取率以免落人口实吗？” 秘书赶紧回答说：“确实有交代下去，招生办说今年的男生确实比女生多，可是报告上显示男生所有学科录取率都比女生低了很多。” 那么问题是：有可能在男生所有学科录取率均比女生低的情况下，总性别比例还保持不变吗？ 辛普森悖论.jpg-8.6kB 概率论中需要注意的基础问题 以中国人平均数学水平来说，基础概率的计算基本不是问题，只需要注意一些概率计算的基本假设： 赌徒谬论：当事件与事件之间是相互独立的，不能联合计算概率，百度百科 三门问题：概率是随条件的改变而变化的，概率存在于被给予的条件下，概率不能寄托在实际的物体上，百度百科 辛普森悖论：在某个条件下的两组数据，分别讨论时都会满足某种性质，可是一旦合并考虑，却可能导致相反的结论，百度百科。 组合法则 组合法则是迅速计算在N内取n个有多少种方法的计算公式，体现为： \\[\\left( \\begin{matrix} N \\\\ n \\\\ \\end{matrix} \\right) = \\frac{N!}{(N-n)!} \\] 例如从4个项目中挑选2个来进行投资，则有: \\[\\left( \\begin{matrix} N \\\\ \\end{matrix} \\right) = \\frac{4!}{2! 2!} = \\frac{4\\times3\\times2\\times1}{(2\\times1)(2\\times1)} = 6\\] 贝叶斯定理 通常，事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的，然而，这两者是有确定的关系的；贝叶斯定理通过的是A、B事件转换而得到目标概率，贝叶斯公式的一个用途在于通过已知的三个概率函数推出第四个。 $ P(A|B) = $ 下面通过一道题来理解贝叶斯定理。 对残疾人来说，电动轮椅很难驾驭。假设在房间的某一位置，轮椅使用者可以选择D（穿过房门），S（直行），T（停在桌旁），使用者意愿的概率分别是P(D)=0.5，P(S)=0.2，P(T)=0.3；当轮椅使用者将控制杆转向前时（记为J），则有P(J|D)=0.3，P(J|S)=0.4，P(J|T)=0.05。如果此时使用者将操作杆向前，各种情况的概率各是多少？ 其中P(D|J)的解为： P(D|J) = P(J|D) * P(D) / P(J) P(D|J) = P(J|D) * P(D) / (P(D) * P(J|D) + P(S) * P(J|S) + P(T) * P(T|J)) P(D|J) = 0.3 * 0.5 / 0.245 P(D|J) = 0.612 置信区间 置信区间：置信区间是对分布（尤其是正态分布）的一种深入研究。通过对样本的计算，得到对某个总体参数的区间估计，展现为总体参数的真实值有多少概率落在所计算的区间里（目前国内的统计不接受概率的说法，认为此操作不应属于概率的范畴）。置信水平越高，置信区间就会越宽；在置信水平不变的情况下，样本数量越多，置信区间越窄，且置信区间的需求样本量可以被计算出来。 一般来说，置信区间一般会选择95%。 假设检验 “反证法是统计学者最强大的武器！” – 想不起来谁说的了 假设检验是一种用反证法证明对立条件（原假设/H0），从而判断目前条件（备择假设/Ha）是否正确的证明方式。常见名词如下： P值：观测的显著性水平，通过不同方式计算后，如果P值落在原假设的拒绝域，则拒绝原假设。 接受域：原假设的区间。 拒绝域：拒绝原假设的区间。 I类错误：原假设为真时，拒绝原假设。 II类错误：备择假设为真时，接受原假设。 单样本下的计算方法 在单样本下，置信区间和假设检验主要使用z统计和学生t统计（也称t检验）。 z统计和学生t统计是两种应用正态分布，求置信区间的计算方式。z统计应用于大样本的计算，因为中心极限定理的存在，z统计不要求该样本分布属于正态分布（因为会整理成近似正态分布）；学生t统计量主要应用于小样本的计算，要求总体分布必须近似正态分布，同时所取得的置信区间在同置信水平下也要比z统计得到的结果更宽。 两样本下的计算方法 两样本下，在统一量纲，消除样本均值差后，就可以使用常用统计方法来检验相关性。除了z统计和t统计外，还可使用卡方检验、F检验等。 在SPSS/SAS甚至excel等软件中，以上基础的检验方法都被大规模的修正/革新，所以基础的公式基本已经不会被使用了。 ## 更多样本，更多方法 在更多样本中，不同计算方法的差距会越来越大，传统数据分析更着眼于方差分析等检测样本相关性，简单线性回归线等，因此，样本的抽样和配对方法对模型造成的影响也会增加。除了以上介绍的统计方法，还有F统计，U检定等大项，每种大项下都有许多经过统计学家门修正的方法，在此不多赘述。 抽样组织形式 简单随机抽样：任何样本数为n的抽样几率都是相等的。 等距抽样/系统抽样：以总体N除以样本数n得到整数K，随机选择一个元素为R，以R开始，每隔K个的元素作为样本。 集群抽样：将总数分割成小的集群，再用简单随机抽样抽取小的集群，将集群作为样本。 二段随机抽样：先用集群抽样出集群，再对集群进行集群抽样。 传统统计分析总结 在以上知识的基础上，统计学家们更热衷修正原有的经典检验方法，以逻辑为推进从数学上增强各种推断检验标准，但也人类思维的局限性，模型的解释性强必然造成预测能力的削弱，这也是机器学习专家和统计学家重要的不同之处。统计学家集中了现代数学的精华，从数理上证明了SVM，boosting等理论基础，但是在实践和探索未知知识的路上，统计学家并未作出卓越的贡献。","tags":[]},{"title":"瞎聊聊怎么造个人工智能","date":"2017-01-05T05:13:19.000Z","path":"2017/01/05/瞎聊聊怎么造个人工智能/","text":"0 给黑猩猩一把枪，在教会他开枪之后，他会无聊的把枪丢掉，即使知道这能帮助他杀死自己的敌人。 人类则不一样，人类在大自然中是一个奇怪的、乐于争斗的物种。在电影里，为了营造冲突，人类不停的给自己制造着反派。随着人类认知的发展，我们得不停的为反派升级来试着在电影里毁灭人类，从鬼神、人类本身、外星人到新一代的人工智能。 从《太空漫游》开始，人类就不断在畅想人工智能，《太空漫游》还不错，起码AI不是坏人，接下去就慢慢不太对劲了，《终结者》、《机器公敌》、《鹰眼》，反正一个个人工智能都想毁灭人类。 OK，那我们作为程序员的问题是，怎样才能造一款能毁灭人类的人工智能？ 一个AI，想要迅速成长，一定不能受人力的限；不受人力的限制，一定要实现自学习；要应用自学习的成果，一定要实现代码的自我迭代，让机器自己给自己写代码。 就像《环形使者》一样，AI造出自己的下一版AI，如果AI可用，将会杀死前一个自己，不停的尝试自我毁灭的方式。 1 刚开始思考的时候就陷入一个错误的方向：AI要实现自学习，第一要实现代码识别，分析代码的意图，了解代码的意义，从而学会为了实现这个意义的写作方式；第二要实现提出需求，AI得知道“我需要一个列表来放置元素”才能制造一个列表。以此不断分解业务，由无数小的业务组成大的业务，最终实现自学习。 直到我看到了鸭子预测 &gt; 如果一只鸭子看起来像鸭子，游泳像鸭子，叫声像鸭子，那么它就是只鸭子。 智能生物对明显具有外部特征的定义，会选择只看表象来判断，而不是把它解剖了来确认；同理，机器只要了解通过代码中的数值转换，明白其最终达到了业务目标，那这就是代码的意义，除了结果之外，代码内容只是实现的“工具”而已，AI只需记住这么转换是可行的，下次就照着做准没错–如果错了，就去找下一段代码。 1956年的夏天，40岁的香农和28岁的麦卡锡、明斯基、37岁的罗切斯特及其他六位大神一起举办了朴茨茅斯会议，尝试解决自然语言处理的问题。世界上最聪明的10个人头脑风暴了一个月，最后得到的结论还有没有当代一个机器学习PHD在读脑子里的东西多–他们一度陷入了机器“翻译”是基于“理解”的错误上，实际上，华生实验室90年代的成果表明，机器不需要了解词语的意义，只需要按照大数据的理念找到这种情况的其他做法就行，这一成果一夜之间让机器翻译的准确率从70%提升到了90%。 其实人类对“原理”的理解，也只是用自己的所见去解释而已，我们不知道我们是不是活在《黑客帝国》里，我们不知道是不是三体人的质子正在干扰我们，我们不需要知道这么多。 AI也不需要知道。","tags":[]}]